{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf07fc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "sys.path.append(str(ROOT))\n",
    "\n",
    "import torch\n",
    "import mlflow\n",
    "import datetime\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Metric, MetricCollection\n",
    "from torchmetrics.classification import (\n",
    "    BinaryAccuracy, BinaryAUROC, BinaryF1Score, BinaryMatthewsCorrCoef,\n",
    "    MulticlassAccuracy, MulticlassAUROC, MulticlassF1Score)\n",
    "from pathlib import Path\n",
    "\n",
    "from src.datasets.dual_input import DualInputSequenceDataset\n",
    "from src.models.gru import GRUModel\n",
    "from src.data.pipeline import IngestionPipeline\n",
    "from src.train.gru import train_gru\n",
    "from src.utils.utils import CustomReduceLROnPlateau, collate_with_macro, TrainConfig, FocalLoss\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "def load_yaml_file(path):\n",
    "    with open(path) as stream:\n",
    "        try:\n",
    "            config_dict=yaml.safe_load(stream)\n",
    "            return config_dict\n",
    "        except yaml.YAMLError as e:\n",
    "            TypeError(f\"Config file could not be loaded: {e}\")\n",
    "    \n",
    "def train_model_from_config(cfg: TrainConfig) -> GRUModel:\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    return train(\n",
    "        company_path = Path(\"../\" + cfg.firm_data),\n",
    "        macro_paths = [str(id) for id in cfg.macro_data],\n",
    "        bankruptcy_col = str(cfg.bankruptcy_col),\n",
    "        company_col=str(cfg.company_col),\n",
    "        revenue_cap=int(cfg.revenue_cap),\n",
    "        metrics=cfg.get_metrics().to(cfg.device),\n",
    "        device=str(cfg.device),\n",
    "        num_layers=int(cfg.num_classes),\n",
    "        hidden_size=int(cfg.hidden_size),\n",
    "        output_size=1,\n",
    "        epochs=int(cfg.epochs),\n",
    "        lr=float(cfg.lr),\n",
    "        train_fract=float(cfg.train_fract),\n",
    "        dropout=int(cfg.dropout),\n",
    "        alpha=float(cfg.alpha),\n",
    "        gamma=float(cfg.gamma),\n",
    "        scheduler_factor=float(cfg.scheduler_factor),\n",
    "        scheduler_patience=int(cfg.scheduler_patience),\n",
    "        stopping_patience=int(cfg.stopping_patience),\n",
    "        decay_ih=float(cfg.decay_ih),\n",
    "        decay_hh=float(cfg.decay_hh),\n",
    "        decay_other=float(cfg.decay_other),\n",
    "        seed=int(cfg.seed)\n",
    "    )\n",
    "\n",
    "def train(\n",
    "    company_path: str,\n",
    "    macro_paths: list[str],\n",
    "    bankruptcy_col: str,\n",
    "    company_col: str,\n",
    "    revenue_cap: int,\n",
    "    metrics: list[Metric],\n",
    "    seed: int,\n",
    "    num_layers: int = 2,\n",
    "    hidden_size: int = 64,\n",
    "    output_size: int = 1,\n",
    "    epochs: int = 50,\n",
    "    lr: float = 1e-3,\n",
    "    train_fract: float = 0.8,\n",
    "    dropout: float = 0.2,\n",
    "    alpha: float = 0.9,\n",
    "    gamma: float = 2.0,\n",
    "    scheduler_factor: float = 0.85,\n",
    "    scheduler_patience: int = 50,\n",
    "    stopping_patience: int = 10,\n",
    "    stopping_window: int = 5,\n",
    "    min_lr: float = 0.0,\n",
    "    decay_ih:float = 1e-5,\n",
    "    decay_hh:float = 1e-5,\n",
    "    decay_other:float = 1e-5,\n",
    "    device: str=\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "):  \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    ingestion = IngestionPipeline(\n",
    "        company_path=company_path,\n",
    "        macro_paths=macro_paths,\n",
    "        company_col=company_col,\n",
    "        bankruptcy_col=bankruptcy_col,\n",
    "        revenue_cap=revenue_cap\n",
    "    )\n",
    "    \n",
    "    ingestion.run()\n",
    "    X, M_past, M_future, y = ingestion.get_tensors()\n",
    "    \n",
    "    dataset = DualInputSequenceDataset(\n",
    "        firm_tensor = X,\n",
    "        macro_past_tensor = M_past,\n",
    "        macro_future_tensor = M_future,\n",
    "        labels = y\n",
    "    )\n",
    "    \n",
    "    train_ds, val_ds, seed = dataset.stratified_split(train_fract)\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_with_macro)\n",
    "    val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=collate_with_macro)\n",
    "\n",
    "    logger.info(f\"Device: {device}\")\n",
    "    \n",
    "    metrics.to(device)\n",
    "    train_ds = train_ds.to_device(device)\n",
    "    val_ds = val_ds.to_device(device)\n",
    "    \n",
    "    pos_weight = dataset.pos_weight()\n",
    "    \n",
    "    firm_input_size, macro_input_size, _ = dataset.input_dims()\n",
    "    firm_input_size = firm_input_size[-1]\n",
    "    macro_input_size = macro_input_size[-2]\n",
    "    \n",
    "    mlflow.set_tracking_uri('http://127.0.0.1:8080')\n",
    "    mlflow.set_experiment('bankruptcy-predictions')\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag(\"model_type\", \"gru\")\n",
    "        mlflow.log_param(\"seed\", seed)\n",
    "        model = GRUModel(\n",
    "            firm_input_size=firm_input_size,\n",
    "            macro_input_size=macro_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=output_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        model = model.to(device)\n",
    "        \n",
    "        # loss_fn = FocalLoss(alpha=alpha, gamma=gamma, reduction=\"mean\")\n",
    "        loss_fn = BCEWithLogitsLoss(pos_weight = pos_weight)\n",
    "        \n",
    "        # Logging hyperparameters\n",
    "        mlflow.log_param(\"hidden_size\", hidden_size)\n",
    "        mlflow.log_param(\"output_size\", output_size)\n",
    "        mlflow.log_param(\"num_layers\", num_layers)\n",
    "        mlflow.log_param(\"dropout\", dropout)\n",
    "        mlflow.log_param(\"lr\", lr)\n",
    "\n",
    "        ih_params = []\n",
    "        hh_params = []\n",
    "        other_params = []\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                ih_params.append(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                hh_params.append(param)\n",
    "            else:\n",
    "                other_params.append(param)\n",
    "        \n",
    "        optimizer = Adam([\n",
    "                {'params': ih_params, 'weight_decay': decay_ih},\n",
    "                {'params': hh_params, 'weight_decay': decay_hh},\n",
    "                {'params': other_params, 'weight_decay': decay_other},\n",
    "            ], lr=lr\n",
    "        )\n",
    "        scheduler=ReduceLROnPlateau(\n",
    "            optimizer=optimizer,\n",
    "            mode=\"min\",\n",
    "            factor=scheduler_factor,\n",
    "            patience=scheduler_patience,\n",
    "            min_lr=min_lr\n",
    "        )\n",
    "        \n",
    "        train_gru(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            stopping_patience=stopping_patience,\n",
    "            stopping_window=stopping_window,\n",
    "            device=device,\n",
    "            epochs=epochs,\n",
    "            metrics=metrics\n",
    "        )\n",
    "        \n",
    "        model_name = f\"GRUModel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        mlflow.pytorch.log_model(model, model_name)\n",
    "        torch.save(obj = model.state_dict(), f = f\"../models/{model_name}.pth\")\n",
    "        print(f\"Model saved: {model_name}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "383adb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = load_yaml_file(\"../config/gru_config.yml\")\n",
    "cfg = TrainConfig(**config_dict)\n",
    "\n",
    "company_data_path = Path(\"../\" + cfg.firm_data)\n",
    "macro_data_path = [str(id) for id in cfg.macro_data]\n",
    "bankruptcy_col = str(cfg.bankruptcy_col)\n",
    "company_col=str(cfg.company_col)\n",
    "revenue_cap=int(cfg.revenue_cap)\n",
    "metrics=cfg.get_metrics().to(cfg.device)\n",
    "device=str(cfg.device)\n",
    "num_layers=int(cfg.num_classes)\n",
    "hidden_size=16\n",
    "output_size=1\n",
    "epochs=int(cfg.epochs)\n",
    "lr=float(cfg.lr)\n",
    "train_fract=float(cfg.train_fract)\n",
    "dropout=int(cfg.dropout)\n",
    "scheduler_factor=float(cfg.scheduler_factor)\n",
    "scheduler_patience=int(cfg.scheduler_patience)\n",
    "decay_ih=float(cfg.decay_ih)\n",
    "decay_hh=float(cfg.decay_hh)\n",
    "decay_other=float(cfg.decay_other)\n",
    "seed=int(cfg.seed)\n",
    "\n",
    "ingestion = IngestionPipeline(\n",
    "    company_path=company_data_path,\n",
    "    macro_paths=macro_data_path,\n",
    "    company_col=company_col,\n",
    "    bankruptcy_col=bankruptcy_col,\n",
    "    revenue_cap=revenue_cap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5486b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.loaders:Reading file: ../data/4941.xlsx\n",
      "INFO:src.data.loaders:Dropping high-revenue outliers...\n",
      "ERROR:root:Training failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 92, in _clean\n",
      "    df[self.bankruptcy_col] = pd.to_datetime(\n",
      "                              ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 1067, in to_datetime\n",
      "    values = convert_listlike(arg._values, format)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 407, in _convert_listlike_datetimes\n",
      "    return _to_datetime_with_unit(arg, unit, name, utc, errors)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 512, in _to_datetime_with_unit\n",
      "    arr = cast_from_unit_vectorized(arg, unit=unit)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"conversion.pyx\", line 149, in pandas._libs.tslibs.conversion.cast_from_unit_vectorized\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/numpy/core/fromnumeric.py\", line 3360, in round\n",
      "    return _wrapfunc(a, 'round', decimals=decimals, out=out)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/numpy/core/fromnumeric.py\", line 59, in _wrapfunc\n",
      "    return bound(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "FloatingPointError: overflow encountered in multiply\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3222305754.py\", line 7, in <module>\n",
      "    model = train_model_from_config(cfg)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3601185949.py\", line 46, in train_model_from_config\n",
      "    return train(\n",
      "           ^^^^^^\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3601185949.py\", line 110, in train\n",
      "    ingestion.run()\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/pipeline.py\", line 41, in run\n",
      "    company_df = self.company_loader.load(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 43, in load\n",
      "    df = self._clean(df)\n",
      "         ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 108, in _clean\n",
      "    raise RuntimeError(f\"Failed to process data after {max_attempts} attempts\")\n",
      "RuntimeError: Failed to process data after 10 attempts\n",
      "INFO:src.data.loaders:Reading file: ../data/4941.xlsx\n",
      "INFO:src.data.loaders:Dropping high-revenue outliers...\n",
      "INFO:src.data.loaders:Applying feature engineering...\n",
      "INFO:src.data.feature_engineer:Fitting feature engineer...\n",
      "INFO:src.data.feature_engineer:Engineering features...\n",
      "INFO:src.data.loaders:Loading 3 macroeconomic series...\n",
      "/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/sdmx/rest/common.py:367: UserWarning: 'agency_id' argument is redundant for data queries\n",
      "  getattr(self, f\"handle_{query_type}\")()\n",
      "INFO:sdmx.client:Request https://www.bdm.insee.fr/series/sdmx/data/SERIES_BDM/001587668\n",
      "INFO:sdmx.client:with headers {'User-Agent': 'python-requests/2.32.3', 'Accept-Encoding': 'gzip, deflate, br, zstd', 'Accept': 'application/vnd.sdmx.genericdata+xml;version=2.1', 'Connection': 'keep-alive'}\n",
      "/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/sdmx/rest/common.py:367: UserWarning: 'agency_id' argument is redundant for data queries\n",
      "  getattr(self, f\"handle_{query_type}\")()\n",
      "INFO:sdmx.client:Request https://www.bdm.insee.fr/series/sdmx/data/SERIES_BDM/000442588\n",
      "INFO:sdmx.client:with headers {'User-Agent': 'python-requests/2.32.3', 'Accept-Encoding': 'gzip, deflate, br, zstd', 'Accept': 'application/vnd.sdmx.genericdata+xml;version=2.1', 'Connection': 'keep-alive'}\n",
      "/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/sdmx/rest/common.py:367: UserWarning: 'agency_id' argument is redundant for data queries\n",
      "  getattr(self, f\"handle_{query_type}\")()\n",
      "INFO:sdmx.client:Request https://www.bdm.insee.fr/series/sdmx/data/SERIES_BDM/010774511\n",
      "INFO:sdmx.client:with headers {'User-Agent': 'python-requests/2.32.3', 'Accept-Encoding': 'gzip, deflate, br, zstd', 'Accept': 'application/vnd.sdmx.genericdata+xml;version=2.1', 'Connection': 'keep-alive'}\n",
      "INFO:src.data.tensor_factory:Converting financial series to tensors...\n",
      "INFO:src.data.tensor_factory:Scaling financial data with RobustScaler...\n",
      "INFO:src.data.tensor_factory:Financial data tensor shape: (1167, 3, 13)\n",
      "INFO:src.data.tensor_factory:Converting macroeconomic series to tensors...\n",
      "INFO:src.data.tensor_factory:Downsampling macro data to annual resolution...\n",
      "INFO:src.data.tensor_factory:Macro data tensor shape: torch.Size([3, 1]) (past), torch.Size([3, 1]) (future)\n",
      "INFO:__main__:Device: mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Past: torch.Size([3, 1]), Future: torch.Size([3, 1])\n",
      "Data sent to device: mps\n",
      "Data sent to device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50 | Loss: 0.22356 | ACCURACY: 0.99571 | AUROC: 0.99852 | F1: 0.81818 | MATTHEWS: 0.81601 | LOSS: 0.22356 | LR: 0.00042:  46%|████▌     | 23/50 [00:32<00:38,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 23, restoring model from epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[31m2025/08/04 19:45:23 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "INFO:src.data.loaders:Reading file: ../data/4941.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved: GRUModel_20250804_194519\n",
      "🏃 View run fun-horse-204 at: http://127.0.0.1:8080/#/experiments/387584985157093548/runs/0f1b954130ae4dbbaa2321edd2c31a40\n",
      "🧪 View experiment at: http://127.0.0.1:8080/#/experiments/387584985157093548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.loaders:Dropping high-revenue outliers...\n",
      "ERROR:root:Training failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 92, in _clean\n",
      "    df[self.bankruptcy_col] = pd.to_datetime(\n",
      "                              ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 1067, in to_datetime\n",
      "    values = convert_listlike(arg._values, format)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 407, in _convert_listlike_datetimes\n",
      "    return _to_datetime_with_unit(arg, unit, name, utc, errors)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 512, in _to_datetime_with_unit\n",
      "    arr = cast_from_unit_vectorized(arg, unit=unit)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"conversion.pyx\", line 149, in pandas._libs.tslibs.conversion.cast_from_unit_vectorized\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/numpy/core/fromnumeric.py\", line 3360, in round\n",
      "    return _wrapfunc(a, 'round', decimals=decimals, out=out)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/numpy/core/fromnumeric.py\", line 59, in _wrapfunc\n",
      "    return bound(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "FloatingPointError: overflow encountered in multiply\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3222305754.py\", line 7, in <module>\n",
      "    model = train_model_from_config(cfg)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3601185949.py\", line 46, in train_model_from_config\n",
      "    return train(\n",
      "           ^^^^^^\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3601185949.py\", line 110, in train\n",
      "    ingestion.run()\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/pipeline.py\", line 41, in run\n",
      "    company_df = self.company_loader.load(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 43, in load\n",
      "    df = self._clean(df)\n",
      "         ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 108, in _clean\n",
      "    raise RuntimeError(f\"Failed to process data after {max_attempts} attempts\")\n",
      "RuntimeError: Failed to process data after 10 attempts\n",
      "INFO:src.data.loaders:Reading file: ../data/4941.xlsx\n",
      "INFO:src.data.loaders:Dropping high-revenue outliers...\n",
      "ERROR:root:Training failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 92, in _clean\n",
      "    df[self.bankruptcy_col] = pd.to_datetime(\n",
      "                              ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 1067, in to_datetime\n",
      "    values = convert_listlike(arg._values, format)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 407, in _convert_listlike_datetimes\n",
      "    return _to_datetime_with_unit(arg, unit, name, utc, errors)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 512, in _to_datetime_with_unit\n",
      "    arr = cast_from_unit_vectorized(arg, unit=unit)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"conversion.pyx\", line 149, in pandas._libs.tslibs.conversion.cast_from_unit_vectorized\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/numpy/core/fromnumeric.py\", line 3360, in round\n",
      "    return _wrapfunc(a, 'round', decimals=decimals, out=out)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/numpy/core/fromnumeric.py\", line 59, in _wrapfunc\n",
      "    return bound(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "FloatingPointError: overflow encountered in multiply\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3222305754.py\", line 7, in <module>\n",
      "    model = train_model_from_config(cfg)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3601185949.py\", line 46, in train_model_from_config\n",
      "    return train(\n",
      "           ^^^^^^\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3601185949.py\", line 110, in train\n",
      "    ingestion.run()\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/pipeline.py\", line 41, in run\n",
      "    company_df = self.company_loader.load(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 43, in load\n",
      "    df = self._clean(df)\n",
      "         ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 108, in _clean\n",
      "    raise RuntimeError(f\"Failed to process data after {max_attempts} attempts\")\n",
      "RuntimeError: Failed to process data after 10 attempts\n",
      "INFO:src.data.loaders:Reading file: ../data/4941.xlsx\n",
      "INFO:src.data.loaders:Dropping high-revenue outliers...\n",
      "ERROR:root:Training failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 92, in _clean\n",
      "    df[self.bankruptcy_col] = pd.to_datetime(\n",
      "                              ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 1067, in to_datetime\n",
      "    values = convert_listlike(arg._values, format)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 407, in _convert_listlike_datetimes\n",
      "    return _to_datetime_with_unit(arg, unit, name, utc, errors)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 512, in _to_datetime_with_unit\n",
      "    arr = cast_from_unit_vectorized(arg, unit=unit)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"conversion.pyx\", line 149, in pandas._libs.tslibs.conversion.cast_from_unit_vectorized\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/numpy/core/fromnumeric.py\", line 3360, in round\n",
      "    return _wrapfunc(a, 'round', decimals=decimals, out=out)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/numpy/core/fromnumeric.py\", line 59, in _wrapfunc\n",
      "    return bound(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "FloatingPointError: overflow encountered in multiply\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3222305754.py\", line 7, in <module>\n",
      "    model = train_model_from_config(cfg)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3601185949.py\", line 46, in train_model_from_config\n",
      "    return train(\n",
      "           ^^^^^^\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3601185949.py\", line 110, in train\n",
      "    ingestion.run()\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/pipeline.py\", line 41, in run\n",
      "    company_df = self.company_loader.load(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 43, in load\n",
      "    df = self._clean(df)\n",
      "         ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 108, in _clean\n",
      "    raise RuntimeError(f\"Failed to process data after {max_attempts} attempts\")\n",
      "RuntimeError: Failed to process data after 10 attempts\n",
      "INFO:src.data.loaders:Reading file: ../data/4941.xlsx\n",
      "INFO:src.data.loaders:Dropping high-revenue outliers...\n",
      "ERROR:root:Training failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 92, in _clean\n",
      "    df[self.bankruptcy_col] = pd.to_datetime(\n",
      "                              ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 1067, in to_datetime\n",
      "    values = convert_listlike(arg._values, format)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 407, in _convert_listlike_datetimes\n",
      "    return _to_datetime_with_unit(arg, unit, name, utc, errors)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 512, in _to_datetime_with_unit\n",
      "    arr = cast_from_unit_vectorized(arg, unit=unit)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"conversion.pyx\", line 149, in pandas._libs.tslibs.conversion.cast_from_unit_vectorized\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/numpy/core/fromnumeric.py\", line 3360, in round\n",
      "    return _wrapfunc(a, 'round', decimals=decimals, out=out)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/numpy/core/fromnumeric.py\", line 59, in _wrapfunc\n",
      "    return bound(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "FloatingPointError: overflow encountered in multiply\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3222305754.py\", line 7, in <module>\n",
      "    model = train_model_from_config(cfg)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3601185949.py\", line 46, in train_model_from_config\n",
      "    return train(\n",
      "           ^^^^^^\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3601185949.py\", line 110, in train\n",
      "    ingestion.run()\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/pipeline.py\", line 41, in run\n",
      "    company_df = self.company_loader.load(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 43, in load\n",
      "    df = self._clean(df)\n",
      "         ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 108, in _clean\n",
      "    raise RuntimeError(f\"Failed to process data after {max_attempts} attempts\")\n",
      "RuntimeError: Failed to process data after 10 attempts\n",
      "INFO:src.data.loaders:Reading file: ../data/4941.xlsx\n",
      "INFO:src.data.loaders:Dropping high-revenue outliers...\n",
      "ERROR:root:Training failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 92, in _clean\n",
      "    df[self.bankruptcy_col] = pd.to_datetime(\n",
      "                              ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 1067, in to_datetime\n",
      "    values = convert_listlike(arg._values, format)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 407, in _convert_listlike_datetimes\n",
      "    return _to_datetime_with_unit(arg, unit, name, utc, errors)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 512, in _to_datetime_with_unit\n",
      "    arr = cast_from_unit_vectorized(arg, unit=unit)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"conversion.pyx\", line 149, in pandas._libs.tslibs.conversion.cast_from_unit_vectorized\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/numpy/core/fromnumeric.py\", line 3360, in round\n",
      "    return _wrapfunc(a, 'round', decimals=decimals, out=out)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/numpy/core/fromnumeric.py\", line 59, in _wrapfunc\n",
      "    return bound(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "FloatingPointError: overflow encountered in multiply\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3222305754.py\", line 7, in <module>\n",
      "    model = train_model_from_config(cfg)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3601185949.py\", line 46, in train_model_from_config\n",
      "    return train(\n",
      "           ^^^^^^\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3601185949.py\", line 110, in train\n",
      "    ingestion.run()\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/pipeline.py\", line 41, in run\n",
      "    company_df = self.company_loader.load(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 43, in load\n",
      "    df = self._clean(df)\n",
      "         ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 108, in _clean\n",
      "    raise RuntimeError(f\"Failed to process data after {max_attempts} attempts\")\n",
      "RuntimeError: Failed to process data after 10 attempts\n",
      "INFO:src.data.loaders:Reading file: ../data/4941.xlsx\n",
      "INFO:src.data.loaders:Dropping high-revenue outliers...\n",
      "ERROR:root:Training failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 92, in _clean\n",
      "    df[self.bankruptcy_col] = pd.to_datetime(\n",
      "                              ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 1067, in to_datetime\n",
      "    values = convert_listlike(arg._values, format)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 407, in _convert_listlike_datetimes\n",
      "    return _to_datetime_with_unit(arg, unit, name, utc, errors)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 512, in _to_datetime_with_unit\n",
      "    arr = cast_from_unit_vectorized(arg, unit=unit)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"conversion.pyx\", line 149, in pandas._libs.tslibs.conversion.cast_from_unit_vectorized\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/numpy/core/fromnumeric.py\", line 3360, in round\n",
      "    return _wrapfunc(a, 'round', decimals=decimals, out=out)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/numpy/core/fromnumeric.py\", line 59, in _wrapfunc\n",
      "    return bound(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "FloatingPointError: overflow encountered in multiply\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3222305754.py\", line 7, in <module>\n",
      "    model = train_model_from_config(cfg)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3601185949.py\", line 46, in train_model_from_config\n",
      "    return train(\n",
      "           ^^^^^^\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3601185949.py\", line 110, in train\n",
      "    ingestion.run()\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/pipeline.py\", line 41, in run\n",
      "    company_df = self.company_loader.load(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 43, in load\n",
      "    df = self._clean(df)\n",
      "         ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 108, in _clean\n",
      "    raise RuntimeError(f\"Failed to process data after {max_attempts} attempts\")\n",
      "RuntimeError: Failed to process data after 10 attempts\n",
      "INFO:src.data.loaders:Reading file: ../data/4941.xlsx\n",
      "INFO:src.data.loaders:Dropping high-revenue outliers...\n",
      "ERROR:root:Training failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 92, in _clean\n",
      "    df[self.bankruptcy_col] = pd.to_datetime(\n",
      "                              ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 1067, in to_datetime\n",
      "    values = convert_listlike(arg._values, format)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 407, in _convert_listlike_datetimes\n",
      "    return _to_datetime_with_unit(arg, unit, name, utc, errors)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 512, in _to_datetime_with_unit\n",
      "    arr = cast_from_unit_vectorized(arg, unit=unit)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"conversion.pyx\", line 149, in pandas._libs.tslibs.conversion.cast_from_unit_vectorized\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/numpy/core/fromnumeric.py\", line 3360, in round\n",
      "    return _wrapfunc(a, 'round', decimals=decimals, out=out)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/numpy/core/fromnumeric.py\", line 59, in _wrapfunc\n",
      "    return bound(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "FloatingPointError: overflow encountered in multiply\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3222305754.py\", line 7, in <module>\n",
      "    model = train_model_from_config(cfg)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3601185949.py\", line 46, in train_model_from_config\n",
      "    return train(\n",
      "           ^^^^^^\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3601185949.py\", line 110, in train\n",
      "    ingestion.run()\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/pipeline.py\", line 41, in run\n",
      "    company_df = self.company_loader.load(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 43, in load\n",
      "    df = self._clean(df)\n",
      "         ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 108, in _clean\n",
      "    raise RuntimeError(f\"Failed to process data after {max_attempts} attempts\")\n",
      "RuntimeError: Failed to process data after 10 attempts\n",
      "INFO:src.data.loaders:Reading file: ../data/4941.xlsx\n",
      "INFO:src.data.loaders:Dropping high-revenue outliers...\n",
      "ERROR:root:Training failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 92, in _clean\n",
      "    df[self.bankruptcy_col] = pd.to_datetime(\n",
      "                              ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 1067, in to_datetime\n",
      "    values = convert_listlike(arg._values, format)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 407, in _convert_listlike_datetimes\n",
      "    return _to_datetime_with_unit(arg, unit, name, utc, errors)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py\", line 512, in _to_datetime_with_unit\n",
      "    arr = cast_from_unit_vectorized(arg, unit=unit)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"conversion.pyx\", line 149, in pandas._libs.tslibs.conversion.cast_from_unit_vectorized\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/numpy/core/fromnumeric.py\", line 3360, in round\n",
      "    return _wrapfunc(a, 'round', decimals=decimals, out=out)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/numpy/core/fromnumeric.py\", line 59, in _wrapfunc\n",
      "    return bound(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "FloatingPointError: overflow encountered in multiply\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3222305754.py\", line 7, in <module>\n",
      "    model = train_model_from_config(cfg)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3601185949.py\", line 46, in train_model_from_config\n",
      "    return train(\n",
      "           ^^^^^^\n",
      "  File \"/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/ipykernel_32279/3601185949.py\", line 110, in train\n",
      "    ingestion.run()\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/pipeline.py\", line 41, in run\n",
      "    company_df = self.company_loader.load(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 43, in load\n",
      "    df = self._clean(df)\n",
      "         ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py\", line 108, in _clean\n",
      "    raise RuntimeError(f\"Failed to process data after {max_attempts} attempts\")\n",
      "RuntimeError: Failed to process data after 10 attempts\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFloatingPointError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py:92\u001b[39m, in \u001b[36mCompanyDataLoader._clean\u001b[39m\u001b[34m(self, df)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     df[\u001b[38;5;28mself\u001b[39m.bankruptcy_col] = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1899-12-30\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcoerce\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     target_year = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m.years[-\u001b[32m1\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:1067\u001b[39m, in \u001b[36mto_datetime\u001b[39m\u001b[34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[39m\n\u001b[32m   1066\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1067\u001b[39m     values = \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1068\u001b[39m     result = arg._constructor(values, index=arg.index, name=arg.name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:407\u001b[39m, in \u001b[36m_convert_listlike_datetimes\u001b[39m\u001b[34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[39m\n\u001b[32m    406\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot specify both format and unit\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to_datetime_with_unit\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(arg, \u001b[33m\"\u001b[39m\u001b[33mndim\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m) > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:512\u001b[39m, in \u001b[36m_to_datetime_with_unit\u001b[39m\u001b[34m(arg, unit, name, utc, errors)\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m512\u001b[39m     arr = \u001b[43mcast_from_unit_vectorized\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[43m=\u001b[49m\u001b[43munit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OutOfBoundsDatetime:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mconversion.pyx:149\u001b[39m, in \u001b[36mpandas._libs.tslibs.conversion.cast_from_unit_vectorized\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3360\u001b[39m, in \u001b[36mround\u001b[39m\u001b[34m(a, decimals, out)\u001b[39m\n\u001b[32m   3271\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3272\u001b[39m \u001b[33;03mEvenly round to the given number of decimals.\u001b[39;00m\n\u001b[32m   3273\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3358\u001b[39m \n\u001b[32m   3359\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3360\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mround\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecimals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecimals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59\u001b[39m, in \u001b[36m_wrapfunc\u001b[39m\u001b[34m(obj, method, *args, **kwds)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m     61\u001b[39m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[32m     62\u001b[39m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n",
      "\u001b[31mFloatingPointError\u001b[39m: overflow encountered in multiply",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     model = \u001b[43mtrain_model_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mtrain_model_from_config\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Main training function\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompany_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfirm_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmacro_paths\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmacro_data\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevenue_cap\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrevenue_cap\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_fract\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_fract\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscheduler_factor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler_patience\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscheduler_patience\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_patience\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstopping_patience\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecay_ih\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecay_ih\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecay_hh\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecay_hh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecay_other\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecay_other\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 110\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(company_path, macro_paths, bankruptcy_col, company_col, revenue_cap, metrics, seed, num_layers, hidden_size, output_size, epochs, lr, train_fract, dropout, alpha, gamma, scheduler_factor, scheduler_patience, stopping_patience, stopping_window, min_lr, decay_ih, decay_hh, decay_other, device)\u001b[39m\n\u001b[32m    102\u001b[39m ingestion = IngestionPipeline(\n\u001b[32m    103\u001b[39m     company_path=company_path,\n\u001b[32m    104\u001b[39m     macro_paths=macro_paths,\n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m     revenue_cap=revenue_cap\n\u001b[32m    108\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[43mingestion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m X, M_past, M_future, y = ingestion.get_tensors()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/src/data/pipeline.py:41\u001b[39m, in \u001b[36mIngestionPipeline.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     company_df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompany_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     macro_df= \u001b[38;5;28mself\u001b[39m.macro_loader.load()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py:43\u001b[39m, in \u001b[36mCompanyDataLoader.load\u001b[39m\u001b[34m(self, company_col, bankruptcy_col, mode)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mself\u001b[39m.years = \u001b[38;5;28msorted\u001b[39m(\n\u001b[32m     40\u001b[39m     {\u001b[38;5;28mint\u001b[39m(col[-\u001b[32m4\u001b[39m:]) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df.columns \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mrevenue\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m col}\n\u001b[32m     41\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_clean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.feature_engineer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py:108\u001b[39m, in \u001b[36mCompanyDataLoader._clean\u001b[39m\u001b[34m(self, df)\u001b[39m\n\u001b[32m    107\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m attempt == (max_attempts - \u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to process data after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_attempts\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m attempts\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    111\u001b[39m df.drop(\u001b[38;5;28mself\u001b[39m.bankruptcy_col, axis = \u001b[32m1\u001b[39m, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to process data after 10 attempts",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m     12\u001b[39m     logging.error(\u001b[33m\"\u001b[39m\u001b[33mTraining failed.\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        model = train_model_from_config(cfg)\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.mps.empty_cache()\n",
    "    except:\n",
    "        logging.error(\"Training failed.\", exc_info=True)\n",
    "        time.sleep(3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ee888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "import logging\n",
    "import mlflow\n",
    "import yaml\n",
    "\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import MetricCollection\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from src.data.pipeline import IngestionPipeline\n",
    "from src.datasets.dual_input import DualInputSequenceDataset\n",
    "from src.models.tft import TFTModel\n",
    "from src.utils.utils import CustomReduceLROnPlateau, TrainConfig, collate_with_macro\n",
    "\n",
    "def load_yaml_file(path):\n",
    "    with open(path) as stream:\n",
    "        try:\n",
    "            config_dict=yaml.safe_load(stream)\n",
    "            return config_dict\n",
    "        except yaml.YAMLError as e:\n",
    "            TypeError(f\"Config file could not be loaded: {e}\")\n",
    "\n",
    "\n",
    "def train_tft(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    num_epochs: int,\n",
    "    metrics: MetricCollection,\n",
    "    criterion: nn.Module = nn.MSELoss(),\n",
    "    optimizer_cls=torch.optim.Adam,\n",
    "    lr: float = 1e-3,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"mps\" \n",
    "        if torch.mps.is_available() else \"cpu\",\n",
    "    early_stopping_patience: int = 10,\n",
    "    scheduler_cls=None,\n",
    "    scheduler_kwargs=None,\n",
    "    log_fn=None,\n",
    "):\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer=optimizer_cls(model.parameters(), lr=lr)\n",
    "    scheduler=scheduler_cls(optimizer, **scheduler_kwargs) if scheduler_cls else None\n",
    "    \n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} / {num_epochs} — Training\"):\n",
    "            firm_x = batch[\"firm_seq\"].to(device)\n",
    "            macro_x = batch[\"macro_seq\"].to(device)\n",
    "            decoder_x = macro_x[:, -12:, :] # Condition on the last year of macro data\n",
    "            y = batch[\"label\"].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(firm_x, macro_x, decoder_x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "        train_loss = sum(train_losses) / len(train_losses)\n",
    "        computed_metrics = {\n",
    "            name: metric.compute().item() for name, metric in metrics.items()\n",
    "        }\n",
    "        computed_metrics[\"loss\"] = train_loss\n",
    "        for name, value in metrics.items():\n",
    "            mlflow.log_metric(f\"train_{name}\", value, step = epoch)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} / {num_epochs} — Validation\"):\n",
    "                firm_x = batch[\"firm_seq\"].to(device)\n",
    "                macro_x = batch[\"macro_seq\"].to(device)\n",
    "                decoder_x = macro_x[:, -12:, :] # Condition on the last year of macro data\n",
    "                y_hat = model(firm_x, macro_x, decoder_x)\n",
    "                \n",
    "                val_loss = criterion(y_hat, y)\n",
    "                val_losses.append(val_loss.item())\n",
    "        \n",
    "        val_loss = sum(val_losses) / len(val_losses)\n",
    "        computed_metrics = {\n",
    "            name: metric.compute().item() for name, metric in metrics.items()\n",
    "        }\n",
    "        for name, value in metrics.items():\n",
    "            mlflow.log_metric(f\"val_{name}\", value, step = epoch)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        \n",
    "        if log_fn:\n",
    "            log_fn(epoch=epoch, train_loss=train_loss, val_loss=val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_tft_model.pt\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "    model_name = f\"model_{datetime.datetime.now()}\"\n",
    "    mlflow.pytorch.log_model(model, model_name)\n",
    "    torch.save(obj = model.state_dict(), f = f\"models/{model_name}.pth\")\n",
    "    print(f\"Model saved: {model_name}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "def main():\n",
    "    config_dict = load_yaml_file(\"config/model_config.yml\")\n",
    "    cfg = TrainConfig(**config_dict)\n",
    "\n",
    "    company_path = Path(cfg.firm_data)\n",
    "    macro_paths = [Path(path) for path in cfg.macro_data]\n",
    "    bankruptcy_col = str(cfg.bankruptcy_col)\n",
    "    company_col=str(cfg.company_col)\n",
    "    revenue_cap=int(cfg.revenue_cap)\n",
    "    metrics=cfg.get_metrics().to(cfg.device)\n",
    "    device=str(cfg.device)\n",
    "    num_layers=int(cfg.num_classes)\n",
    "    hidden_size=int(cfg.hidden_size)\n",
    "    output_size=1\n",
    "    epochs=int(cfg.epochs)\n",
    "    lr=float(cfg.lr)\n",
    "    train_fract=float(cfg.train_fract)\n",
    "    dropout=int(cfg.dropout)\n",
    "    scheduler_factor=float(cfg.scheduler_factor)\n",
    "    scheduler_patience=int(cfg.scheduler_patience)\n",
    "    decay_ih=float(cfg.decay_ih)\n",
    "    decay_hh=float(cfg.decay_hh)\n",
    "    decay_other=float(cfg.decay_other)\n",
    "    seed=int(cfg.seed)\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    ingestion = IngestionPipeline(\n",
    "        company_path=company_path,\n",
    "        macro_paths=macro_paths,\n",
    "        company_col=company_col,\n",
    "        bankruptcy_col=bankruptcy_col,\n",
    "        revenue_cap=revenue_cap\n",
    "    )\n",
    "    \n",
    "    ingestion.run()\n",
    "    \n",
    "    X, M, y = ingestion.get_tensors()\n",
    "    \n",
    "    dataset = DualInputSequenceDataset(\n",
    "        firm_tensor = X,\n",
    "        macro_tensor = M,\n",
    "        labels = y\n",
    "    )\n",
    "    \n",
    "    train_ds, val_ds, seed = dataset.stratified_split(train_fract)\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_with_macro)\n",
    "    val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=collate_with_macro)\n",
    "\n",
    "    logger.info(f\"Device: {device}\")\n",
    "    \n",
    "    firm_input_size, macro_input_size = dataset.input_dims()\n",
    "\n",
    "    optimizer = Adam\n",
    "    scheduler=CustomReduceLROnPlateau\n",
    "    scheduler_kwargs = {\n",
    "        \"mode\": \"min\",\n",
    "        \"factor\": scheduler_factor,\n",
    "        \"patience\": scheduler_patience,\n",
    "        \"min_lr\": 0.0\n",
    "    }\n",
    "    \n",
    "    pos_weight = dataset.pos_weight()\n",
    "    loss_fn = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    mlflow.set_tracking_uri('http://127.0.0.1:8080')\n",
    "    mlflow.set_experiment('bankruptcy-predictions')\n",
    "    \n",
    "    # Logging hyperparameters\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"hidden_size\", hidden_size)\n",
    "        mlflow.log_param(\"output_size\", output_size)\n",
    "        mlflow.log_param(\"num_layers\", num_layers)\n",
    "        mlflow.log_param(\"dropout\", dropout)\n",
    "        mlflow.log_param(\"lr\", lr)\n",
    "        mlflow.log_param(\"seed\", seed)\n",
    "        \n",
    "        model = TFTModel(\n",
    "            static_input_dim=firm_input_size,\n",
    "            encoder_input_dims=[1] * macro_input_size,\n",
    "            decoder_input_dims=[1] * macro_input_size,\n",
    "            hidden_dim=hidden_size,\n",
    "            attention_heads=4,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        train_tft(\n",
    "            model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            num_epochs=epochs,\n",
    "            criterion=loss_fn,\n",
    "            optimizer_cls=optimizer,\n",
    "            lr=lr,\n",
    "            scheduler_cls=scheduler,\n",
    "            scheduler_kwargs=scheduler_kwargs,\n",
    "            metrics=metrics\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88fe24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db3aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160dc5d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

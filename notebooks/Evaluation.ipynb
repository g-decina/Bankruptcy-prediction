{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3516f21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "sys.path.append(str(ROOT))\n",
    "\n",
    "import mlflow\n",
    "import yaml\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.data.pipeline import IngestionPipeline\n",
    "from src.datasets.dual_input import DualInputSequenceDataset\n",
    "from src.models.gru import GRUModel, EnsembleGRU\n",
    "from src.utils.utils import collate_with_macro, TrainConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f19ab804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.loaders:Reading file: ../data/demo_data.xlsx\n",
      "INFO:src.data.loaders:Dropping high-revenue outliers...\n",
      "INFO:src.data.loaders:Loading 3 macroeconomic series...\n",
      "/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py:136: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"Date\"]=pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
      "/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py:136: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"Date\"]=pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
      "/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py:136: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"Date\"]=pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
      "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "DEBUG:cmdstanpy:input tempfile: /var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpjakkf4gv/ywpx4dfs.json\n",
      "DEBUG:cmdstanpy:input tempfile: /var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpjakkf4gv/5mi40v0c.json\n",
      "DEBUG:cmdstanpy:idx 0\n",
      "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
      "DEBUG:cmdstanpy:CmdStan args: ['/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=56512', 'data', 'file=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpjakkf4gv/ywpx4dfs.json', 'init=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpjakkf4gv/5mi40v0c.json', 'output', 'file=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpjakkf4gv/prophet_model1pxmge1g/prophet_model-20250724172236.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
      "17:22:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "INFO:cmdstanpy:Chain [1] start processing\n",
      "17:22:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "INFO:cmdstanpy:Chain [1] done processing\n",
      "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "DEBUG:cmdstanpy:input tempfile: /var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpjakkf4gv/z0qlot7k.json\n",
      "DEBUG:cmdstanpy:input tempfile: /var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpjakkf4gv/erz9qg_5.json\n",
      "DEBUG:cmdstanpy:idx 0\n",
      "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
      "DEBUG:cmdstanpy:CmdStan args: ['/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=57183', 'data', 'file=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpjakkf4gv/z0qlot7k.json', 'init=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpjakkf4gv/erz9qg_5.json', 'output', 'file=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpjakkf4gv/prophet_modell1oizm_u/prophet_model-20250724172236.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
      "17:22:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "INFO:cmdstanpy:Chain [1] start processing\n",
      "17:22:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "INFO:cmdstanpy:Chain [1] done processing\n",
      "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "DEBUG:cmdstanpy:input tempfile: /var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpjakkf4gv/xhygcdi0.json\n",
      "DEBUG:cmdstanpy:input tempfile: /var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpjakkf4gv/_ngkqb_3.json\n",
      "DEBUG:cmdstanpy:idx 0\n",
      "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
      "DEBUG:cmdstanpy:CmdStan args: ['/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=79067', 'data', 'file=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpjakkf4gv/xhygcdi0.json', 'init=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpjakkf4gv/_ngkqb_3.json', 'output', 'file=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpjakkf4gv/prophet_model0dxxg0lm/prophet_model-20250724172237.csv', 'method=optimize', 'algorithm=newton', 'iter=10000']\n",
      "17:22:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "INFO:cmdstanpy:Chain [1] start processing\n",
      "17:22:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "INFO:cmdstanpy:Chain [1] done processing\n",
      "INFO:src.data.feature_engineer:Transforming new data with fitted features...\n",
      "INFO:src.data.feature_engineer:Engineering features...\n",
      "INFO:src.data.tensor_factory:Converting financial series to tensors...\n",
      "INFO:src.data.tensor_factory:Scaling financial data with RobustScaler...\n",
      "INFO:src.data.tensor_factory:Shaped financial data tensor: (6296, 3, 4)\n",
      "INFO:src.data.tensor_factory:Shaped macro data tensor: torch.Size([3, 36])\n"
     ]
    }
   ],
   "source": [
    "with open(\"../config/model_config.yml\") as stream:\n",
    "        config=yaml.safe_load(stream)\n",
    "        config=TrainConfig(**config)\n",
    "\n",
    "macro_paths = [\"../\" + path for path in config.macro_data]\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    company_path = \"../\" + config.firm_data,\n",
    "    macro_paths = macro_paths,\n",
    "    company_col = config.company_col,\n",
    "    bankruptcy_col = config.bankruptcy_col\n",
    ")\n",
    "\n",
    "pipeline.run()\n",
    "X, M, y = pipeline.get_tensors()\n",
    "\n",
    "dataset = DualInputSequenceDataset(firm_tensor=X, macro_tensor=M, labels=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e36fc49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.firm_tensor.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a7c5d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sent to device: mps\n"
     ]
    }
   ],
   "source": [
    "device=\"mps\"\n",
    "dataset.to_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "010daa51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6296, 3, 4]), torch.Size([3, 36]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.input_dims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb00783",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EnsembleGRU' object has no attribute 'weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweights\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1928\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1926\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1927\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1928\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1929\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1930\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'EnsembleGRU' object has no attribute 'weights'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d534815e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895ceaee79dd444b80b7085828824cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import torch\n",
    "from src.models.gru import GRUModel\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
    "model_uri = \"runs:/ab1759a5e70b495ab95660a139a635aa/model_2025-07-23 22:11:49.460279\"\n",
    "\n",
    "model = mlflow.pytorch.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1d262b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ad09497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "efd47051",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = EnsembleGRU(models=model.models, hidden_sizes=[32, 16],\n",
    "                    threshold=model.threshold, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "357718a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ad5e5fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnsembleGRU(\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=32, bias=True)\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): GELU(approximate='none')\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564d6c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1228ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = dict(torch.load(\"../models/model_2025-07-24 09:03:44.021601.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d6296bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnsembleGRU(\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=32, bias=True)\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): GELU(approximate='none')\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81a8b843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnsembleGRU(\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=32, bias=True)\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): GELU(approximate='none')\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d726fef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_items([('mlp.0.weight', tensor([[ 0.2907, -0.1019, -0.2359,  0.0024,  0.4878],\n",
       "        [ 0.7834,  0.1607, -0.1966, -0.2057, -0.3746],\n",
       "        [-0.2993,  0.4070,  0.2314, -0.0563, -0.0835],\n",
       "        [-0.4575, -0.2892,  0.6885, -0.1054, -0.3468],\n",
       "        [-0.7886,  0.5006, -0.0492,  0.0852, -0.1190],\n",
       "        [ 0.2763,  0.2465, -0.5119,  0.1890, -0.5665],\n",
       "        [ 0.0242, -0.0562, -0.0910,  0.7594, -0.1328],\n",
       "        [-0.0890,  0.2369,  0.1490, -0.0857, -0.7911],\n",
       "        [-0.5069,  0.0232, -0.3157,  0.4576,  0.1612],\n",
       "        [ 0.3682,  0.4130, -0.4107, -0.2102, -0.5238],\n",
       "        [-0.0534,  0.0021,  0.1241,  0.8764,  0.0791],\n",
       "        [-0.2333,  0.1001, -0.3299,  0.5708,  0.1567],\n",
       "        [ 0.2389, -0.2877, -0.3486,  0.5910,  0.4528],\n",
       "        [ 0.0195, -0.3562,  0.1867,  0.4644,  0.3978],\n",
       "        [-0.2698, -0.0608,  0.0355,  0.8782,  0.1496],\n",
       "        [ 0.4117, -0.2977,  0.0456, -0.5023,  0.3168],\n",
       "        [ 0.3175,  0.3767,  0.4090, -0.3336,  0.2096],\n",
       "        [ 0.5913, -0.2702, -0.2036,  0.2511,  0.0588],\n",
       "        [-0.6026, -0.0829,  0.4312, -0.0389, -0.1918],\n",
       "        [-0.3390, -0.0979, -0.5003,  0.6164, -0.1379],\n",
       "        [-0.9114, -0.0513,  0.0023,  0.1980,  0.1415],\n",
       "        [ 0.2093, -0.0838, -0.6694,  0.1823, -0.3622],\n",
       "        [-0.9813,  0.0059, -0.1762,  0.3018,  0.0053],\n",
       "        [-0.7251, -0.1047,  0.4675, -0.1136,  0.3058],\n",
       "        [ 0.2548, -0.3052,  0.2566, -0.4787,  0.4450],\n",
       "        [-0.3592,  0.3539,  0.0626, -0.1053, -0.2416],\n",
       "        [-0.5094, -0.2997, -0.0995,  0.4546, -0.0193],\n",
       "        [ 0.2378, -0.1025, -0.1202, -0.5737,  0.7343],\n",
       "        [-0.2631, -0.1345,  0.2214, -0.2290,  0.9057],\n",
       "        [-0.5316,  0.5052,  0.1958, -0.2583,  0.1241],\n",
       "        [ 0.4545, -0.1439, -0.2724,  0.2897, -0.5278],\n",
       "        [ 0.5874, -0.2771, -0.1186,  0.1162, -0.1798]], device='mps:0')), ('mlp.0.bias', tensor([-0.2210, -0.0922,  0.0218, -0.0149,  0.0388,  0.0097, -0.0049,  0.0214,\n",
       "         0.0274, -0.0262, -0.0031, -0.0345, -0.0632,  0.0060, -0.0450, -0.0114,\n",
       "        -0.0015, -0.0417, -0.2136,  0.0237,  0.0266,  0.0287, -0.0406,  0.0040,\n",
       "        -0.0867,  0.2141, -0.0451, -0.1877, -0.0388, -0.0048, -0.1273,  0.0090],\n",
       "       device='mps:0')), ('mlp.1.weight', tensor([1.0070, 1.3416, 0.8763, 1.1648, 1.0403, 1.2703, 0.7514, 0.9871, 0.8454,\n",
       "        1.1019, 1.0078, 1.0709, 0.9690, 0.8752, 1.0049, 0.8753, 0.4544, 0.8835,\n",
       "        1.2482, 0.8725, 0.9769, 1.2934, 1.2730, 0.9011, 1.0808, 1.1267, 0.9887,\n",
       "        1.2281, 1.1623, 1.1816, 1.0304, 0.7914], device='mps:0')), ('mlp.1.bias', tensor([ 0.3263, -0.2882,  0.0567,  0.1712,  0.5047, -0.2622, -0.3905,  0.0424,\n",
       "        -0.3737, -0.3150,  0.0968, -0.2045,  0.1651, -0.1082,  0.0016, -0.0332,\n",
       "        -0.1410,  0.2086,  0.0231, -0.3571, -0.0857, -0.1547,  0.0502, -0.0144,\n",
       "        -0.2168,  0.3874, -0.0237, -0.2051, -0.1942,  0.2310,  0.1653,  0.1009],\n",
       "       device='mps:0')), ('mlp.1.running_mean', tensor([-0.2207, -0.0904,  0.0235, -0.0118,  0.0383,  0.0112, -0.0047,  0.0210,\n",
       "         0.0271, -0.0271, -0.0017, -0.0352, -0.0627,  0.0069, -0.0451, -0.0065,\n",
       "        -0.0016, -0.0400, -0.2116,  0.0241,  0.0262,  0.0270, -0.0395,  0.0045,\n",
       "        -0.0915,  0.2146, -0.0450, -0.1877, -0.0388, -0.0072, -0.1290,  0.0077],\n",
       "       device='mps:0')), ('mlp.1.running_var', tensor([0.1697, 0.1306, 0.0723, 0.2764, 0.2020, 0.1963, 0.3063, 0.3280, 0.0900,\n",
       "        0.2307, 1.0014, 0.1187, 0.4461, 0.5083, 0.5410, 0.0404, 0.9331, 0.2184,\n",
       "        0.2575, 0.2700, 0.4360, 0.5596, 0.7443, 0.1417, 0.0704, 0.0865, 0.2612,\n",
       "        0.0912, 0.3425, 0.0884, 0.1242, 0.0775], device='mps:0')), ('mlp.1.num_batches_tracked', tensor(5372, device='mps:0')), ('mlp.4.weight', tensor([[-4.4788e-01,  1.5507e-01, -4.2349e-02,  2.6675e-01,  6.0537e-02,\n",
       "         -7.9667e-02, -1.9699e-01,  4.5342e-01,  1.4532e-01,  2.6145e-01,\n",
       "         -4.8157e-01, -6.1084e-02, -4.9855e-01, -3.0023e-01, -2.6574e-01,\n",
       "         -1.5193e-01, -1.8105e-01, -3.0289e-01,  3.7518e-01, -1.3461e-01,\n",
       "          2.9805e-01,  1.4644e-01,  3.3839e-01,  2.9428e-01, -2.3377e-01,\n",
       "         -1.7714e-01, -1.9814e-02, -2.6536e-01, -2.4770e-01,  3.6073e-02,\n",
       "         -4.3022e-01, -3.9283e-02],\n",
       "        [-6.8358e-02, -5.7065e-01, -6.7393e-02,  7.2935e-03, -1.1285e-01,\n",
       "         -3.6685e-01, -7.3630e-02, -2.7179e-01,  3.9001e-03, -2.2227e-01,\n",
       "         -1.0243e-02, -2.1825e-01, -2.8220e-01, -8.8179e-02, -1.5189e-01,\n",
       "         -2.5997e-01,  2.6043e-01, -1.9033e-01,  1.8206e-02, -2.5185e-01,\n",
       "          7.1151e-02,  7.5897e-02, -9.2139e-02, -1.5304e-01, -4.8756e-01,\n",
       "          2.2455e-01, -1.9736e-01, -3.7725e-01, -4.1734e-01,  2.3131e-01,\n",
       "         -6.2006e-01, -3.2902e-01],\n",
       "        [-5.1276e-02, -1.5210e-01, -3.6921e-01,  1.3133e-02, -1.0198e-01,\n",
       "         -2.5379e-01,  3.6744e-01, -1.3743e-01, -2.0687e-01, -1.1191e-01,\n",
       "          3.5176e-01,  2.0914e-01,  2.0340e-01, -1.1806e-01,  1.3347e-01,\n",
       "         -5.5191e-02,  2.3554e-02,  7.4626e-02, -1.5233e-01, -2.6459e-01,\n",
       "         -1.0648e-01, -3.1656e-01, -2.0466e-01, -2.6455e-01, -1.7307e-01,\n",
       "         -3.6960e-01,  9.1526e-02, -2.2832e-01, -1.2329e-01, -8.4180e-01,\n",
       "          1.4152e-01, -1.1120e-02],\n",
       "        [-2.2717e-01,  2.4929e-02, -4.3095e-01,  2.6124e-02, -9.5941e-01,\n",
       "         -7.4519e-01,  3.5355e-01, -4.2373e-01, -5.6389e-02, -4.4166e-01,\n",
       "          3.5871e-01,  2.0507e-01,  5.1721e-02, -1.8362e-02, -5.1665e-02,\n",
       "         -1.9982e-01, -3.2176e-02,  2.3931e-01, -2.3490e-01,  6.0957e-02,\n",
       "          5.8855e-02, -2.1286e-01,  2.0381e-02, -2.5090e-01, -2.0117e-01,\n",
       "         -3.5968e-01,  1.3939e-01, -5.3670e-02, -1.7542e-01, -2.6635e-01,\n",
       "          3.7321e-01,  1.4695e-01],\n",
       "        [-6.8897e-01,  3.1051e-01,  3.3933e-02,  1.4372e-01,  4.5154e-03,\n",
       "          5.5727e-02, -1.2019e-01,  2.7966e-01,  2.9384e-03,  5.3755e-01,\n",
       "         -7.0493e-01, -2.1203e-01, -3.0812e-01, -2.0346e-01, -2.3422e-01,\n",
       "         -1.1408e-01, -2.1701e-01, -2.2243e-01,  2.2226e-01, -1.3282e-01,\n",
       "          2.1960e-01,  1.3824e-01, -6.5565e-02,  1.3458e-02, -2.5716e-01,\n",
       "          3.0920e-01, -4.9763e-02, -1.3735e-01, -1.8494e-01,  3.1038e-01,\n",
       "          6.6666e-02,  2.3154e-01],\n",
       "        [ 3.2773e-01,  5.2083e-02, -2.2807e-01, -2.5895e-01, -1.9259e-01,\n",
       "         -3.1947e-01,  1.4417e-01, -3.3589e-01, -2.5279e-03, -2.8276e-01,\n",
       "          1.7991e-01,  2.8409e-01, -1.2800e-02,  2.1001e-01,  2.9134e-01,\n",
       "         -1.7012e-01,  1.2636e-01,  1.4095e-01, -1.9764e-01, -2.2749e-01,\n",
       "          5.0167e-02, -5.5205e-01, -2.1470e-01,  2.2463e-01, -1.5660e-02,\n",
       "         -6.3930e-01, -1.1971e-01,  1.9260e-01,  1.7081e-01,  4.1751e-02,\n",
       "          6.8750e-02,  1.2062e-01],\n",
       "        [ 1.9724e-01, -4.1746e-02, -4.4931e-01, -2.0384e-01, -5.2013e-01,\n",
       "         -1.8951e-01,  1.1212e-01, -2.6564e-01,  5.1183e-02, -1.0030e-01,\n",
       "          1.1973e-01,  2.8311e-01, -3.9155e-02,  2.3367e-02,  2.7853e-01,\n",
       "         -2.5917e-01,  1.9138e-01, -1.4948e-01, -1.9212e-01, -4.4552e-02,\n",
       "         -2.9638e-01, -2.7296e-01, -2.5859e-01, -4.8905e-03, -1.6488e-01,\n",
       "         -3.8112e-01, -1.1114e-01,  1.6983e-01,  1.1166e-02, -2.1224e-01,\n",
       "          6.0667e-02, -1.2515e-01],\n",
       "        [ 2.4993e-01,  1.2871e-01, -4.8385e-01, -1.9049e-01, -6.7459e-01,\n",
       "         -4.5503e-01,  3.7192e-02, -2.5410e-01, -1.7261e-01, -3.0429e-01,\n",
       "          1.3495e-01,  8.7846e-02, -5.9984e-02, -1.3292e-02,  1.2745e-01,\n",
       "          4.7691e-02,  3.6496e-02,  1.0856e-01, -1.8670e-01, -2.9305e-02,\n",
       "         -3.9384e-02, -3.4858e-01, -1.3465e-01, -5.5803e-02, -1.3737e-01,\n",
       "         -4.1902e-01,  1.2155e-01,  1.2132e-01,  8.2728e-02, -9.9552e-02,\n",
       "          1.4467e-01,  7.8583e-02],\n",
       "        [-1.6067e-01, -4.2170e-01,  1.0272e-02,  2.9314e-01,  2.2968e-01,\n",
       "          2.1547e-02,  8.5379e-02, -3.0275e-02,  1.9583e-01, -2.2163e-01,\n",
       "         -1.4858e-01,  3.4986e-02, -2.0435e-01, -1.1973e-01, -1.2764e-01,\n",
       "         -1.5309e-01, -7.5454e-02, -2.4354e-01,  1.8732e-01,  2.2660e-01,\n",
       "          3.8265e-01,  1.7069e-01,  4.2374e-01, -3.9167e-02, -1.4305e-01,\n",
       "         -7.9055e-02,  3.6732e-01, -3.1990e-01, -1.5097e-01, -7.7802e-02,\n",
       "         -3.1438e-01, -1.1275e-01],\n",
       "        [ 1.0418e-01, -6.1423e-02, -3.5055e-01, -1.4216e-01, -3.3823e-01,\n",
       "         -3.2441e-01, -4.7373e-02, -3.1572e-01, -2.8455e-01, -2.3080e-01,\n",
       "          2.4705e-02,  1.1056e-01,  9.9175e-03, -4.6262e-02, -1.0126e-02,\n",
       "          5.6601e-04, -9.8282e-02,  1.9649e-01, -2.7902e-02, -9.7910e-02,\n",
       "          3.6264e-03, -3.7416e-01, -1.3397e-01, -2.8981e-01, -1.5139e-01,\n",
       "         -4.1559e-01,  8.4960e-02, -1.4354e-01, -3.8743e-01, -8.3920e-01,\n",
       "          7.4087e-02,  6.7132e-02],\n",
       "        [ 1.2621e-01,  1.3834e-01, -5.1546e-01, -3.0999e-01, -1.7277e-01,\n",
       "         -3.2600e-01,  3.8825e-01, -2.3165e-01,  1.1563e-01, -1.0223e-01,\n",
       "          9.7755e-02,  2.4481e-01, -5.4454e-02,  2.2882e-01,  2.2145e-01,\n",
       "         -3.0165e-01, -3.6687e-03,  1.9451e-01, -9.5443e-02, -1.8269e-03,\n",
       "         -1.7107e-01, -1.9094e-01, -5.4877e-02,  4.2985e-02, -2.6625e-02,\n",
       "         -5.5301e-01, -1.0346e-01,  2.3727e-01,  3.9725e-01,  2.7732e-01,\n",
       "          1.9023e-01, -2.0763e-01],\n",
       "        [ 1.0202e-01, -5.6162e-01,  1.3245e-01, -3.0312e-01,  2.5810e-02,\n",
       "         -4.2214e-01, -8.8577e-02, -1.2567e-01,  7.0256e-02, -3.3675e-01,\n",
       "         -5.8804e-02, -2.6514e-01,  7.0579e-03, -1.0418e-01, -2.6529e-01,\n",
       "         -2.9772e-01,  5.0796e-02, -1.4996e-01, -8.2478e-02, -8.4246e-02,\n",
       "          2.2553e-01,  4.3060e-02,  3.0154e-01, -1.9718e-01, -5.1113e-01,\n",
       "          3.8008e-01, -3.7166e-02, -2.6891e-01, -2.0072e-01,  4.6869e-03,\n",
       "         -4.2242e-01, -3.7228e-01],\n",
       "        [-1.2489e-01, -4.5400e-01, -6.3468e-02,  3.0428e-01,  3.8559e-02,\n",
       "         -2.7359e-01,  9.7888e-03, -1.1386e-01,  1.3586e-02, -3.1210e-01,\n",
       "         -1.2116e-01, -8.9934e-02, -3.7020e-01, -2.2904e-01, -1.3664e-01,\n",
       "         -2.0889e-01, -1.8550e-01, -2.1671e-01,  4.4564e-01,  3.9943e-02,\n",
       "         -2.3825e-03, -6.3281e-02,  2.9714e-01,  7.6917e-02, -2.3771e-01,\n",
       "          1.4454e-01,  2.2643e-01, -5.1835e-01, -1.9294e-01,  7.9762e-02,\n",
       "         -4.0481e-01, -1.9591e-01],\n",
       "        [ 1.1072e-02, -4.9106e-02, -5.7497e-01, -3.3104e-01, -9.6414e-01,\n",
       "         -3.2512e-01,  1.1916e-01, -2.9010e-01, -6.6498e-02, -2.7203e-01,\n",
       "          2.5229e-01,  3.6528e-02,  1.1030e-01,  4.9814e-02,  1.4501e-01,\n",
       "          2.0568e-01, -1.9570e-01,  1.9471e-01, -4.4841e-02, -3.8916e-02,\n",
       "         -2.4068e-01, -1.8557e-01, -4.0956e-02, -3.7945e-01, -1.4034e-01,\n",
       "         -3.1033e-01,  3.1580e-01, -1.3987e-01, -1.6703e-01, -6.7686e-02,\n",
       "          4.7752e-02,  1.0207e-02],\n",
       "        [-4.3784e-01,  4.2502e-02, -1.1163e-01,  3.6484e-01,  1.6455e-01,\n",
       "         -2.8748e-01, -5.1820e-02,  2.1973e-01, -1.6303e-02,  1.2750e-01,\n",
       "         -2.5858e-01, -1.2875e-01, -3.4702e-01, -3.6557e-01, -2.1894e-01,\n",
       "         -1.2473e-01, -1.1411e-02, -2.7999e-01,  3.2778e-01, -1.5859e-01,\n",
       "         -3.6536e-02, -5.4584e-02,  4.2734e-02,  1.7754e-01, -3.4859e-01,\n",
       "          2.8068e-01, -1.4918e-01, -4.3494e-01, -2.0221e-01,  2.2086e-01,\n",
       "         -2.3045e-01,  1.3754e-01],\n",
       "        [-1.6784e-01,  1.6417e-01, -9.8158e-02,  2.8837e-01,  1.7148e-01,\n",
       "          2.9374e-01, -5.7446e-02,  8.4078e-02,  9.4343e-02,  3.0471e-01,\n",
       "         -3.5542e-01,  8.5172e-02, -2.4451e-01, -4.2902e-01, -3.3916e-01,\n",
       "         -3.7190e-01, -3.1996e-01, -1.1398e-01,  3.6415e-01,  3.4953e-01,\n",
       "          2.3950e-01,  4.9519e-01,  1.7719e-01,  4.1294e-01, -1.9523e-01,\n",
       "         -1.6355e-01,  3.0067e-01, -5.3779e-02, -6.2222e-02, -9.5736e-02,\n",
       "          8.4394e-02,  1.5317e-01]], device='mps:0')), ('mlp.4.bias', tensor([ 0.0021,  0.0418,  0.1048, -0.0006, -0.0732,  0.0295, -0.0292, -0.0549,\n",
       "         0.1115, -0.0639,  0.0008,  0.0408,  0.0456,  0.0073,  0.0776,  0.0252],\n",
       "       device='mps:0')), ('mlp.5.weight', tensor([0.8687, 0.9597, 0.9262, 0.6868, 0.8349, 0.7567, 0.6483, 0.7909, 1.0050,\n",
       "        0.8870, 0.6832, 0.9863, 1.0192, 0.6708, 0.8641, 1.1209],\n",
       "       device='mps:0')), ('mlp.5.bias', tensor([-0.0019, -0.1218, -0.0713, -0.1811,  0.1044, -0.0842, -0.1713, -0.0710,\n",
       "         0.0283, -0.1158, -0.1491,  0.0447,  0.0859, -0.1324,  0.1170,  0.2161],\n",
       "       device='mps:0')), ('mlp.5.running_mean', tensor([-0.5248, -1.2178, -0.9532, -1.3959, -0.2853, -0.6186, -1.1975, -1.0802,\n",
       "        -0.0090, -1.5308, -0.4565, -0.7693, -0.5328, -1.2605, -0.3206,  0.3332],\n",
       "       device='mps:0')), ('mlp.5.running_var', tensor([6.5950, 2.4908, 3.7508, 5.4668, 6.2667, 7.8746, 5.0125, 5.4344, 4.7212,\n",
       "        4.2162, 4.0131, 1.9382, 3.6337, 5.3196, 4.2427, 7.7339],\n",
       "       device='mps:0')), ('mlp.5.num_batches_tracked', tensor(5372, device='mps:0')), ('mlp.8.weight', tensor([[-0.1597, -0.2229,  0.1295,  0.2012, -0.1554,  0.1548,  0.1352,  0.1753,\n",
       "         -0.1759,  0.1524,  0.2101, -0.2066, -0.2785,  0.1168, -0.2452, -0.2915]],\n",
       "       device='mps:0')), ('mlp.8.bias', tensor([0.0082], device='mps:0'))])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8d110ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  4.7836,  -6.9714,  -8.3270,  -4.3388],\n",
      "         [  5.0638, -15.1471, -11.9996,  -9.9014],\n",
      "         [  3.4292, -12.9364, -14.3504,  -9.3069]]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(X[4].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75e80210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-73.0000, -75.0000, -75.0000, -74.0000, -68.0000, -61.0000, -57.0000,\n",
      "          -59.0000, -52.0000, -59.0000, -58.0000, -59.0000, -60.0000, -65.0000,\n",
      "          -59.0000, -68.0000, -67.0000, -72.0000, -78.0000, -74.0000, -77.0000,\n",
      "          -80.0000, -81.0000, -84.0000, -81.0000, -83.0000, -84.0000, -84.0000,\n",
      "          -83.0000, -77.0000, -79.0000, -76.0000, -80.0000, -82.0000, -81.0000,\n",
      "          -81.0000],\n",
      "         [-27.0000, -27.0000, -24.0000, -20.0000, -12.0000, -14.0000, -13.0000,\n",
      "          -11.0000, -10.0000,   8.0000,  -7.0000,  -9.0000,  -5.0000, -12.0000,\n",
      "           41.0000,  23.0000,  12.0000,   2.0000,  -1.0000, -11.0000,  -2.0000,\n",
      "           -1.0000,  -6.0000,  -3.0000, -15.0000, -13.0000,  -8.0000, -27.0000,\n",
      "          -44.0000, -56.0000, -63.0000, -49.0000, -47.0000, -45.0000, -49.0000,\n",
      "          -48.0000],\n",
      "         [107.5600, 107.0900, 107.5700, 107.7400, 106.6200, 107.4800, 107.6600,\n",
      "          107.8000, 108.7700, 109.0300, 109.2100, 109.3800, 109.7800, 109.9900,\n",
      "          110.3100, 110.6600, 111.4900, 111.9900, 112.5400, 113.0900, 113.6300,\n",
      "          114.0000, 114.4100, 114.7800, 115.5900, 116.1200, 116.6800, 117.2400,\n",
      "          117.9400, 118.2900, 118.6600, 118.9700, 119.2400, 119.6200, 119.8600,\n",
      "          120.1800]]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(M.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8a0fafb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6296, 3, 4])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "29708f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 36])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "48f5ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6f83faab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 32])",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m X = X.to(device)\n\u001b[32m      2\u001b[39m M = M.to(device)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mmodel2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/src/models/gru.py:156\u001b[39m, in \u001b[36mEnsembleGRU.predict_one\u001b[39m\u001b[34m(self, firm, macro)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict_one\u001b[39m(\n\u001b[32m    152\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    153\u001b[39m     firm,\n\u001b[32m    154\u001b[39m     macro\n\u001b[32m    155\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmacro\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     prob = torch.sigmoid(logits).item()\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    159\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mprobability\u001b[39m\u001b[33m\"\u001b[39m: prob,\n\u001b[32m    160\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(prob >= \u001b[38;5;28mself\u001b[39m.threshold)\n\u001b[32m    161\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/src/models/gru.py:149\u001b[39m, in \u001b[36mEnsembleGRU.forward\u001b[39m\u001b[34m(self, firm, macro)\u001b[39m\n\u001b[32m    146\u001b[39m normalized_logits = normalize_logits(logits_tensor)\n\u001b[32m    148\u001b[39m \u001b[38;5;66;03m# context, _ = self.attn(logits_tensor)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized_logits\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py:193\u001b[39m, in \u001b[36m_BatchNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    186\u001b[39m     bn_training = (\u001b[38;5;28mself\u001b[39m.running_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.running_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    188\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[33;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_mean\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/functional.py:2820\u001b[39m, in \u001b[36mbatch_norm\u001b[39m\u001b[34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[39m\n\u001b[32m   2807\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m   2808\u001b[39m         batch_norm,\n\u001b[32m   2809\u001b[39m         (\u001b[38;5;28minput\u001b[39m, running_mean, running_var, weight, bias),\n\u001b[32m   (...)\u001b[39m\u001b[32m   2817\u001b[39m         eps=eps,\n\u001b[32m   2818\u001b[39m     )\n\u001b[32m   2819\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[32m-> \u001b[39m\u001b[32m2820\u001b[39m     \u001b[43m_verify_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2822\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.batch_norm(\n\u001b[32m   2823\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2824\u001b[39m     weight,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2831\u001b[39m     torch.backends.cudnn.enabled,\n\u001b[32m   2832\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torch/nn/functional.py:2786\u001b[39m, in \u001b[36m_verify_batch_size\u001b[39m\u001b[34m(size)\u001b[39m\n\u001b[32m   2784\u001b[39m     size_prods *= size[i + \u001b[32m2\u001b[39m]\n\u001b[32m   2785\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_prods == \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2786\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2787\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected more than 1 value per channel when training, got input size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2788\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Expected more than 1 value per channel when training, got input size torch.Size([1, 32])"
     ]
    }
   ],
   "source": [
    "X = X.to(device)\n",
    "M = M.to(device)\n",
    "\n",
    "model2.predict_one(X[4].unsqueeze(0), M.T.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bfaa30f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
    "device=\"mps\"\n",
    "\n",
    "loader = DataLoader(dataset, batch_size = 1, shuffle = True, collate_fn = collate_with_macro)\n",
    "\n",
    "preds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03acc44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ff12c2b0764ba7826ab7556e1ed860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_uri = \"runs:/ed24e67a8e5b439caf7825229ed3cbe0/model_2025-07-24 09:03:35.937812\"\n",
    "model = mlflow.pytorch.load_model(model_uri=model_uri)\n",
    "\n",
    "device=\"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a75009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.client import MlflowClient\n",
    "\n",
    "def get_best_models(n_models: int, metric: str = \"val_matthews\"):\n",
    "    mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
    "    client = MlflowClient()\n",
    "    \n",
    "    raw_list = mlflow.search_runs(experiment_names=[\"bankruptcy-predictions\"])\n",
    "    runs = raw_list.sort_values(by = \"metrics.\" + metric, ascending = False)\n",
    "    runs.reset_index(inplace=True)\n",
    "    top_runs = runs[:n_models][\"run_id\"]\n",
    "    print(top_runs)\n",
    "    model_paths = []\n",
    "    \n",
    "    for run in top_runs:\n",
    "        artifacts = client.list_artifacts(run)\n",
    "        \n",
    "        for artifact in artifacts:\n",
    "            if artifact.is_dir and artifact.path.startswith(\"GRUModel_\"):\n",
    "                model_name = artifact.path\n",
    "                model_uri = f\"runs:/{run}/{model_name}\"\n",
    "                model_paths.append(model_uri)\n",
    "\n",
    "    print(model_paths)\n",
    "    models = []\n",
    "\n",
    "    for model_uri in model_paths:\n",
    "        model = mlflow.pytorch.load_model(model_uri=model_uri)\n",
    "        models.append(model)\n",
    "        \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0e65ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_models(n_models: int, metric: str = \"val_matthews\"):\n",
    "    mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
    "    client = MlflowClient()\n",
    "    \n",
    "    raw_list = mlflow.search_runs(experiment_names=[\"bankruptcy-predictions\"])\n",
    "    runs = raw_list.sort_values(by = \"metrics.\" + metric, ascending = False)\n",
    "    runs.reset_index(inplace=True)\n",
    "    run_ids = runs[\"run_id\"]\n",
    "    \n",
    "    models = []\n",
    "    current_pool = 0\n",
    "    \n",
    "    for run in run_ids:\n",
    "        try:\n",
    "            artifacts = client.list_artifacts(run)\n",
    "            for artifact in artifacts:\n",
    "                if artifact.is_dir and artifact.path.startswith(\"GRUModel_\"):\n",
    "                    model_name = artifact.path\n",
    "                    model_uri = f\"runs:/{run}/{model_name}\"\n",
    "                    print(f\"Loading model: {model_uri}\")\n",
    "                    model = mlflow.pytorch.load_model(model_uri=model_uri)\n",
    "                    models.append(model)\n",
    "                    \n",
    "                    current_pool += 1\n",
    "                    if current_pool == n_models:\n",
    "                        print(f\"Retrieved {len(models)} models.\")\n",
    "                        return models\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Skipping run {run_id} due to error: {e}\")\n",
    "    \n",
    "    print(f\"Retrieved {len(models)} models.\")\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df058131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GRUModel(\n",
       "   (firm_gru): GRU(4, 32, num_layers=2, batch_first=True)\n",
       "   (macro_gru): GRU(3, 32, num_layers=2, batch_first=True)\n",
       "   (fc): Sequential(\n",
       "     (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "     (1): Dropout(p=0, inplace=False)\n",
       "     (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "     (3): ReLU()\n",
       "     (4): Linear(in_features=32, out_features=1, bias=True)\n",
       "   )\n",
       "   (firm_bn): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "   (macro_bn): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       " ),\n",
       " GRUModel(\n",
       "   (firm_gru): GRU(4, 32, num_layers=2, batch_first=True)\n",
       "   (macro_gru): GRU(3, 32, num_layers=2, batch_first=True)\n",
       "   (fc): Sequential(\n",
       "     (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "     (1): Dropout(p=0, inplace=False)\n",
       "     (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "     (3): ReLU()\n",
       "     (4): Linear(in_features=32, out_features=1, bias=True)\n",
       "   )\n",
       "   (firm_bn): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "   (macro_bn): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       " ),\n",
       " GRUModel(\n",
       "   (firm_gru): GRU(4, 32, num_layers=2, batch_first=True)\n",
       "   (macro_gru): GRU(3, 32, num_layers=2, batch_first=True)\n",
       "   (fc): Sequential(\n",
       "     (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "     (1): Dropout(p=0, inplace=False)\n",
       "     (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "     (3): ReLU()\n",
       "     (4): Linear(in_features=32, out_features=1, bias=True)\n",
       "   )\n",
       "   (firm_bn): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "   (macro_bn): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       " ),\n",
       " GRUModel(\n",
       "   (firm_gru): GRU(4, 32, num_layers=2, batch_first=True)\n",
       "   (macro_gru): GRU(3, 32, num_layers=2, batch_first=True)\n",
       "   (fc): Sequential(\n",
       "     (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "     (1): Dropout(p=0, inplace=False)\n",
       "     (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "     (3): ReLU()\n",
       "     (4): Linear(in_features=32, out_features=1, bias=True)\n",
       "   )\n",
       "   (firm_bn): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "   (macro_bn): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       " ),\n",
       " GRUModel(\n",
       "   (firm_gru): GRU(4, 32, num_layers=2, batch_first=True)\n",
       "   (macro_gru): GRU(3, 32, num_layers=2, batch_first=True)\n",
       "   (fc): Sequential(\n",
       "     (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "     (1): Dropout(p=0, inplace=False)\n",
       "     (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "     (3): ReLU()\n",
       "     (4): Linear(in_features=32, out_features=1, bias=True)\n",
       "   )\n",
       "   (firm_bn): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "   (macro_bn): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       " )]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ddd2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F1': 0.0, 'MCC': 0.04332416132092476, 'loss': nan}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torchmetrics.classification import BinaryF1Score, BinaryMatthewsCorrCoef\n",
    "from train.gru import evaluate_one_epoch\n",
    "\n",
    "loss_fn = BCEWithLogitsLoss()\n",
    "\n",
    "evaluate_one_epoch(\n",
    "    model, loader, loss_fn, device=\"mps\", metrics={\n",
    "        \"F1\": BinaryF1Score().to(device), \n",
    "        \"MCC\": BinaryMatthewsCorrCoef().to(device)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6778e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DynamicNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    A safe replacement for LayerNorm that dynamically uses BatchNorm1d\n",
    "    with the correct feature size on the first forward call.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.bn is None:\n",
    "            feature_dim = x.size(1)\n",
    "            self.bn = nn.BatchNorm1d(feature_dim).to(x.device)\n",
    "        return self.bn(x)\n",
    "\n",
    "def replace_ln_with_dn(module: nn.Module) -> None:\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.LayerNorm):\n",
    "            setattr(module, name, DynamicNorm())\n",
    "        else:\n",
    "            replace_ln_with_dn(child)\n",
    "            \n",
    "for submodel in model.models:\n",
    "    replace_ln_with_dn(submodel.fc)\n",
    "\n",
    "replace_ln_with_dn(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2947d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "import datetime\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Metric, MetricCollection\n",
    "from torchmetrics.classification import (\n",
    "    BinaryAccuracy, BinaryAUROC, BinaryF1Score, BinaryMatthewsCorrCoef,\n",
    "    MulticlassAccuracy, MulticlassAUROC, MulticlassF1Score)\n",
    "from pathlib import Path\n",
    "\n",
    "from src.datasets.dual_input import DualInputSequenceDataset\n",
    "from src.models.gru import GRUModel\n",
    "from src.data.pipeline import IngestionPipeline\n",
    "from src.train import train_model\n",
    "from src.utils.utils import CustomReduceLROnPlateau, collate_with_macro\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "def load_yaml_file(path):\n",
    "    with open(path) as stream:\n",
    "        try:\n",
    "            config_dict=yaml.safe_load(stream)\n",
    "            return config_dict\n",
    "        except yaml.YAMLError as e:\n",
    "            TypeError(f\"Config file could not be loaded: {e}\")\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    firm_data: str\n",
    "    macro_data: list[str]\n",
    "    bankruptcy_col: str\n",
    "    company_col: str\n",
    "    revenue_cap: int = 3_000\n",
    "    num_classes: int = 2\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 40\n",
    "    lr: float = 1e-3\n",
    "    hidden_size: int = 64\n",
    "    num_layers: int = 2\n",
    "    dropout: float = .2\n",
    "    threshold: float = 0.5\n",
    "    scheduler_factor: float=0.85\n",
    "    scheduler_patience: int = 50\n",
    "    min_lr:float = 0.0\n",
    "    decay_ih: float = 1e-5\n",
    "    decay_hh: float = 1e-5\n",
    "    decay_other: float = 1e-5\n",
    "    train_fract: float = .8\n",
    "    seed: int = 42\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "    metrics: list[str] = field(default_factory=lambda: [\"f1\", \"accuracy\"])\n",
    "    \n",
    "    def get_metrics(self) -> MetricCollection:\n",
    "        \"\"\"Constructs a MetricCollection from the specified config\"\"\"\n",
    "        if self.num_classes == 2:\n",
    "            available = {\n",
    "                \"f1\": BinaryF1Score(self.threshold),\n",
    "                \"accuracy\": BinaryAccuracy(self.threshold),\n",
    "                \"auroc\": BinaryAUROC(),\n",
    "                \"matthews\": BinaryMatthewsCorrCoef(self.threshold)\n",
    "            }\n",
    "        else:\n",
    "            available = {\n",
    "                \"f1\": MulticlassF1Score(num_classes=self.num_classes),\n",
    "                \"accuracy\": MulticlassAccuracy(num_classes=self.num_classes),\n",
    "                \"auroc\": MulticlassAUROC(num_classes=self.num_classes)\n",
    "            }\n",
    "        selected = {k: available[k] for k in self.metrics if k in available}\n",
    "        return MetricCollection(selected)\n",
    "    \n",
    "def _make_class_weights(labels: torch.Tensor, num_classes:int) -> torch.Tensor:\n",
    "    \"\"\"Helper function to compute class weights.\"\"\"\n",
    "    counts = torch.bincount(labels.long(), minlength=num_classes)\n",
    "    weights = counts.sum() / (num_classes * counts.float())\n",
    "    return weights\n",
    "    \n",
    "def train_model_from_config(cfg: TrainConfig) -> GRUModel:\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    return train(\n",
    "        company_data_path = Path(cfg.firm_data),\n",
    "        macro_data_path = [Path(path) for path in cfg.macro_data],\n",
    "        bankruptcy_col = cfg.bankruptcy_col,\n",
    "        company_col=cfg.company_col,\n",
    "        revenue_cap=cfg.revenue_cap,\n",
    "        metrics=cfg.get_metrics().to(cfg.device),\n",
    "        device=cfg.device,\n",
    "        num_layers=cfg.num_classes,\n",
    "        hidden_size=cfg.hidden_size,\n",
    "        output_size=cfg.num_classes,\n",
    "        epochs=cfg.epochs,\n",
    "        lr=cfg.lr,\n",
    "        train_fract=cfg.train_fract,\n",
    "        dropout=cfg.dropout,\n",
    "        scheduler_factor=cfg.scheduler_factor,\n",
    "        scheduler_patience=cfg.scheduler_patience,\n",
    "        decay_ih=cfg.decay_ih,\n",
    "        decay_hh=cfg.decay_hh,\n",
    "        decay_other=cfg.decay_other,\n",
    "        seed=cfg.seed\n",
    "    )\n",
    "\n",
    "def train(\n",
    "    company_data_path: str,\n",
    "    macro_data_path: list[str],\n",
    "    bankruptcy_col: str,\n",
    "    company_col: str,\n",
    "    revenue_cap: int,\n",
    "    metrics: list[Metric],\n",
    "    seed: int,\n",
    "    num_layers: int = 2,\n",
    "    hidden_size: int = 64,\n",
    "    output_size: int = 1,\n",
    "    epochs: int = 50,\n",
    "    lr: float = 1e-3,\n",
    "    train_fract: float = 0.8,\n",
    "    dropout: float = 0.2,\n",
    "    scheduler_factor: float = 0.85,\n",
    "    scheduler_patience: int = 50,\n",
    "    min_lr: float = 0.0,\n",
    "    decay_ih:float = 1e-5,\n",
    "    decay_hh:float = 1e-5,\n",
    "    decay_other:float = 1e-5,\n",
    "    device: str=\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "):  \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    ingestion = IngestionPipeline(\n",
    "        company_data_path=company_data_path,\n",
    "        macro_data_path=macro_data_path,\n",
    "        company_col=company_col,\n",
    "        bankruptcy_col=bankruptcy_col,\n",
    "        sheet_name=\"Results\",\n",
    "        revenue_cap=revenue_cap\n",
    "    )\n",
    "    ingestion.load()\n",
    "    series = ingestion.process_data()\n",
    "    financials, macro, labels = series.export_tensors()\n",
    "    \n",
    "    dataset = DualInputSequenceDataset(\n",
    "        firm_tensor = financials,\n",
    "        macro_tensor = macro,\n",
    "        labels = labels\n",
    "    )\n",
    "    \n",
    "    train_ds, val_ds, seed = dataset.stratified_split(train_fract)\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_with_macro)\n",
    "    val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=collate_with_macro)\n",
    "\n",
    "    logger.info(f\"Device: {device}\")\n",
    "    \n",
    "    metrics_dict = dict()\n",
    "    for metric in metrics:\n",
    "        metrics_dict[metric._get_name()] = metric.to(device)\n",
    "    metrics = metrics_dict \n",
    "    \n",
    "    train_ds = train_ds.to_device(device)\n",
    "    val_ds = val_ds.to_device(device)\n",
    "    \n",
    "    firm_input_size, macro_input_size = dataset.input_dims()\n",
    "    \n",
    "    mlflow.set_tracking_uri('http://127.0.0.1:8080')\n",
    "    mlflow.set_experiment('bankruptcy-predictions')\n",
    "    \n",
    "    mlflow.log_param(\"seed\", seed)\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        model = GRUModel(\n",
    "            firm_input_size=firm_input_size,\n",
    "            macro_input_size=macro_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=output_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        model = model.to(device)\n",
    "        \n",
    "        pos_weight = dataset.pos_weight()\n",
    "        loss_fn = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        \n",
    "        # Logging hyperparameters\n",
    "        mlflow.log_param(\"hidden_size\", hidden_size)\n",
    "        mlflow.log_param(\"output_size\", output_size)\n",
    "        mlflow.log_param(\"num_layers\", num_layers)\n",
    "        mlflow.log_param(\"dropout\", dropout)\n",
    "        mlflow.log_param(\"lr\", lr)\n",
    "\n",
    "        ih_params = []\n",
    "        hh_params = []\n",
    "        other_params = []\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                ih_params.append(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                hh_params.append(param)\n",
    "            else:\n",
    "                other_params.append(param)\n",
    "        \n",
    "        optimizer = Adam([\n",
    "                {'params': ih_params, 'weight_decay': decay_ih},\n",
    "                {'params': hh_params, 'weight_decay': decay_hh},\n",
    "                {'params': other_params, 'weight_decay': decay_other},\n",
    "            ], lr=lr\n",
    "        )\n",
    "        scheduler=CustomReduceLROnPlateau(\n",
    "            optimizer=optimizer,\n",
    "            mode=\"min\",\n",
    "            factor=scheduler_factor,\n",
    "            patience=scheduler_patience,\n",
    "            min_lr=min_lr\n",
    "        )\n",
    "        \n",
    "        train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            epochs=epochs,\n",
    "            metrics=metrics\n",
    "        )\n",
    "        \n",
    "        mlflow.pytorch.log_model(model, f\"model_{datetime.datetime.now()}\")\n",
    "        torch.save(obj = model.state_dict(), f = f\"model_{datetime.datetime.now()}.pth\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4150d157",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = load_yaml_file(\"config/model_config.yml\")\n",
    "cfg = TrainConfig(**config_dict)\n",
    "\n",
    "company_data_path = Path(cfg.firm_data)\n",
    "macro_data_path = [Path(path) for path in cfg.macro_data]\n",
    "bankruptcy_col = str(cfg.bankruptcy_col)\n",
    "company_col=str(cfg.company_col)\n",
    "revenue_cap=int(cfg.revenue_cap)\n",
    "metrics=cfg.get_metrics().to(cfg.device)\n",
    "device=str(cfg.device)\n",
    "num_layers=int(cfg.num_classes)\n",
    "hidden_size=int(cfg.hidden_size)\n",
    "output_size=1\n",
    "epochs=int(cfg.epochs)\n",
    "lr=float(cfg.lr)\n",
    "train_fract=float(cfg.train_fract)\n",
    "dropout=int(cfg.dropout)\n",
    "scheduler_factor=float(cfg.scheduler_factor)\n",
    "scheduler_patience=int(cfg.scheduler_patience)\n",
    "decay_ih=float(cfg.decay_ih)\n",
    "decay_hh=float(cfg.decay_hh)\n",
    "decay_other=float(cfg.decay_other)\n",
    "seed=int(cfg.seed)\n",
    "\n",
    "ingestion = IngestionPipeline(\n",
    "    company_path=company_data_path,\n",
    "    macro_paths=macro_data_path,\n",
    "    company_col=company_col,\n",
    "    bankruptcy_col=bankruptcy_col,\n",
    "    revenue_cap=revenue_cap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42cefa80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.loaders:Reading file: data/demo_data.xlsx\n",
      "INFO:src.data.loaders:Dropping high-revenue outliers...\n",
      "INFO:src.data.loaders:Loading 3 macroeconomic series...\n",
      "/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py:126: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"Date\"]=pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
      "/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py:126: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"Date\"]=pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
      "/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py:126: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"Date\"]=pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
      "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "DEBUG:cmdstanpy:input tempfile: /var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/tq29nhl7.json\n",
      "DEBUG:cmdstanpy:input tempfile: /var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/nwfxs8k3.json\n",
      "DEBUG:cmdstanpy:idx 0\n",
      "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
      "DEBUG:cmdstanpy:CmdStan args: ['/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=4432', 'data', 'file=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/tq29nhl7.json', 'init=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/nwfxs8k3.json', 'output', 'file=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/prophet_modelkr1flrrk/prophet_model-20250628192036.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
      "19:20:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "INFO:cmdstanpy:Chain [1] start processing\n",
      "19:20:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "INFO:cmdstanpy:Chain [1] done processing\n",
      "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "DEBUG:cmdstanpy:input tempfile: /var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/wvnm9vzs.json\n",
      "DEBUG:cmdstanpy:input tempfile: /var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/hmdzjanf.json\n",
      "DEBUG:cmdstanpy:idx 0\n",
      "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
      "DEBUG:cmdstanpy:CmdStan args: ['/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=91360', 'data', 'file=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/wvnm9vzs.json', 'init=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/hmdzjanf.json', 'output', 'file=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/prophet_modelaankywie/prophet_model-20250628192036.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
      "19:20:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "INFO:cmdstanpy:Chain [1] start processing\n",
      "19:20:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "INFO:cmdstanpy:Chain [1] done processing\n",
      "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "DEBUG:cmdstanpy:input tempfile: /var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/r6rv40ez.json\n",
      "DEBUG:cmdstanpy:input tempfile: /var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/g4t5ek70.json\n",
      "DEBUG:cmdstanpy:idx 0\n",
      "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
      "DEBUG:cmdstanpy:CmdStan args: ['/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=89630', 'data', 'file=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/r6rv40ez.json', 'init=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/g4t5ek70.json', 'output', 'file=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/prophet_model29jfylq3/prophet_model-20250628192036.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
      "19:20:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "INFO:cmdstanpy:Chain [1] start processing\n",
      "19:20:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "INFO:cmdstanpy:Chain [1] done processing\n",
      "INFO:src.data.feature_engineer:Transforming new data with fitted features...\n",
      "INFO:src.data.feature_engineer:Engineering features...\n"
     ]
    }
   ],
   "source": [
    "ingestion.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8f36ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.tensor_factory:Converting financial series to tensors...\n",
      "INFO:src.data.tensor_factory:Scaling financial data with RobustScaler...\n",
      "INFO:src.data.tensor_factory:Shaped financial data tensor: (6338, 3, 4)\n",
      "INFO:src.data.tensor_factory:Shaped macro data tensor: torch.Size([3, 422])\n"
     ]
    }
   ],
   "source": [
    "X, M, y = ingestion.get_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4da2a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DualInputSequenceDataset(\n",
    "        firm_tensor = X,\n",
    "        macro_tensor = M,\n",
    "        labels = y\n",
    "    )\n",
    "\n",
    "train_ds, val_ds, seed = dataset.stratified_split(train_fract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a4c3f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_with_macro)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=collate_with_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11feb902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2947"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.to(device)\n",
    "train_ds = train_ds.to_device(device)\n",
    "val_ds = val_ds.to_device(device)\n",
    "\n",
    "firm_input_size, macro_input_size = dataset.input_dims()\n",
    "\n",
    "mlflow.set_tracking_uri('http://127.0.0.1:8080')\n",
    "mlflow.set_experiment('bankruptcy-predictions')\n",
    "\n",
    "mlflow.log_param(\"seed\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea8fffc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run caring-tern-39 at: http://127.0.0.1:8080/#/experiments/387584985157093548/runs/53ffe63e18164788be8088bf5f677b88\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/387584985157093548\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf4a53ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 | Loss: 1.00572 | ACCURACY: 0.70158 | AUROC: 0.82659 | F1: 0.07688 | MATTHEWS: 0.13534 | LR: 0.00100:  10%|‚ñà         | 10/100 [00:39<05:51,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[31m2025/06/28 19:21:20 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run painted-sheep-140 at: http://127.0.0.1:8080/#/experiments/387584985157093548/runs/bffc2f977fb04db49aa670bd988c60a5\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/387584985157093548\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    model = GRUModel(\n",
    "        firm_input_size=firm_input_size,\n",
    "        macro_input_size=macro_input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        output_size=output_size,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    pos_weight = dataset.pos_weight()\n",
    "    loss_fn = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    # Logging hyperparameters\n",
    "    mlflow.log_param(\"hidden_size\", hidden_size)\n",
    "    mlflow.log_param(\"output_size\", output_size)\n",
    "    mlflow.log_param(\"num_layers\", num_layers)\n",
    "    mlflow.log_param(\"dropout\", dropout)\n",
    "    mlflow.log_param(\"lr\", lr)\n",
    "\n",
    "    ih_params = []\n",
    "    hh_params = []\n",
    "    other_params = []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight_ih' in name:\n",
    "            ih_params.append(param)\n",
    "        elif 'weight_hh' in name:\n",
    "            hh_params.append(param)\n",
    "        else:\n",
    "            other_params.append(param)\n",
    "    \n",
    "    optimizer = Adam([\n",
    "            {'params': ih_params, 'weight_decay': decay_ih},\n",
    "            {'params': hh_params, 'weight_decay': decay_hh},\n",
    "            {'params': other_params, 'weight_decay': decay_other},\n",
    "        ], lr=lr\n",
    "    )\n",
    "    scheduler=CustomReduceLROnPlateau(\n",
    "        optimizer=optimizer,\n",
    "        mode=\"min\",\n",
    "        factor=scheduler_factor,\n",
    "        patience=scheduler_patience,\n",
    "        min_lr=0.0\n",
    "    )\n",
    "    \n",
    "    train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        epochs=epochs,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    \n",
    "    mlflow.pytorch.log_model(model, f\"model_{datetime.datetime.now()}\")\n",
    "    torch.save(obj = model.state_dict(), f = f\"model_{datetime.datetime.now()}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9525f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6338, 3, 4)\n",
      "torch.Size([3, 422])\n",
      "(6338,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(M.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8db3fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "import datetime\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Metric, MetricCollection\n",
    "from torchmetrics.classification import (\n",
    "    BinaryAccuracy, BinaryAUROC, BinaryF1Score, BinaryMatthewsCorrCoef,\n",
    "    MulticlassAccuracy, MulticlassAUROC, MulticlassF1Score)\n",
    "from pathlib import Path\n",
    "\n",
    "from src.datasets.dual_input import DualInputSequenceDataset\n",
    "from src.models.gru import GRUModel\n",
    "from src.data.pipeline import IngestionPipeline\n",
    "from src.train import train_model\n",
    "from src.utils.utils import CustomReduceLROnPlateau, collate_with_macro\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "def load_yaml_file(path):\n",
    "    with open(path) as stream:\n",
    "        try:\n",
    "            config_dict=yaml.safe_load(stream)\n",
    "            return config_dict\n",
    "        except yaml.YAMLError as e:\n",
    "            TypeError(f\"Config file could not be loaded: {e}\")\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    firm_data: str\n",
    "    macro_data: list[str]\n",
    "    bankruptcy_col: str\n",
    "    company_col: str\n",
    "    revenue_cap: int = 3_000\n",
    "    num_classes: int = 2\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 40\n",
    "    lr: float = 1e-3\n",
    "    hidden_size: int = 64\n",
    "    num_layers: int = 2\n",
    "    dropout: float = .2\n",
    "    threshold: float = 0.5\n",
    "    scheduler_factor: float=0.85\n",
    "    scheduler_patience: int = 50\n",
    "    min_lr:float = 0.0\n",
    "    decay_ih: float = 1e-5\n",
    "    decay_hh: float = 1e-5\n",
    "    decay_other: float = 1e-5\n",
    "    train_fract: float = .8\n",
    "    seed: int = 42\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "    metrics: list[str] = field(default_factory=lambda: [\"f1\", \"accuracy\"])\n",
    "    \n",
    "    def get_metrics(self) -> MetricCollection:\n",
    "        \"\"\"Constructs a MetricCollection from the specified config\"\"\"\n",
    "        if self.num_classes == 2:\n",
    "            available = {\n",
    "                \"f1\": BinaryF1Score(self.threshold),\n",
    "                \"accuracy\": BinaryAccuracy(self.threshold),\n",
    "                \"auroc\": BinaryAUROC(),\n",
    "                \"matthews\": BinaryMatthewsCorrCoef(self.threshold)\n",
    "            }\n",
    "        else:\n",
    "            available = {\n",
    "                \"f1\": MulticlassF1Score(num_classes=self.num_classes),\n",
    "                \"accuracy\": MulticlassAccuracy(num_classes=self.num_classes),\n",
    "                \"auroc\": MulticlassAUROC(num_classes=self.num_classes)\n",
    "            }\n",
    "        selected = {k: available[k] for k in self.metrics if k in available}\n",
    "        return MetricCollection(selected)\n",
    "    \n",
    "def train_model_from_config(cfg: TrainConfig) -> GRUModel:\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    return train(\n",
    "        company_path = Path(cfg.firm_data),\n",
    "        macro_paths = [Path(path) for path in cfg.macro_data],\n",
    "        bankruptcy_col = str(cfg.bankruptcy_col),\n",
    "        company_col=str(cfg.company_col),\n",
    "        revenue_cap=int(cfg.revenue_cap),\n",
    "        metrics=cfg.get_metrics().to(cfg.device),\n",
    "        device=str(cfg.device),\n",
    "        num_layers=int(cfg.num_classes),\n",
    "        hidden_size=int(cfg.hidden_size),\n",
    "        output_size=1,\n",
    "        epochs=int(cfg.epochs),\n",
    "        lr=float(cfg.lr),\n",
    "        train_fract=float(cfg.train_fract),\n",
    "        dropout=int(cfg.dropout),\n",
    "        scheduler_factor=float(cfg.scheduler_factor),\n",
    "        scheduler_patience=int(cfg.scheduler_patience),\n",
    "        decay_ih=float(cfg.decay_ih),\n",
    "        decay_hh=float(cfg.decay_hh),\n",
    "        decay_other=float(cfg.decay_other),\n",
    "        seed=int(cfg.seed)\n",
    "    )\n",
    "\n",
    "def train(\n",
    "    company_path: str,\n",
    "    macro_paths: list[str],\n",
    "    bankruptcy_col: str,\n",
    "    company_col: str,\n",
    "    revenue_cap: int,\n",
    "    metrics: list[Metric],\n",
    "    seed: int,\n",
    "    num_layers: int = 2,\n",
    "    hidden_size: int = 64,\n",
    "    output_size: int = 1,\n",
    "    epochs: int = 50,\n",
    "    lr: float = 1e-3,\n",
    "    train_fract: float = 0.8,\n",
    "    dropout: float = 0.2,\n",
    "    scheduler_factor: float = 0.85,\n",
    "    scheduler_patience: int = 50,\n",
    "    min_lr: float = 0.0,\n",
    "    decay_ih:float = 1e-5,\n",
    "    decay_hh:float = 1e-5,\n",
    "    decay_other:float = 1e-5,\n",
    "    device: str=\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "):  \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    ingestion = IngestionPipeline(\n",
    "        company_path=company_path,\n",
    "        macro_paths=macro_paths,\n",
    "        company_col=company_col,\n",
    "        bankruptcy_col=bankruptcy_col,\n",
    "        revenue_cap=revenue_cap\n",
    "    )\n",
    "    \n",
    "    ingestion.run()\n",
    "    X, M, y = ingestion.get_tensors()\n",
    "    \n",
    "    dataset = DualInputSequenceDataset(\n",
    "        firm_tensor = X,\n",
    "        macro_tensor = M,\n",
    "        labels = y\n",
    "    )\n",
    "    \n",
    "    train_ds, val_ds, seed = dataset.stratified_split(train_fract)\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_with_macro)\n",
    "    val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=collate_with_macro)\n",
    "\n",
    "    logger.info(f\"Device: {device}\")\n",
    "    \n",
    "    metrics.to(device)\n",
    "    train_ds = train_ds.to_device(device)\n",
    "    val_ds = val_ds.to_device(device)\n",
    "    \n",
    "    firm_input_size, macro_input_size = dataset.input_dims()\n",
    "    \n",
    "    mlflow.set_tracking_uri('http://127.0.0.1:8080')\n",
    "    mlflow.set_experiment('bankruptcy-predictions')\n",
    "    \n",
    "    mlflow.log_param(\"seed\", seed)\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        model = GRUModel(\n",
    "            firm_input_size=firm_input_size,\n",
    "            macro_input_size=macro_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=output_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        model = model.to(device)\n",
    "        \n",
    "        pos_weight = dataset.pos_weight()\n",
    "        loss_fn = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        \n",
    "        # Logging hyperparameters\n",
    "        mlflow.log_param(\"hidden_size\", hidden_size)\n",
    "        mlflow.log_param(\"output_size\", output_size)\n",
    "        mlflow.log_param(\"num_layers\", num_layers)\n",
    "        mlflow.log_param(\"dropout\", dropout)\n",
    "        mlflow.log_param(\"lr\", lr)\n",
    "\n",
    "        ih_params = []\n",
    "        hh_params = []\n",
    "        other_params = []\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                ih_params.append(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                hh_params.append(param)\n",
    "            else:\n",
    "                other_params.append(param)\n",
    "        \n",
    "        optimizer = Adam([\n",
    "                {'params': ih_params, 'weight_decay': decay_ih},\n",
    "                {'params': hh_params, 'weight_decay': decay_hh},\n",
    "                {'params': other_params, 'weight_decay': decay_other},\n",
    "            ], lr=lr\n",
    "        )\n",
    "        scheduler=CustomReduceLROnPlateau(\n",
    "            optimizer=optimizer,\n",
    "            mode=\"min\",\n",
    "            factor=scheduler_factor,\n",
    "            patience=scheduler_patience,\n",
    "            min_lr=min_lr\n",
    "        )\n",
    "        \n",
    "        train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            epochs=epochs,\n",
    "            metrics=metrics\n",
    "        )\n",
    "        \n",
    "        mlflow.pytorch.log_model(model, f\"model_{datetime.datetime.now()}\")\n",
    "        torch.save(obj = model.state_dict(), f = f\"model_{datetime.datetime.now()}.pth\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fcc0d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainConfig(firm_data='data/demo_data.xlsx', macro_data=['insee/serie_000857176_04042025/valeurs_mensuelles.csv', 'insee/serie_000857180_04042025/valeurs_mensuelles.csv', 'insee/serie_001763782_04042025/valeurs_mensuelles.csv'], bankruptcy_col='Status date', company_col='Company name Latin alphabet', revenue_cap=3000, num_classes=2, batch_size=32, epochs=100, lr='1e-3', hidden_size=64, num_layers=2, dropout=0.4, threshold=0.4, scheduler_factor=0.85, scheduler_patience=50, min_lr=0.0, decay_ih='1e-5', decay_hh='1e-4', decay_other='1e-5', train_fract=0.8, seed=2025, device='mps', metrics=['f1', 'accuracy', 'auroc', 'matthews'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffb6020b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.loaders:Reading file: data/demo_data.xlsx\n",
      "INFO:src.data.loaders:Dropping high-revenue outliers...\n",
      "INFO:src.data.loaders:Loading 3 macroeconomic series...\n",
      "/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py:126: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"Date\"]=pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
      "/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py:126: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"Date\"]=pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
      "/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py:126: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"Date\"]=pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
      "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "DEBUG:cmdstanpy:input tempfile: /var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/v3to4gc7.json\n",
      "DEBUG:cmdstanpy:input tempfile: /var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/dtv1fhag.json\n",
      "DEBUG:cmdstanpy:idx 0\n",
      "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
      "DEBUG:cmdstanpy:CmdStan args: ['/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=15692', 'data', 'file=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/v3to4gc7.json', 'init=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/dtv1fhag.json', 'output', 'file=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/prophet_modelbgoaxg3a/prophet_model-20250628192627.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
      "19:26:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "INFO:cmdstanpy:Chain [1] start processing\n",
      "19:26:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "INFO:cmdstanpy:Chain [1] done processing\n",
      "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "DEBUG:cmdstanpy:input tempfile: /var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/aa67vq2w.json\n",
      "DEBUG:cmdstanpy:input tempfile: /var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/7j7duo0k.json\n",
      "DEBUG:cmdstanpy:idx 0\n",
      "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
      "DEBUG:cmdstanpy:CmdStan args: ['/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=69653', 'data', 'file=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/aa67vq2w.json', 'init=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/7j7duo0k.json', 'output', 'file=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/prophet_model_lrsx9ak/prophet_model-20250628192627.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
      "19:26:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "INFO:cmdstanpy:Chain [1] start processing\n",
      "19:26:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "INFO:cmdstanpy:Chain [1] done processing\n",
      "INFO:prophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "DEBUG:cmdstanpy:input tempfile: /var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/9xt8mtzs.json\n",
      "DEBUG:cmdstanpy:input tempfile: /var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/gukp79dk.json\n",
      "DEBUG:cmdstanpy:idx 0\n",
      "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
      "DEBUG:cmdstanpy:CmdStan args: ['/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=17199', 'data', 'file=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/9xt8mtzs.json', 'init=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/gukp79dk.json', 'output', 'file=/var/folders/h1/hrjhnsw55w3fh7wq8fc7_bcm0000gn/T/tmpb0h5k1st/prophet_modellrjtpimp/prophet_model-20250628192627.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
      "19:26:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "INFO:cmdstanpy:Chain [1] start processing\n",
      "19:26:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "INFO:cmdstanpy:Chain [1] done processing\n",
      "INFO:src.data.feature_engineer:Transforming new data with fitted features...\n",
      "INFO:src.data.feature_engineer:Engineering features...\n",
      "INFO:src.data.tensor_factory:Converting financial series to tensors...\n",
      "INFO:src.data.tensor_factory:Scaling financial data with RobustScaler...\n",
      "INFO:src.data.tensor_factory:Shaped financial data tensor: (6338, 3, 4)\n",
      "INFO:src.data.tensor_factory:Shaped macro data tensor: torch.Size([3, 422])\n",
      "INFO:__main__:Device: mps\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Run with UUID 06653c474f224c5086793f7ff21e61b6 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_model_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36mtrain_model_from_config\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_model_from_config\u001b[39m(cfg: TrainConfig) -> GRUModel:\n\u001b[32m     83\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Main training function\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompany_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfirm_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmacro_paths\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmacro_data\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevenue_cap\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrevenue_cap\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_fract\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_fract\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscheduler_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscheduler_factor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscheduler_patience\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscheduler_patience\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecay_ih\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecay_ih\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecay_hh\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecay_hh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecay_other\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecay_other\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 167\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(company_path, macro_paths, bankruptcy_col, company_col, revenue_cap, metrics, seed, num_layers, hidden_size, output_size, epochs, lr, train_fract, dropout, scheduler_factor, scheduler_patience, min_lr, decay_ih, decay_hh, decay_other, device)\u001b[39m\n\u001b[32m    163\u001b[39m mlflow.set_experiment(\u001b[33m'\u001b[39m\u001b[33mbankruptcy-predictions\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    165\u001b[39m mlflow.log_param(\u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m, seed)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    168\u001b[39m     model = GRUModel(\n\u001b[32m    169\u001b[39m         firm_input_size=firm_input_size,\n\u001b[32m    170\u001b[39m         macro_input_size=macro_input_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m    174\u001b[39m         dropout=dropout\n\u001b[32m    175\u001b[39m     )\n\u001b[32m    177\u001b[39m     model = model.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/mlflow/tracking/fluent.py:351\u001b[39m, in \u001b[36mstart_run\u001b[39m\u001b[34m(run_id, experiment_id, run_name, nested, parent_run_id, tags, description, log_system_metrics)\u001b[39m\n\u001b[32m    349\u001b[39m experiment_id = \u001b[38;5;28mstr\u001b[39m(experiment_id) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(experiment_id, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m experiment_id\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(active_run_stack) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nested:\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[32m    352\u001b[39m         (\n\u001b[32m    353\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mRun with UUID \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m is already active. To start a new run, first end the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    354\u001b[39m             + \u001b[33m\"\u001b[39m\u001b[33mcurrent run with mlflow.end_run(). To start a nested \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    355\u001b[39m             + \u001b[33m\"\u001b[39m\u001b[33mrun, call start_run with nested=True\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    356\u001b[39m         ).format(active_run_stack[\u001b[32m0\u001b[39m].info.run_id)\n\u001b[32m    357\u001b[39m     )\n\u001b[32m    358\u001b[39m client = MlflowClient()\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_id:\n",
      "\u001b[31mException\u001b[39m: Run with UUID 06653c474f224c5086793f7ff21e61b6 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True"
     ]
    }
   ],
   "source": [
    "train_model_from_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "373daf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run painted-wren-983 at: http://127.0.0.1:8080/#/experiments/387584985157093548/runs/06653c474f224c5086793f7ff21e61b6\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/387584985157093548\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f865a5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to base (Python 3.12.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'logger' from 'logging' (/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/logging/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdual_input\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DualInputSequenceDataset\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgru\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GRUModel\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataIngestionPipeline\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_model, CustomReduceLROnPlateau, collate_with_macro\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/src/data/processing.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtimeseries\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SeriesHandler, MacroTimeSeries, FinancialTimeSeries\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mFeatureEngineer\u001b[39;00m:\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'logger' from 'logging' (/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/logging/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import mlflow\n",
    "import datetime\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Metric, MetricCollection\n",
    "from torchmetrics.classification import (\n",
    "    BinaryAccuracy, BinaryAUROC, BinaryF1Score, BinaryMatthewsCorrCoef,\n",
    "    MulticlassAccuracy, MulticlassAUROC, MulticlassF1Score)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from src.datasets.dual_input import DualInputSequenceDataset\n",
    "from src.models.gru import GRUModel\n",
    "from src.data.pipeline import DataIngestionPipeline\n",
    "from src.train import train_model, CustomReduceLROnPlateau, collate_with_macro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'FeatureEngineer' from partially initialized module 'src.data.processing' (most likely due to a circular import) (/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/processing.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdual_input\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DualInputSequenceDataset\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgru\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GRUModel\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataIngestionPipeline\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_model, CustomReduceLROnPlateau, collate_with_macro\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/src/data/processing.py:7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtimeseries\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SeriesHandler, MacroTimeSeries, FinancialTimeSeries\n\u001b[32m      9\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mFeatureEngineer\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/src/data/timeseries.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RobustScaler\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprophet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Prophet\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FeatureEngineer\n\u001b[32m     12\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     13\u001b[39m logging.basicConfig(level = logging.INFO)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'FeatureEngineer' from partially initialized module 'src.data.processing' (most likely due to a circular import) (/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/processing.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import mlflow\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Metric, MetricCollection\n",
    "from torchmetrics.classification import (\n",
    "    BinaryAccuracy, BinaryAUROC, BinaryF1Score, BinaryMatthewsCorrCoef,\n",
    "    MulticlassAccuracy, MulticlassAUROC, MulticlassF1Score)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from src.datasets.dual_input import DualInputSequenceDataset\n",
    "from src.models.gru import GRUModel\n",
    "from src.data.processing import DataIngestionPipeline\n",
    "from src.train import train_model, CustomReduceLROnPlateau, collate_with_macro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'FeatureEngineer' from partially initialized module 'src.data.processing' (most likely due to a circular import) (/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/processing.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdual_input\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DualInputSequenceDataset\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgru\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GRUModel\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataIngestionPipeline\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_model\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CustomReduceLROnPlateau, collate_with_macro\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/src/data/processing.py:7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtimeseries\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MacroTimeSeries, FinancialTimeSeries\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RobustScaler\n\u001b[32m     10\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/src/data/timeseries.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RobustScaler\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprophet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Prophet\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FeatureEngineer\n\u001b[32m     12\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     13\u001b[39m logging.basicConfig(level = logging.INFO)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'FeatureEngineer' from partially initialized module 'src.data.processing' (most likely due to a circular import) (/Users/guillaumedecina-halmi/Documents/202504 Bankruptcy prediction on restaurants/src/data/processing.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import mlflow\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Metric, MetricCollection\n",
    "from torchmetrics.classification import (\n",
    "    BinaryAccuracy, BinaryAUROC, BinaryF1Score, BinaryMatthewsCorrCoef,\n",
    "    MulticlassAccuracy, MulticlassAUROC, MulticlassF1Score)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from src.datasets.dual_input import DualInputSequenceDataset\n",
    "from src.models.gru import GRUModel\n",
    "from src.data.processing import DataIngestionPipeline\n",
    "from src.train import train_model\n",
    "from src.utils.utils import CustomReduceLROnPlateau, collate_with_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SeriesHandler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdual_input\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DualInputSequenceDataset\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgru\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GRUModel\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataIngestionPipeline\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_model\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CustomReduceLROnPlateau, collate_with_macro\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/src/data/processing.py:113\u001b[39m\n\u001b[32m    110\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    111\u001b[39m             \u001b[38;5;28mself\u001b[39m.expected_columns = json.load(f)\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mDataIngestionPipeline\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompany_data_path\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmacro_data_path\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m    122\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevenue_cap\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m=\u001b[49m\u001b[32;43m3000\u001b[39;49m\n\u001b[32m    123\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompany_col\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/src/data/processing.py:270\u001b[39m, in \u001b[36mDataIngestionPipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    264\u001b[39m     df.sort_index(inplace = \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    266\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.Series(df[\u001b[33m\"\u001b[39m\u001b[33mValue\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess_data\u001b[39m(\n\u001b[32m    269\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     ) -> \u001b[43mSeriesHandler\u001b[49m:\n\u001b[32m    271\u001b[39m     company_ts = FinancialTimeSeries(\u001b[38;5;28mself\u001b[39m.processed_company_data, \u001b[38;5;28mself\u001b[39m.n_years)\n\u001b[32m    273\u001b[39m     macro_ts = []\n",
      "\u001b[31mNameError\u001b[39m: name 'SeriesHandler' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import mlflow\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Metric, MetricCollection\n",
    "from torchmetrics.classification import (\n",
    "    BinaryAccuracy, BinaryAUROC, BinaryF1Score, BinaryMatthewsCorrCoef,\n",
    "    MulticlassAccuracy, MulticlassAUROC, MulticlassF1Score)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from src.datasets.dual_input import DualInputSequenceDataset\n",
    "from src.models.gru import GRUModel\n",
    "from src.data.processing import DataIngestionPipeline\n",
    "from src.train import train_model\n",
    "from src.utils.utils import CustomReduceLROnPlateau, collate_with_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import mlflow\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Metric, MetricCollection\n",
    "from torchmetrics.classification import (\n",
    "    BinaryAccuracy, BinaryAUROC, BinaryF1Score, BinaryMatthewsCorrCoef,\n",
    "    MulticlassAccuracy, MulticlassAUROC, MulticlassF1Score)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from src.datasets.dual_input import DualInputSequenceDataset\n",
    "from src.models.gru import GRUModel\n",
    "from src.data.processing import DataIngestionPipeline\n",
    "from src.train import train_model\n",
    "from src.utils.utils import CustomReduceLROnPlateau, collate_with_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load() missing 1 required positional argument: 'Loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43myaml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfig/model_config.yml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: load() missing 1 required positional argument: 'Loader'"
     ]
    }
   ],
   "source": [
    "yaml.load(\"config/model_config.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Loader.__init__() missing 1 required positional argument: 'stream'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m yaml.load(\u001b[33m\"\u001b[39m\u001b[33mconfig/model_config.yml\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43myaml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mTypeError\u001b[39m: Loader.__init__() missing 1 required positional argument: 'stream'"
     ]
    }
   ],
   "source": [
    "yaml.load(\"config/model_config.yml\", yaml.Loader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'config/model_config.yml'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml.safe_load(\"config/model_config.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'firm_data': 'data/demo_data.xlsx', 'macro_data': ['data/inflation.csv', 'data/gdp.csv', 'data/unemployment.csv'], 'bankruptcy_col': 'bankrupt', 'company_col': 'firm_id', 'revenue_cap': 3000, 'num_classes': 2, 'batch_size': 32, 'epochs': 40, 'lr': 0.001, 'hidden_size': 64, 'num_layers': 2, 'dropout': 0.2, 'scheduler_factor': 0.85, 'scheduler_patience': 50, 'min_lr': 0.0, 'decay_ih': '1e-5', 'decay_hh': '1e-5', 'decay_other': '1e-5', 'train_fract': 0.8, 'seed': 42, 'device': 'cuda', 'metrics': ['f1', 'accuracy', 'auroc']}\n"
     ]
    }
   ],
   "source": [
    "with open(\"config/model_config.yml\") as stream:\n",
    "    print(yaml.safe_load(stream))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config/model_config.yml\") as stream:\n",
    "    data=yaml.safe_load(stream)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'firm_data': 'data/demo_data.xlsx',\n",
       " 'macro_data': ['data/inflation.csv', 'data/gdp.csv', 'data/unemployment.csv'],\n",
       " 'bankruptcy_col': 'bankrupt',\n",
       " 'company_col': 'firm_id',\n",
       " 'revenue_cap': 3000,\n",
       " 'num_classes': 2,\n",
       " 'batch_size': 32,\n",
       " 'epochs': 40,\n",
       " 'lr': 0.001,\n",
       " 'hidden_size': 64,\n",
       " 'num_layers': 2,\n",
       " 'dropout': 0.2,\n",
       " 'scheduler_factor': 0.85,\n",
       " 'scheduler_patience': 50,\n",
       " 'min_lr': 0.0,\n",
       " 'decay_ih': '1e-5',\n",
       " 'decay_hh': '1e-5',\n",
       " 'decay_other': '1e-5',\n",
       " 'train_fract': 0.8,\n",
       " 'seed': 42,\n",
       " 'device': 'cuda',\n",
       " 'metrics': ['f1', 'accuracy', 'auroc']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainConfig:\n",
    "    firm_data: Path\n",
    "    macro_data: list[Path]\n",
    "    bankruptcy_col: str\n",
    "    company_col: str\n",
    "    revenue_cap: int = 3_000\n",
    "    num_classes: int = 2\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 40\n",
    "    lr: float = 1e-3\n",
    "    hidden_size: int = 64\n",
    "    num_layers: int = 2\n",
    "    dropout: float = .2\n",
    "    scheduler_factor: float=0.85\n",
    "    scheduler_patience: int = 50\n",
    "    min_lr:float = 0.0\n",
    "    decay_ih: float = 1e-5\n",
    "    decay_hh: float = 1e-5\n",
    "    decay_other: float = 1e-5\n",
    "    train_fract: float = .8\n",
    "    seed: int = 42\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "    metrics: list[str] = field(default_factory=lambda: [\"f1\", \"accuracy\"])\n",
    "    \n",
    "    def get_metrics(self, *threshold) -> MetricCollection:\n",
    "        \"\"\"Constructs a MetricCollection from the specified config\"\"\"\n",
    "        if self.num_classes == 2:\n",
    "            available = {\n",
    "                \"f1\": BinaryF1Score(threshold),\n",
    "                \"accuracy\": BinaryAccuracy(threshold),\n",
    "                \"auroc\": BinaryAUROC(pos_label = 1),\n",
    "                \"matthews\": BinaryMatthewsCorrCoef(threshold)\n",
    "            }\n",
    "        else:\n",
    "            available = {\n",
    "                \"f1\": MulticlassF1Score(num_classes=self.num_classes),\n",
    "                \"accuracy\": MulticlassAccuracy(num_classes=self.num_classes),\n",
    "                \"auroc\": MulticlassAUROC(num_classes=self.num_classes)\n",
    "            }\n",
    "        selected = {k: available[k] for k in self.metrics if k in available}\n",
    "        return MetricCollection(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config/model_config.yml\") as stream:\n",
    "    data=yaml.safe_load(stream)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    firm_data: str\n",
    "    macro_data: list[str]\n",
    "    bankruptcy_col: str\n",
    "    company_col: str\n",
    "    revenue_cap: int = 3_000\n",
    "    num_classes: int = 2\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 40\n",
    "    lr: float = 1e-3\n",
    "    hidden_size: int = 64\n",
    "    num_layers: int = 2\n",
    "    dropout: float = .2\n",
    "    scheduler_factor: float=0.85\n",
    "    scheduler_patience: int = 50\n",
    "    min_lr:float = 0.0\n",
    "    decay_ih: float = 1e-5\n",
    "    decay_hh: float = 1e-5\n",
    "    decay_other: float = 1e-5\n",
    "    train_fract: float = .8\n",
    "    seed: int = 42\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "    metrics: list[str] = field(default_factory=lambda: [\"f1\", \"accuracy\"])\n",
    "    \n",
    "    def get_metrics(self, *threshold) -> MetricCollection:\n",
    "        \"\"\"Constructs a MetricCollection from the specified config\"\"\"\n",
    "        if self.num_classes == 2:\n",
    "            available = {\n",
    "                \"f1\": BinaryF1Score(threshold),\n",
    "                \"accuracy\": BinaryAccuracy(threshold),\n",
    "                \"auroc\": BinaryAUROC(pos_label = 1),\n",
    "                \"matthews\": BinaryMatthewsCorrCoef(threshold)\n",
    "            }\n",
    "        else:\n",
    "            available = {\n",
    "                \"f1\": MulticlassF1Score(num_classes=self.num_classes),\n",
    "                \"accuracy\": MulticlassAccuracy(num_classes=self.num_classes),\n",
    "                \"auroc\": MulticlassAUROC(num_classes=self.num_classes)\n",
    "            }\n",
    "        selected = {k: available[k] for k in self.metrics if k in available}\n",
    "        return MetricCollection(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m TrainConfig(\u001b[43mconfig\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "TrainConfig(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config/model_config.yml\") as stream:\n",
    "    config=yaml.safe_load(stream)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainConfig.__init__() missing 3 required positional arguments: 'macro_data', 'bankruptcy_col', and 'company_col'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mTrainConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: TrainConfig.__init__() missing 3 required positional arguments: 'macro_data', 'bankruptcy_col', and 'company_col'"
     ]
    }
   ],
   "source": [
    "TrainConfig(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainConfig(firm_data='data/demo_data.xlsx', macro_data=['insee/serie_000857176_04042025/valeurs_mensuelles.csv', 'insee/serie_000857180_04042025/valeurs_mensuelles.csv', 'insee/serie_001763782_04042025/valeurs_mensuelles.csv'], bankruptcy_col='Status date', company_col='Company name Latin alphabet', revenue_cap=3000, num_classes=1, batch_size=32, epochs=100, lr='1e-3', hidden_size=64, num_layers=2, dropout=0.4, scheduler_factor=0.85, scheduler_patience=50, min_lr=0.0, decay_ih='1e-5', decay_hh='1e-4', decay_other='1e-5', train_fract=0.8, seed=2025, device='mps', metrics=['f1', 'accuracy', 'auroc', 'matthews'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainConfig(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_from_config(cfg: TrainConfig) -> GRUModel:\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    return train(\n",
    "        company_data_path = Path(cfg.firm_data),\n",
    "        macro_data_path = [Path(path) for path in cfg.macro_data],\n",
    "        bankruptcy_col = cfg.bankruptcy_col,\n",
    "        company_col=cfg.company_col,\n",
    "        metrics=cfg.get_metrics().to(cfg.device),\n",
    "        device=cfg.device,\n",
    "        num_layers=cfg.num_classes,\n",
    "        hidden_size=cfg.hidden_size,\n",
    "        output_size=cfg.num_classes,\n",
    "        epochs=cfg.epochs,\n",
    "        lr=cfg.lr,\n",
    "        train_fract=cfg.train_fract,\n",
    "        dropout=cfg.dropout,\n",
    "        scheduler_factor=cfg.scheduler_factor,\n",
    "        scheduler_patience=cfg.scheduler_patience,\n",
    "        decay_ih=cfg.decay_ih,\n",
    "        decay_hh=cfg.decay_hh,\n",
    "        decay_other=cfg.decay_other,\n",
    "        seed=cfg.seed\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_model_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrainConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mtrain_model_from_config\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_model_from_config\u001b[39m(cfg: TrainConfig) -> GRUModel:\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Main training function\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain\u001b[49m(\n\u001b[32m      4\u001b[39m         company_data_path = Path(cfg.firm_data),\n\u001b[32m      5\u001b[39m         macro_data_path = [Path(path) \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m cfg.macro_data],\n\u001b[32m      6\u001b[39m         bankruptcy_col = cfg.bankruptcy_col,\n\u001b[32m      7\u001b[39m         company_col=cfg.company_col,\n\u001b[32m      8\u001b[39m         metrics=cfg.get_metrics().to(cfg.device),\n\u001b[32m      9\u001b[39m         device=cfg.device,\n\u001b[32m     10\u001b[39m         num_layers=cfg.num_classes,\n\u001b[32m     11\u001b[39m         hidden_size=cfg.hidden_size,\n\u001b[32m     12\u001b[39m         output_size=cfg.num_classes,\n\u001b[32m     13\u001b[39m         epochs=cfg.epochs,\n\u001b[32m     14\u001b[39m         lr=cfg.lr,\n\u001b[32m     15\u001b[39m         train_fract=cfg.train_fract,\n\u001b[32m     16\u001b[39m         dropout=cfg.dropout,\n\u001b[32m     17\u001b[39m         scheduler_factor=cfg.scheduler_factor,\n\u001b[32m     18\u001b[39m         scheduler_patience=cfg.scheduler_patience,\n\u001b[32m     19\u001b[39m         decay_ih=cfg.decay_ih,\n\u001b[32m     20\u001b[39m         decay_hh=cfg.decay_hh,\n\u001b[32m     21\u001b[39m         decay_other=cfg.decay_other,\n\u001b[32m     22\u001b[39m         seed=cfg.seed\n\u001b[32m     23\u001b[39m     )\n",
      "\u001b[31mNameError\u001b[39m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train_model_from_config(TrainConfig(**config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(\n",
    "    company_data_path: str,\n",
    "    macro_data_path: list[str],\n",
    "    bankruptcy_col: str,\n",
    "    company_col: str,\n",
    "    revenue_cap: int,\n",
    "    metrics: list[Metric],\n",
    "    seed: int,\n",
    "    num_layers: int = 2,\n",
    "    hidden_size: int = 64,\n",
    "    output_size: int = 1,\n",
    "    epochs: int = 50,\n",
    "    lr: float = 1e-3,\n",
    "    train_fract: float = 0.8,\n",
    "    dropout: float = 0.2,\n",
    "    scheduler_factor: float = 0.85,\n",
    "    scheduler_patience: int = 50,\n",
    "    min_lr: float = 0.0,\n",
    "    decay_ih:float = 1e-5,\n",
    "    decay_hh:float = 1e-5,\n",
    "    decay_other:float = 1e-5,\n",
    "):  \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    ingestion = DataIngestionPipeline(\n",
    "        company_data_path=company_data_path,\n",
    "        macro_data_path=macro_data_path,\n",
    "        company_col=company_col,\n",
    "        bankruptcy_col=bankruptcy_col,\n",
    "        sheet_name=\"Results\",\n",
    "        revenue_cap=revenue_cap\n",
    "    )\n",
    "    ingestion.load()\n",
    "    series = ingestion.process_data()\n",
    "    financials, macro, labels = series.export_tensors()\n",
    "    \n",
    "    dataset = DualInputSequenceDataset(\n",
    "        firm_tensor = financials,\n",
    "        macro_tensor = macro,\n",
    "        labels = labels\n",
    "    )\n",
    "    \n",
    "    train_ds, val_ds, seed = dataset.stratified_split(train_fract)\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_with_macro)\n",
    "    val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=collate_with_macro)\n",
    "\n",
    "    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu') # Using hardware acceleration if on Mac\n",
    "    logger.info(f\"Device: {device}\")\n",
    "    \n",
    "    metrics_dict = dict()\n",
    "    for metric in metrics:\n",
    "        metrics_dict[metric._get_name()] = metric.to(device)\n",
    "    metrics = metrics_dict \n",
    "    \n",
    "    train_ds = train_ds.to_device(device)\n",
    "    val_ds = val_ds.to_device(device)\n",
    "    \n",
    "    firm_input_size, macro_input_size = dataset.input_dims()\n",
    "    \n",
    "    mlflow.set_tracking_uri('http://127.0.0.1:8080')\n",
    "    mlflow.set_experiment('bankruptcy-predictions')\n",
    "    \n",
    "    mlflow.log_param(\"seed\", seed)\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        model = GRUModel(\n",
    "            firm_input_size=firm_input_size,\n",
    "            macro_input_size=macro_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=output_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        model = model.to(device)\n",
    "        \n",
    "        pos_weight = dataset.pos_weight()\n",
    "        loss_fn = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        \n",
    "        # Logging hyperparameters\n",
    "        mlflow.log_param(\"hidden_size\", hidden_size)\n",
    "        mlflow.log_param(\"output_size\", output_size)\n",
    "        mlflow.log_param(\"num_layers\", num_layers)\n",
    "        mlflow.log_param(\"dropout\", dropout)\n",
    "        mlflow.log_param(\"lr\", lr)\n",
    "\n",
    "        ih_params = []\n",
    "        hh_params = []\n",
    "        other_params = []\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                ih_params.append(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                hh_params.append(param)\n",
    "            else:\n",
    "                other_params.append(param)\n",
    "        \n",
    "        optimizer = Adam([\n",
    "                {'params': ih_params, 'weight_decay': decay_ih},\n",
    "                {'params': hh_params, 'weight_decay': decay_hh},\n",
    "                {'params': other_params, 'weight_decay': decay_other},\n",
    "            ], lr=lr\n",
    "        )\n",
    "        scheduler=CustomReduceLROnPlateau(\n",
    "            optimizer=optimizer,\n",
    "            mode=\"min\",\n",
    "            factor=scheduler_factor,\n",
    "            patience=scheduler_patience,\n",
    "            min_lr=min_lr\n",
    "        )\n",
    "        \n",
    "        train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            epochs=epochs,\n",
    "            metrics=metrics\n",
    "        )\n",
    "        \n",
    "        mlflow.pytorch.log_model(model, f\"model_{datetime.datetime.now()}\")\n",
    "        torch.save(obj = model.state_dict(), f = f\"model_{datetime.datetime.now()}.pth\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected argument `num_classes` to be an integer larger than 1, but got 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_model_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrainConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mtrain_model_from_config\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_model_from_config\u001b[39m(cfg: TrainConfig) -> GRUModel:\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Main training function\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m train(\n\u001b[32m      4\u001b[39m         company_data_path = Path(cfg.firm_data),\n\u001b[32m      5\u001b[39m         macro_data_path = [Path(path) \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m cfg.macro_data],\n\u001b[32m      6\u001b[39m         bankruptcy_col = cfg.bankruptcy_col,\n\u001b[32m      7\u001b[39m         company_col=cfg.company_col,\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m         metrics=\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.to(cfg.device),\n\u001b[32m      9\u001b[39m         device=cfg.device,\n\u001b[32m     10\u001b[39m         num_layers=cfg.num_classes,\n\u001b[32m     11\u001b[39m         hidden_size=cfg.hidden_size,\n\u001b[32m     12\u001b[39m         output_size=cfg.num_classes,\n\u001b[32m     13\u001b[39m         epochs=cfg.epochs,\n\u001b[32m     14\u001b[39m         lr=cfg.lr,\n\u001b[32m     15\u001b[39m         train_fract=cfg.train_fract,\n\u001b[32m     16\u001b[39m         dropout=cfg.dropout,\n\u001b[32m     17\u001b[39m         scheduler_factor=cfg.scheduler_factor,\n\u001b[32m     18\u001b[39m         scheduler_patience=cfg.scheduler_patience,\n\u001b[32m     19\u001b[39m         decay_ih=cfg.decay_ih,\n\u001b[32m     20\u001b[39m         decay_hh=cfg.decay_hh,\n\u001b[32m     21\u001b[39m         decay_other=cfg.decay_other,\n\u001b[32m     22\u001b[39m         seed=cfg.seed\n\u001b[32m     23\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mTrainConfig.get_metrics\u001b[39m\u001b[34m(self, *threshold)\u001b[39m\n\u001b[32m     29\u001b[39m     available = {\n\u001b[32m     30\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m: BinaryF1Score(threshold),\n\u001b[32m     31\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: BinaryAccuracy(threshold),\n\u001b[32m     32\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mauroc\u001b[39m\u001b[33m\"\u001b[39m: BinaryAUROC(pos_label = \u001b[32m1\u001b[39m),\n\u001b[32m     33\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmatthews\u001b[39m\u001b[33m\"\u001b[39m: BinaryMatthewsCorrCoef(threshold)\n\u001b[32m     34\u001b[39m     }\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     36\u001b[39m     available = {\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mMulticlassF1Score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     38\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: MulticlassAccuracy(num_classes=\u001b[38;5;28mself\u001b[39m.num_classes),\n\u001b[32m     39\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mauroc\u001b[39m\u001b[33m\"\u001b[39m: MulticlassAUROC(num_classes=\u001b[38;5;28mself\u001b[39m.num_classes)\n\u001b[32m     40\u001b[39m     }\n\u001b[32m     41\u001b[39m selected = {k: available[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metrics \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m available}\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m MetricCollection(selected)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torchmetrics/classification/f_beta.py:861\u001b[39m, in \u001b[36mMulticlassF1Score.__init__\u001b[39m\u001b[34m(self, num_classes, top_k, average, multidim_average, ignore_index, validate_args, zero_division, **kwargs)\u001b[39m\n\u001b[32m    850\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    851\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    852\u001b[39m     num_classes: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    859\u001b[39m     **kwargs: Any,\n\u001b[32m    860\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m=\u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmultidim_average\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultidim_average\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torchmetrics/classification/f_beta.py:339\u001b[39m, in \u001b[36mMulticlassFBetaScore.__init__\u001b[39m\u001b[34m(self, beta, num_classes, top_k, average, multidim_average, ignore_index, validate_args, zero_division, **kwargs)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    330\u001b[39m     num_classes=num_classes,\n\u001b[32m    331\u001b[39m     top_k=top_k,\n\u001b[32m   (...)\u001b[39m\u001b[32m    336\u001b[39m     **kwargs,\n\u001b[32m    337\u001b[39m )\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m validate_args:\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m     \u001b[43m_multiclass_fbeta_score_arg_validation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultidim_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_division\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[38;5;28mself\u001b[39m.validate_args = validate_args\n\u001b[32m    343\u001b[39m \u001b[38;5;28mself\u001b[39m.zero_division = zero_division\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torchmetrics/functional/classification/f_beta.py:164\u001b[39m, in \u001b[36m_multiclass_fbeta_score_arg_validation\u001b[39m\u001b[34m(beta, num_classes, top_k, average, multidim_average, ignore_index, zero_division)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(beta, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m beta > \u001b[32m0\u001b[39m):\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected argument `beta` to be a float larger than 0, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbeta\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m \u001b[43m_multiclass_stat_scores_arg_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultidim_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torchmetrics/functional/classification/stat_scores.py:243\u001b[39m, in \u001b[36m_multiclass_stat_scores_arg_validation\u001b[39m\u001b[34m(num_classes, top_k, average, multidim_average, ignore_index, zero_division)\u001b[39m\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    240\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mArgument `num_classes` can only be `None` for `average=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmicro\u001b[39m\u001b[33m'\u001b[39m\u001b[33m`, but got `average=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    241\u001b[39m     )\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_classes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(num_classes, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m num_classes < \u001b[32m2\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected argument `num_classes` to be an integer larger than 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(top_k, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m top_k < \u001b[32m1\u001b[39m:\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected argument `top_k` to be an integer larger than or equal to 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Expected argument `num_classes` to be an integer larger than 1, but got 1"
     ]
    }
   ],
   "source": [
    "train_model_from_config(TrainConfig(**config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'get_metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_metrics\u001b[49m()\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'get_metrics'"
     ]
    }
   ],
   "source": [
    "config.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainConfig(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected argument `num_classes` to be an integer larger than 1, but got 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mTrainConfig.get_metrics\u001b[39m\u001b[34m(self, *threshold)\u001b[39m\n\u001b[32m     29\u001b[39m     available = {\n\u001b[32m     30\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m: BinaryF1Score(threshold),\n\u001b[32m     31\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: BinaryAccuracy(threshold),\n\u001b[32m     32\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mauroc\u001b[39m\u001b[33m\"\u001b[39m: BinaryAUROC(pos_label = \u001b[32m1\u001b[39m),\n\u001b[32m     33\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmatthews\u001b[39m\u001b[33m\"\u001b[39m: BinaryMatthewsCorrCoef(threshold)\n\u001b[32m     34\u001b[39m     }\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     36\u001b[39m     available = {\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mMulticlassF1Score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     38\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: MulticlassAccuracy(num_classes=\u001b[38;5;28mself\u001b[39m.num_classes),\n\u001b[32m     39\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mauroc\u001b[39m\u001b[33m\"\u001b[39m: MulticlassAUROC(num_classes=\u001b[38;5;28mself\u001b[39m.num_classes)\n\u001b[32m     40\u001b[39m     }\n\u001b[32m     41\u001b[39m selected = {k: available[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metrics \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m available}\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m MetricCollection(selected)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torchmetrics/classification/f_beta.py:861\u001b[39m, in \u001b[36mMulticlassF1Score.__init__\u001b[39m\u001b[34m(self, num_classes, top_k, average, multidim_average, ignore_index, validate_args, zero_division, **kwargs)\u001b[39m\n\u001b[32m    850\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    851\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    852\u001b[39m     num_classes: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    859\u001b[39m     **kwargs: Any,\n\u001b[32m    860\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m=\u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmultidim_average\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultidim_average\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torchmetrics/classification/f_beta.py:339\u001b[39m, in \u001b[36mMulticlassFBetaScore.__init__\u001b[39m\u001b[34m(self, beta, num_classes, top_k, average, multidim_average, ignore_index, validate_args, zero_division, **kwargs)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    330\u001b[39m     num_classes=num_classes,\n\u001b[32m    331\u001b[39m     top_k=top_k,\n\u001b[32m   (...)\u001b[39m\u001b[32m    336\u001b[39m     **kwargs,\n\u001b[32m    337\u001b[39m )\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m validate_args:\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m     \u001b[43m_multiclass_fbeta_score_arg_validation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultidim_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_division\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[38;5;28mself\u001b[39m.validate_args = validate_args\n\u001b[32m    343\u001b[39m \u001b[38;5;28mself\u001b[39m.zero_division = zero_division\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torchmetrics/functional/classification/f_beta.py:164\u001b[39m, in \u001b[36m_multiclass_fbeta_score_arg_validation\u001b[39m\u001b[34m(beta, num_classes, top_k, average, multidim_average, ignore_index, zero_division)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(beta, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m beta > \u001b[32m0\u001b[39m):\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected argument `beta` to be a float larger than 0, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbeta\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m \u001b[43m_multiclass_stat_scores_arg_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultidim_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torchmetrics/functional/classification/stat_scores.py:243\u001b[39m, in \u001b[36m_multiclass_stat_scores_arg_validation\u001b[39m\u001b[34m(num_classes, top_k, average, multidim_average, ignore_index, zero_division)\u001b[39m\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    240\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mArgument `num_classes` can only be `None` for `average=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmicro\u001b[39m\u001b[33m'\u001b[39m\u001b[33m`, but got `average=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    241\u001b[39m     )\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_classes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(num_classes, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m num_classes < \u001b[32m2\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected argument `num_classes` to be an integer larger than 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(top_k, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m top_k < \u001b[32m1\u001b[39m:\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected argument `top_k` to be an integer larger than or equal to 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Expected argument `num_classes` to be an integer larger than 1, but got 1"
     ]
    }
   ],
   "source": [
    "config.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected argument `num_classes` to be an integer larger than 1, but got 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mTrainConfig.get_metrics\u001b[39m\u001b[34m(self, *threshold)\u001b[39m\n\u001b[32m     29\u001b[39m     available = {\n\u001b[32m     30\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m: BinaryF1Score(threshold),\n\u001b[32m     31\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: BinaryAccuracy(threshold),\n\u001b[32m     32\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mauroc\u001b[39m\u001b[33m\"\u001b[39m: BinaryAUROC(pos_label = \u001b[32m1\u001b[39m),\n\u001b[32m     33\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmatthews\u001b[39m\u001b[33m\"\u001b[39m: BinaryMatthewsCorrCoef(threshold)\n\u001b[32m     34\u001b[39m     }\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     36\u001b[39m     available = {\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mMulticlassF1Score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     38\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: MulticlassAccuracy(num_classes=\u001b[38;5;28mself\u001b[39m.num_classes),\n\u001b[32m     39\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mauroc\u001b[39m\u001b[33m\"\u001b[39m: MulticlassAUROC(num_classes=\u001b[38;5;28mself\u001b[39m.num_classes)\n\u001b[32m     40\u001b[39m     }\n\u001b[32m     41\u001b[39m selected = {k: available[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metrics \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m available}\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m MetricCollection(selected)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torchmetrics/classification/f_beta.py:861\u001b[39m, in \u001b[36mMulticlassF1Score.__init__\u001b[39m\u001b[34m(self, num_classes, top_k, average, multidim_average, ignore_index, validate_args, zero_division, **kwargs)\u001b[39m\n\u001b[32m    850\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    851\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    852\u001b[39m     num_classes: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    859\u001b[39m     **kwargs: Any,\n\u001b[32m    860\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m=\u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmultidim_average\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultidim_average\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torchmetrics/classification/f_beta.py:339\u001b[39m, in \u001b[36mMulticlassFBetaScore.__init__\u001b[39m\u001b[34m(self, beta, num_classes, top_k, average, multidim_average, ignore_index, validate_args, zero_division, **kwargs)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    330\u001b[39m     num_classes=num_classes,\n\u001b[32m    331\u001b[39m     top_k=top_k,\n\u001b[32m   (...)\u001b[39m\u001b[32m    336\u001b[39m     **kwargs,\n\u001b[32m    337\u001b[39m )\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m validate_args:\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m     \u001b[43m_multiclass_fbeta_score_arg_validation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultidim_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_division\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[38;5;28mself\u001b[39m.validate_args = validate_args\n\u001b[32m    343\u001b[39m \u001b[38;5;28mself\u001b[39m.zero_division = zero_division\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torchmetrics/functional/classification/f_beta.py:164\u001b[39m, in \u001b[36m_multiclass_fbeta_score_arg_validation\u001b[39m\u001b[34m(beta, num_classes, top_k, average, multidim_average, ignore_index, zero_division)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(beta, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m beta > \u001b[32m0\u001b[39m):\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected argument `beta` to be a float larger than 0, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbeta\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m \u001b[43m_multiclass_stat_scores_arg_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultidim_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torchmetrics/functional/classification/stat_scores.py:243\u001b[39m, in \u001b[36m_multiclass_stat_scores_arg_validation\u001b[39m\u001b[34m(num_classes, top_k, average, multidim_average, ignore_index, zero_division)\u001b[39m\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    240\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mArgument `num_classes` can only be `None` for `average=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmicro\u001b[39m\u001b[33m'\u001b[39m\u001b[33m`, but got `average=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    241\u001b[39m     )\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_classes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(num_classes, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m num_classes < \u001b[32m2\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected argument `num_classes` to be an integer larger than 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(top_k, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m top_k < \u001b[32m1\u001b[39m:\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected argument `top_k` to be an integer larger than or equal to 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Expected argument `num_classes` to be an integer larger than 1, but got 1"
     ]
    }
   ],
   "source": [
    "config.get_metrics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config/model_config.yml\") as stream:\n",
    "    config=yaml.safe_load(stream)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainConfig(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected argument `threshold` to be a float in the [0,1] range, but got ().",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mTrainConfig.get_metrics\u001b[39m\u001b[34m(self, *threshold)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Constructs a MetricCollection from the specified config\"\"\"\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_classes == \u001b[32m2\u001b[39m:\n\u001b[32m     29\u001b[39m     available = {\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mBinaryF1Score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     31\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: BinaryAccuracy(threshold),\n\u001b[32m     32\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mauroc\u001b[39m\u001b[33m\"\u001b[39m: BinaryAUROC(pos_label = \u001b[32m1\u001b[39m),\n\u001b[32m     33\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmatthews\u001b[39m\u001b[33m\"\u001b[39m: BinaryMatthewsCorrCoef(threshold)\n\u001b[32m     34\u001b[39m     }\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     36\u001b[39m     available = {\n\u001b[32m     37\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m: MulticlassF1Score(num_classes=\u001b[38;5;28mself\u001b[39m.num_classes),\n\u001b[32m     38\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: MulticlassAccuracy(num_classes=\u001b[38;5;28mself\u001b[39m.num_classes),\n\u001b[32m     39\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mauroc\u001b[39m\u001b[33m\"\u001b[39m: MulticlassAUROC(num_classes=\u001b[38;5;28mself\u001b[39m.num_classes)\n\u001b[32m     40\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torchmetrics/classification/f_beta.py:686\u001b[39m, in \u001b[36mBinaryF1Score.__init__\u001b[39m\u001b[34m(self, threshold, multidim_average, ignore_index, validate_args, zero_division, **kwargs)\u001b[39m\n\u001b[32m    677\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    678\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    679\u001b[39m     threshold: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.5\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    684\u001b[39m     **kwargs: Any,\n\u001b[32m    685\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m686\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmultidim_average\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultidim_average\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[43m=\u001b[49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torchmetrics/classification/f_beta.py:143\u001b[39m, in \u001b[36mBinaryFBetaScore.__init__\u001b[39m\u001b[34m(self, beta, threshold, multidim_average, ignore_index, validate_args, zero_division, **kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    136\u001b[39m     threshold=threshold,\n\u001b[32m    137\u001b[39m     multidim_average=multidim_average,\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m     **kwargs,\n\u001b[32m    141\u001b[39m )\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m validate_args:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[43m_binary_fbeta_score_arg_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultidim_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;28mself\u001b[39m.validate_args = validate_args\n\u001b[32m    145\u001b[39m \u001b[38;5;28mself\u001b[39m.zero_division = zero_division\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torchmetrics/functional/classification/f_beta.py:70\u001b[39m, in \u001b[36m_binary_fbeta_score_arg_validation\u001b[39m\u001b[34m(beta, threshold, multidim_average, ignore_index, zero_division)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(beta, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m beta > \u001b[32m0\u001b[39m):\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected argument `beta` to be a float larger than 0, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbeta\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[43m_binary_stat_scores_arg_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultidim_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torchmetrics/functional/classification/stat_scores.py:41\u001b[39m, in \u001b[36m_binary_stat_scores_arg_validation\u001b[39m\u001b[34m(threshold, multidim_average, ignore_index, zero_division)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Validate non tensor input.\u001b[39;00m\n\u001b[32m     33\u001b[39m \n\u001b[32m     34\u001b[39m \u001b[33;03m- ``threshold`` has to be a float in the [0,1] range\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(threshold, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[32m0\u001b[39m <= threshold <= \u001b[32m1\u001b[39m)):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected argument `threshold` to be a float in the [0,1] range, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthreshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m allowed_multidim_average = (\u001b[33m\"\u001b[39m\u001b[33mglobal\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msamplewise\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m multidim_average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_multidim_average:\n",
      "\u001b[31mValueError\u001b[39m: Expected argument `threshold` to be a float in the [0,1] range, but got ()."
     ]
    }
   ],
   "source": [
    "config.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__main__.TrainConfig() argument after ** must be a mapping, not TrainConfig",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m config = TrainConfig(**config)\n\u001b[32m      2\u001b[39m config.get_metrics(**config)\n",
      "\u001b[31mTypeError\u001b[39m: __main__.TrainConfig() argument after ** must be a mapping, not TrainConfig"
     ]
    }
   ],
   "source": [
    "config = TrainConfig(**config)\n",
    "config.get_metrics(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainConfig.__init__() got an unexpected keyword argument 'threshold'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mconfig/model_config.yml\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[32m      2\u001b[39m     config=yaml.safe_load(stream)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m config = \u001b[43mTrainConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[32m      4\u001b[39m config.get_metrics()\n",
      "\u001b[31mTypeError\u001b[39m: TrainConfig.__init__() got an unexpected keyword argument 'threshold'"
     ]
    }
   ],
   "source": [
    "with open(\"config/model_config.yml\") as stream:\n",
    "    config=yaml.safe_load(stream)\n",
    "config = TrainConfig(**config)    \n",
    "config.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainConfig.__init__() got an unexpected keyword argument 'threshold'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mconfig/model_config.yml\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[32m      2\u001b[39m     config=yaml.safe_load(stream)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m config = \u001b[43mTrainConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[32m      4\u001b[39m config.get_metrics()\n",
      "\u001b[31mTypeError\u001b[39m: TrainConfig.__init__() got an unexpected keyword argument 'threshold'"
     ]
    }
   ],
   "source": [
    "with open(\"config/model_config.yml\") as stream:\n",
    "    config=yaml.safe_load(stream)\n",
    "config = TrainConfig(**config)    \n",
    "config.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "import datetime\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Metric, MetricCollection\n",
    "from torchmetrics.classification import (\n",
    "    BinaryAccuracy, BinaryAUROC, BinaryF1Score, BinaryMatthewsCorrCoef,\n",
    "    MulticlassAccuracy, MulticlassAUROC, MulticlassF1Score)\n",
    "from pathlib import Path\n",
    "\n",
    "from src.datasets.dual_input import DualInputSequenceDataset\n",
    "from src.models.gru import GRUModel\n",
    "from src.data.processing import DataIngestionPipeline\n",
    "from src.train import train_model\n",
    "from src.utils.utils import CustomReduceLROnPlateau, collate_with_macro\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "def load_yaml_file(path):\n",
    "    with open(path) as stream:\n",
    "        try:\n",
    "            config_dict=yaml.safe_load(stream)\n",
    "            return config_dict\n",
    "        except yaml.YAMLError as e:\n",
    "            TypeError(f\"Config file could not be loaded: {e}\")\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    firm_data: str\n",
    "    macro_data: list[str]\n",
    "    bankruptcy_col: str\n",
    "    company_col: str\n",
    "    revenue_cap: int = 3_000\n",
    "    num_classes: int = 2\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 40\n",
    "    lr: float = 1e-3\n",
    "    hidden_size: int = 64\n",
    "    num_layers: int = 2\n",
    "    dropout: float = .2\n",
    "    threshold: float = 0.5\n",
    "    scheduler_factor: float=0.85\n",
    "    scheduler_patience: int = 50\n",
    "    min_lr:float = 0.0\n",
    "    decay_ih: float = 1e-5\n",
    "    decay_hh: float = 1e-5\n",
    "    decay_other: float = 1e-5\n",
    "    train_fract: float = .8\n",
    "    seed: int = 42\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "    metrics: list[str] = field(default_factory=lambda: [\"f1\", \"accuracy\"])\n",
    "    \n",
    "    def get_metrics(self) -> MetricCollection:\n",
    "        \"\"\"Constructs a MetricCollection from the specified config\"\"\"\n",
    "        if self.num_classes == 2:\n",
    "            available = {\n",
    "                \"f1\": BinaryF1Score(self.threshold),\n",
    "                \"accuracy\": BinaryAccuracy(self.threshold),\n",
    "                \"auroc\": BinaryAUROC(pos_label = 1),\n",
    "                \"matthews\": BinaryMatthewsCorrCoef(self.threshold)\n",
    "            }\n",
    "        else:\n",
    "            available = {\n",
    "                \"f1\": MulticlassF1Score(num_classes=self.num_classes),\n",
    "                \"accuracy\": MulticlassAccuracy(num_classes=self.num_classes),\n",
    "                \"auroc\": MulticlassAUROC(num_classes=self.num_classes)\n",
    "            }\n",
    "        selected = {k: available[k] for k in self.metrics if k in available}\n",
    "        return MetricCollection(selected)\n",
    "    \n",
    "def _make_class_weights(labels: torch.Tensor, num_classes:int) -> torch.Tensor:\n",
    "    \"\"\"Helper function to compute class weights.\"\"\"\n",
    "    counts = torch.bincount(labels.long(), minlength=num_classes)\n",
    "    weights = counts.sum() / (num_classes * counts.float())\n",
    "    return weights\n",
    "    \n",
    "def train_model_from_config(cfg: TrainConfig) -> GRUModel:\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    return train(\n",
    "        company_data_path = Path(cfg.firm_data),\n",
    "        macro_data_path = [Path(path) for path in cfg.macro_data],\n",
    "        bankruptcy_col = cfg.bankruptcy_col,\n",
    "        company_col=cfg.company_col,\n",
    "        metrics=cfg.get_metrics().to(cfg.device),\n",
    "        device=cfg.device,\n",
    "        num_layers=cfg.num_classes,\n",
    "        hidden_size=cfg.hidden_size,\n",
    "        output_size=cfg.num_classes,\n",
    "        epochs=cfg.epochs,\n",
    "        lr=cfg.lr,\n",
    "        train_fract=cfg.train_fract,\n",
    "        dropout=cfg.dropout,\n",
    "        scheduler_factor=cfg.scheduler_factor,\n",
    "        scheduler_patience=cfg.scheduler_patience,\n",
    "        decay_ih=cfg.decay_ih,\n",
    "        decay_hh=cfg.decay_hh,\n",
    "        decay_other=cfg.decay_other,\n",
    "        seed=cfg.seed\n",
    "    )\n",
    "\n",
    "def train(\n",
    "    company_data_path: str,\n",
    "    macro_data_path: list[str],\n",
    "    bankruptcy_col: str,\n",
    "    company_col: str,\n",
    "    revenue_cap: int,\n",
    "    metrics: list[Metric],\n",
    "    seed: int,\n",
    "    num_layers: int = 2,\n",
    "    hidden_size: int = 64,\n",
    "    output_size: int = 1,\n",
    "    epochs: int = 50,\n",
    "    lr: float = 1e-3,\n",
    "    train_fract: float = 0.8,\n",
    "    dropout: float = 0.2,\n",
    "    scheduler_factor: float = 0.85,\n",
    "    scheduler_patience: int = 50,\n",
    "    min_lr: float = 0.0,\n",
    "    decay_ih:float = 1e-5,\n",
    "    decay_hh:float = 1e-5,\n",
    "    decay_other:float = 1e-5,\n",
    "):  \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    ingestion = DataIngestionPipeline(\n",
    "        company_data_path=company_data_path,\n",
    "        macro_data_path=macro_data_path,\n",
    "        company_col=company_col,\n",
    "        bankruptcy_col=bankruptcy_col,\n",
    "        sheet_name=\"Results\",\n",
    "        revenue_cap=revenue_cap\n",
    "    )\n",
    "    ingestion.load()\n",
    "    series = ingestion.process_data()\n",
    "    financials, macro, labels = series.export_tensors()\n",
    "    \n",
    "    dataset = DualInputSequenceDataset(\n",
    "        firm_tensor = financials,\n",
    "        macro_tensor = macro,\n",
    "        labels = labels\n",
    "    )\n",
    "    \n",
    "    train_ds, val_ds, seed = dataset.stratified_split(train_fract)\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_with_macro)\n",
    "    val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=collate_with_macro)\n",
    "\n",
    "    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu') # Using hardware acceleration if on Mac\n",
    "    logger.info(f\"Device: {device}\")\n",
    "    \n",
    "    metrics_dict = dict()\n",
    "    for metric in metrics:\n",
    "        metrics_dict[metric._get_name()] = metric.to(device)\n",
    "    metrics = metrics_dict \n",
    "    \n",
    "    train_ds = train_ds.to_device(device)\n",
    "    val_ds = val_ds.to_device(device)\n",
    "    \n",
    "    firm_input_size, macro_input_size = dataset.input_dims()\n",
    "    \n",
    "    mlflow.set_tracking_uri('http://127.0.0.1:8080')\n",
    "    mlflow.set_experiment('bankruptcy-predictions')\n",
    "    \n",
    "    mlflow.log_param(\"seed\", seed)\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        model = GRUModel(\n",
    "            firm_input_size=firm_input_size,\n",
    "            macro_input_size=macro_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=output_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        model = model.to(device)\n",
    "        \n",
    "        pos_weight = dataset.pos_weight()\n",
    "        loss_fn = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        \n",
    "        # Logging hyperparameters\n",
    "        mlflow.log_param(\"hidden_size\", hidden_size)\n",
    "        mlflow.log_param(\"output_size\", output_size)\n",
    "        mlflow.log_param(\"num_layers\", num_layers)\n",
    "        mlflow.log_param(\"dropout\", dropout)\n",
    "        mlflow.log_param(\"lr\", lr)\n",
    "\n",
    "        ih_params = []\n",
    "        hh_params = []\n",
    "        other_params = []\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                ih_params.append(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                hh_params.append(param)\n",
    "            else:\n",
    "                other_params.append(param)\n",
    "        \n",
    "        optimizer = Adam([\n",
    "                {'params': ih_params, 'weight_decay': decay_ih},\n",
    "                {'params': hh_params, 'weight_decay': decay_hh},\n",
    "                {'params': other_params, 'weight_decay': decay_other},\n",
    "            ], lr=lr\n",
    "        )\n",
    "        scheduler=CustomReduceLROnPlateau(\n",
    "            optimizer=optimizer,\n",
    "            mode=\"min\",\n",
    "            factor=scheduler_factor,\n",
    "            patience=scheduler_patience,\n",
    "            min_lr=min_lr\n",
    "        )\n",
    "        \n",
    "        train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            epochs=epochs,\n",
    "            metrics=metrics\n",
    "        )\n",
    "        \n",
    "        mlflow.pytorch.log_model(model, f\"model_{datetime.datetime.now()}\")\n",
    "        torch.save(obj = model.state_dict(), f = f\"model_{datetime.datetime.now()}.pth\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unexpected keyword arguments: `pos_label`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m     config=yaml.safe_load(stream)\n\u001b[32m      3\u001b[39m config = TrainConfig(**config)    \n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mTrainConfig.get_metrics\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Constructs a MetricCollection from the specified config\"\"\"\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_classes == \u001b[32m2\u001b[39m:\n\u001b[32m     67\u001b[39m     available = {\n\u001b[32m     68\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m: BinaryF1Score(\u001b[38;5;28mself\u001b[39m.threshold),\n\u001b[32m     69\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: BinaryAccuracy(\u001b[38;5;28mself\u001b[39m.threshold),\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mauroc\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mBinaryAUROC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[32m     71\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmatthews\u001b[39m\u001b[33m\"\u001b[39m: BinaryMatthewsCorrCoef(\u001b[38;5;28mself\u001b[39m.threshold)\n\u001b[32m     72\u001b[39m     }\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     74\u001b[39m     available = {\n\u001b[32m     75\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m: MulticlassF1Score(num_classes=\u001b[38;5;28mself\u001b[39m.num_classes),\n\u001b[32m     76\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: MulticlassAccuracy(num_classes=\u001b[38;5;28mself\u001b[39m.num_classes),\n\u001b[32m     77\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mauroc\u001b[39m\u001b[33m\"\u001b[39m: MulticlassAUROC(num_classes=\u001b[38;5;28mself\u001b[39m.num_classes)\n\u001b[32m     78\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torchmetrics/classification/auroc.py:117\u001b[39m, in \u001b[36mBinaryAUROC.__init__\u001b[39m\u001b[34m(self, max_fpr, thresholds, ignore_index, validate_args, **kwargs)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    111\u001b[39m     max_fpr: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    115\u001b[39m     **kwargs: Any,\n\u001b[32m    116\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mthresholds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthresholds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m validate_args:\n\u001b[32m    119\u001b[39m         _binary_auroc_arg_validation(max_fpr, thresholds, ignore_index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torchmetrics/classification/precision_recall_curve.py:144\u001b[39m, in \u001b[36mBinaryPrecisionRecallCurve.__init__\u001b[39m\u001b[34m(self, thresholds, ignore_index, validate_args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    138\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    139\u001b[39m     thresholds: Optional[Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m], Tensor]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    142\u001b[39m     **kwargs: Any,\n\u001b[32m    143\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m validate_args:\n\u001b[32m    146\u001b[39m         _binary_precision_recall_curve_arg_validation(thresholds, ignore_index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/torchmetrics/metric.py:154\u001b[39m, in \u001b[36mMetric.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[32m    153\u001b[39m     kwargs_ = [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ma\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(kwargs)]\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected keyword arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(kwargs_)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    156\u001b[39m \u001b[38;5;66;03m# initialize\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[38;5;28mself\u001b[39m._update_signature = inspect.signature(\u001b[38;5;28mself\u001b[39m.update)\n",
      "\u001b[31mValueError\u001b[39m: Unexpected keyword arguments: `pos_label`"
     ]
    }
   ],
   "source": [
    "with open(\"config/model_config.yml\") as stream:\n",
    "    config=yaml.safe_load(stream)\n",
    "config = TrainConfig(**config)    \n",
    "config.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "import datetime\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Metric, MetricCollection\n",
    "from torchmetrics.classification import (\n",
    "    BinaryAccuracy, BinaryAUROC, BinaryF1Score, BinaryMatthewsCorrCoef,\n",
    "    MulticlassAccuracy, MulticlassAUROC, MulticlassF1Score)\n",
    "from pathlib import Path\n",
    "\n",
    "from src.datasets.dual_input import DualInputSequenceDataset\n",
    "from src.models.gru import GRUModel\n",
    "from src.data.processing import DataIngestionPipeline\n",
    "from src.train import train_model\n",
    "from src.utils.utils import CustomReduceLROnPlateau, collate_with_macro\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "def load_yaml_file(path):\n",
    "    with open(path) as stream:\n",
    "        try:\n",
    "            config_dict=yaml.safe_load(stream)\n",
    "            return config_dict\n",
    "        except yaml.YAMLError as e:\n",
    "            TypeError(f\"Config file could not be loaded: {e}\")\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    firm_data: str\n",
    "    macro_data: list[str]\n",
    "    bankruptcy_col: str\n",
    "    company_col: str\n",
    "    revenue_cap: int = 3_000\n",
    "    num_classes: int = 2\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 40\n",
    "    lr: float = 1e-3\n",
    "    hidden_size: int = 64\n",
    "    num_layers: int = 2\n",
    "    dropout: float = .2\n",
    "    threshold: float = 0.5\n",
    "    scheduler_factor: float=0.85\n",
    "    scheduler_patience: int = 50\n",
    "    min_lr:float = 0.0\n",
    "    decay_ih: float = 1e-5\n",
    "    decay_hh: float = 1e-5\n",
    "    decay_other: float = 1e-5\n",
    "    train_fract: float = .8\n",
    "    seed: int = 42\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "    metrics: list[str] = field(default_factory=lambda: [\"f1\", \"accuracy\"])\n",
    "    \n",
    "    def get_metrics(self) -> MetricCollection:\n",
    "        \"\"\"Constructs a MetricCollection from the specified config\"\"\"\n",
    "        if self.num_classes == 2:\n",
    "            available = {\n",
    "                \"f1\": BinaryF1Score(self.threshold),\n",
    "                \"accuracy\": BinaryAccuracy(self.threshold),\n",
    "                \"auroc\": BinaryAUROC(),\n",
    "                \"matthews\": BinaryMatthewsCorrCoef(self.threshold)\n",
    "            }\n",
    "        else:\n",
    "            available = {\n",
    "                \"f1\": MulticlassF1Score(num_classes=self.num_classes),\n",
    "                \"accuracy\": MulticlassAccuracy(num_classes=self.num_classes),\n",
    "                \"auroc\": MulticlassAUROC(num_classes=self.num_classes)\n",
    "            }\n",
    "        selected = {k: available[k] for k in self.metrics if k in available}\n",
    "        return MetricCollection(selected)\n",
    "    \n",
    "def _make_class_weights(labels: torch.Tensor, num_classes:int) -> torch.Tensor:\n",
    "    \"\"\"Helper function to compute class weights.\"\"\"\n",
    "    counts = torch.bincount(labels.long(), minlength=num_classes)\n",
    "    weights = counts.sum() / (num_classes * counts.float())\n",
    "    return weights\n",
    "    \n",
    "def train_model_from_config(cfg: TrainConfig) -> GRUModel:\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    return train(\n",
    "        company_data_path = Path(cfg.firm_data),\n",
    "        macro_data_path = [Path(path) for path in cfg.macro_data],\n",
    "        bankruptcy_col = cfg.bankruptcy_col,\n",
    "        company_col=cfg.company_col,\n",
    "        metrics=cfg.get_metrics().to(cfg.device),\n",
    "        device=cfg.device,\n",
    "        num_layers=cfg.num_classes,\n",
    "        hidden_size=cfg.hidden_size,\n",
    "        output_size=cfg.num_classes,\n",
    "        epochs=cfg.epochs,\n",
    "        lr=cfg.lr,\n",
    "        train_fract=cfg.train_fract,\n",
    "        dropout=cfg.dropout,\n",
    "        scheduler_factor=cfg.scheduler_factor,\n",
    "        scheduler_patience=cfg.scheduler_patience,\n",
    "        decay_ih=cfg.decay_ih,\n",
    "        decay_hh=cfg.decay_hh,\n",
    "        decay_other=cfg.decay_other,\n",
    "        seed=cfg.seed\n",
    "    )\n",
    "\n",
    "def train(\n",
    "    company_data_path: str,\n",
    "    macro_data_path: list[str],\n",
    "    bankruptcy_col: str,\n",
    "    company_col: str,\n",
    "    revenue_cap: int,\n",
    "    metrics: list[Metric],\n",
    "    seed: int,\n",
    "    num_layers: int = 2,\n",
    "    hidden_size: int = 64,\n",
    "    output_size: int = 1,\n",
    "    epochs: int = 50,\n",
    "    lr: float = 1e-3,\n",
    "    train_fract: float = 0.8,\n",
    "    dropout: float = 0.2,\n",
    "    scheduler_factor: float = 0.85,\n",
    "    scheduler_patience: int = 50,\n",
    "    min_lr: float = 0.0,\n",
    "    decay_ih:float = 1e-5,\n",
    "    decay_hh:float = 1e-5,\n",
    "    decay_other:float = 1e-5,\n",
    "):  \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    ingestion = DataIngestionPipeline(\n",
    "        company_data_path=company_data_path,\n",
    "        macro_data_path=macro_data_path,\n",
    "        company_col=company_col,\n",
    "        bankruptcy_col=bankruptcy_col,\n",
    "        sheet_name=\"Results\",\n",
    "        revenue_cap=revenue_cap\n",
    "    )\n",
    "    ingestion.load()\n",
    "    series = ingestion.process_data()\n",
    "    financials, macro, labels = series.export_tensors()\n",
    "    \n",
    "    dataset = DualInputSequenceDataset(\n",
    "        firm_tensor = financials,\n",
    "        macro_tensor = macro,\n",
    "        labels = labels\n",
    "    )\n",
    "    \n",
    "    train_ds, val_ds, seed = dataset.stratified_split(train_fract)\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_with_macro)\n",
    "    val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=collate_with_macro)\n",
    "\n",
    "    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu') # Using hardware acceleration if on Mac\n",
    "    logger.info(f\"Device: {device}\")\n",
    "    \n",
    "    metrics_dict = dict()\n",
    "    for metric in metrics:\n",
    "        metrics_dict[metric._get_name()] = metric.to(device)\n",
    "    metrics = metrics_dict \n",
    "    \n",
    "    train_ds = train_ds.to_device(device)\n",
    "    val_ds = val_ds.to_device(device)\n",
    "    \n",
    "    firm_input_size, macro_input_size = dataset.input_dims()\n",
    "    \n",
    "    mlflow.set_tracking_uri('http://127.0.0.1:8080')\n",
    "    mlflow.set_experiment('bankruptcy-predictions')\n",
    "    \n",
    "    mlflow.log_param(\"seed\", seed)\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        model = GRUModel(\n",
    "            firm_input_size=firm_input_size,\n",
    "            macro_input_size=macro_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=output_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        model = model.to(device)\n",
    "        \n",
    "        pos_weight = dataset.pos_weight()\n",
    "        loss_fn = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        \n",
    "        # Logging hyperparameters\n",
    "        mlflow.log_param(\"hidden_size\", hidden_size)\n",
    "        mlflow.log_param(\"output_size\", output_size)\n",
    "        mlflow.log_param(\"num_layers\", num_layers)\n",
    "        mlflow.log_param(\"dropout\", dropout)\n",
    "        mlflow.log_param(\"lr\", lr)\n",
    "\n",
    "        ih_params = []\n",
    "        hh_params = []\n",
    "        other_params = []\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                ih_params.append(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                hh_params.append(param)\n",
    "            else:\n",
    "                other_params.append(param)\n",
    "        \n",
    "        optimizer = Adam([\n",
    "                {'params': ih_params, 'weight_decay': decay_ih},\n",
    "                {'params': hh_params, 'weight_decay': decay_hh},\n",
    "                {'params': other_params, 'weight_decay': decay_other},\n",
    "            ], lr=lr\n",
    "        )\n",
    "        scheduler=CustomReduceLROnPlateau(\n",
    "            optimizer=optimizer,\n",
    "            mode=\"min\",\n",
    "            factor=scheduler_factor,\n",
    "            patience=scheduler_patience,\n",
    "            min_lr=min_lr\n",
    "        )\n",
    "        \n",
    "        train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            epochs=epochs,\n",
    "            metrics=metrics\n",
    "        )\n",
    "        \n",
    "        mlflow.pytorch.log_model(model, f\"model_{datetime.datetime.now()}\")\n",
    "        torch.save(obj = model.state_dict(), f = f\"model_{datetime.datetime.now()}.pth\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetricCollection(\n",
       "  (accuracy): BinaryAccuracy()\n",
       "  (auroc): BinaryAUROC()\n",
       "  (f1): BinaryF1Score()\n",
       "  (matthews): BinaryMatthewsCorrCoef()\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"config/model_config.yml\") as stream:\n",
    "    config=yaml.safe_load(stream)\n",
    "config = TrainConfig(**config)    \n",
    "config.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() got an unexpected keyword argument 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_model_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mtrain_model_from_config\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_model_from_config\u001b[39m(cfg: TrainConfig) -> GRUModel:\n\u001b[32m     89\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Main training function\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompany_data_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfirm_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmacro_data_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmacro_data\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_fract\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_fract\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscheduler_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscheduler_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscheduler_patience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscheduler_patience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecay_ih\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecay_ih\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecay_hh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecay_hh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecay_other\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecay_other\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseed\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: train() got an unexpected keyword argument 'device'"
     ]
    }
   ],
   "source": [
    "train_model_from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "import datetime\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Metric, MetricCollection\n",
    "from torchmetrics.classification import (\n",
    "    BinaryAccuracy, BinaryAUROC, BinaryF1Score, BinaryMatthewsCorrCoef,\n",
    "    MulticlassAccuracy, MulticlassAUROC, MulticlassF1Score)\n",
    "from pathlib import Path\n",
    "\n",
    "from src.datasets.dual_input import DualInputSequenceDataset\n",
    "from src.models.gru import GRUModel\n",
    "from src.data.processing import DataIngestionPipeline\n",
    "from src.train import train_model\n",
    "from src.utils.utils import CustomReduceLROnPlateau, collate_with_macro\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "def load_yaml_file(path):\n",
    "    with open(path) as stream:\n",
    "        try:\n",
    "            config_dict=yaml.safe_load(stream)\n",
    "            return config_dict\n",
    "        except yaml.YAMLError as e:\n",
    "            TypeError(f\"Config file could not be loaded: {e}\")\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    firm_data: str\n",
    "    macro_data: list[str]\n",
    "    bankruptcy_col: str\n",
    "    company_col: str\n",
    "    revenue_cap: int = 3_000\n",
    "    num_classes: int = 2\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 40\n",
    "    lr: float = 1e-3\n",
    "    hidden_size: int = 64\n",
    "    num_layers: int = 2\n",
    "    dropout: float = .2\n",
    "    threshold: float = 0.5\n",
    "    scheduler_factor: float=0.85\n",
    "    scheduler_patience: int = 50\n",
    "    min_lr:float = 0.0\n",
    "    decay_ih: float = 1e-5\n",
    "    decay_hh: float = 1e-5\n",
    "    decay_other: float = 1e-5\n",
    "    train_fract: float = .8\n",
    "    seed: int = 42\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "    metrics: list[str] = field(default_factory=lambda: [\"f1\", \"accuracy\"])\n",
    "    \n",
    "    def get_metrics(self) -> MetricCollection:\n",
    "        \"\"\"Constructs a MetricCollection from the specified config\"\"\"\n",
    "        if self.num_classes == 2:\n",
    "            available = {\n",
    "                \"f1\": BinaryF1Score(self.threshold),\n",
    "                \"accuracy\": BinaryAccuracy(self.threshold),\n",
    "                \"auroc\": BinaryAUROC(),\n",
    "                \"matthews\": BinaryMatthewsCorrCoef(self.threshold)\n",
    "            }\n",
    "        else:\n",
    "            available = {\n",
    "                \"f1\": MulticlassF1Score(num_classes=self.num_classes),\n",
    "                \"accuracy\": MulticlassAccuracy(num_classes=self.num_classes),\n",
    "                \"auroc\": MulticlassAUROC(num_classes=self.num_classes)\n",
    "            }\n",
    "        selected = {k: available[k] for k in self.metrics if k in available}\n",
    "        return MetricCollection(selected)\n",
    "    \n",
    "def _make_class_weights(labels: torch.Tensor, num_classes:int) -> torch.Tensor:\n",
    "    \"\"\"Helper function to compute class weights.\"\"\"\n",
    "    counts = torch.bincount(labels.long(), minlength=num_classes)\n",
    "    weights = counts.sum() / (num_classes * counts.float())\n",
    "    return weights\n",
    "    \n",
    "def train_model_from_config(cfg: TrainConfig) -> GRUModel:\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    return train(\n",
    "        company_data_path = Path(cfg.firm_data),\n",
    "        macro_data_path = [Path(path) for path in cfg.macro_data],\n",
    "        bankruptcy_col = cfg.bankruptcy_col,\n",
    "        company_col=cfg.company_col,\n",
    "        metrics=cfg.get_metrics().to(cfg.device),\n",
    "        device=cfg.device,\n",
    "        num_layers=cfg.num_classes,\n",
    "        hidden_size=cfg.hidden_size,\n",
    "        output_size=cfg.num_classes,\n",
    "        epochs=cfg.epochs,\n",
    "        lr=cfg.lr,\n",
    "        train_fract=cfg.train_fract,\n",
    "        dropout=cfg.dropout,\n",
    "        scheduler_factor=cfg.scheduler_factor,\n",
    "        scheduler_patience=cfg.scheduler_patience,\n",
    "        decay_ih=cfg.decay_ih,\n",
    "        decay_hh=cfg.decay_hh,\n",
    "        decay_other=cfg.decay_other,\n",
    "        seed=cfg.seed\n",
    "    )\n",
    "\n",
    "def train(\n",
    "    company_data_path: str,\n",
    "    macro_data_path: list[str],\n",
    "    bankruptcy_col: str,\n",
    "    company_col: str,\n",
    "    revenue_cap: int,\n",
    "    metrics: list[Metric],\n",
    "    seed: int,\n",
    "    num_layers: int = 2,\n",
    "    hidden_size: int = 64,\n",
    "    output_size: int = 1,\n",
    "    epochs: int = 50,\n",
    "    lr: float = 1e-3,\n",
    "    train_fract: float = 0.8,\n",
    "    dropout: float = 0.2,\n",
    "    scheduler_factor: float = 0.85,\n",
    "    scheduler_patience: int = 50,\n",
    "    min_lr: float = 0.0,\n",
    "    decay_ih:float = 1e-5,\n",
    "    decay_hh:float = 1e-5,\n",
    "    decay_other:float = 1e-5,\n",
    "    device: str=\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "):  \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    ingestion = DataIngestionPipeline(\n",
    "        company_data_path=company_data_path,\n",
    "        macro_data_path=macro_data_path,\n",
    "        company_col=company_col,\n",
    "        bankruptcy_col=bankruptcy_col,\n",
    "        sheet_name=\"Results\",\n",
    "        revenue_cap=revenue_cap\n",
    "    )\n",
    "    ingestion.load()\n",
    "    series = ingestion.process_data()\n",
    "    financials, macro, labels = series.export_tensors()\n",
    "    \n",
    "    dataset = DualInputSequenceDataset(\n",
    "        firm_tensor = financials,\n",
    "        macro_tensor = macro,\n",
    "        labels = labels\n",
    "    )\n",
    "    \n",
    "    train_ds, val_ds, seed = dataset.stratified_split(train_fract)\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_with_macro)\n",
    "    val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=collate_with_macro)\n",
    "\n",
    "    logger.info(f\"Device: {device}\")\n",
    "    \n",
    "    metrics_dict = dict()\n",
    "    for metric in metrics:\n",
    "        metrics_dict[metric._get_name()] = metric.to(device)\n",
    "    metrics = metrics_dict \n",
    "    \n",
    "    train_ds = train_ds.to_device(device)\n",
    "    val_ds = val_ds.to_device(device)\n",
    "    \n",
    "    firm_input_size, macro_input_size = dataset.input_dims()\n",
    "    \n",
    "    mlflow.set_tracking_uri('http://127.0.0.1:8080')\n",
    "    mlflow.set_experiment('bankruptcy-predictions')\n",
    "    \n",
    "    mlflow.log_param(\"seed\", seed)\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        model = GRUModel(\n",
    "            firm_input_size=firm_input_size,\n",
    "            macro_input_size=macro_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=output_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        model = model.to(device)\n",
    "        \n",
    "        pos_weight = dataset.pos_weight()\n",
    "        loss_fn = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        \n",
    "        # Logging hyperparameters\n",
    "        mlflow.log_param(\"hidden_size\", hidden_size)\n",
    "        mlflow.log_param(\"output_size\", output_size)\n",
    "        mlflow.log_param(\"num_layers\", num_layers)\n",
    "        mlflow.log_param(\"dropout\", dropout)\n",
    "        mlflow.log_param(\"lr\", lr)\n",
    "\n",
    "        ih_params = []\n",
    "        hh_params = []\n",
    "        other_params = []\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                ih_params.append(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                hh_params.append(param)\n",
    "            else:\n",
    "                other_params.append(param)\n",
    "        \n",
    "        optimizer = Adam([\n",
    "                {'params': ih_params, 'weight_decay': decay_ih},\n",
    "                {'params': hh_params, 'weight_decay': decay_hh},\n",
    "                {'params': other_params, 'weight_decay': decay_other},\n",
    "            ], lr=lr\n",
    "        )\n",
    "        scheduler=CustomReduceLROnPlateau(\n",
    "            optimizer=optimizer,\n",
    "            mode=\"min\",\n",
    "            factor=scheduler_factor,\n",
    "            patience=scheduler_patience,\n",
    "            min_lr=min_lr\n",
    "        )\n",
    "        \n",
    "        train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            epochs=epochs,\n",
    "            metrics=metrics\n",
    "        )\n",
    "        \n",
    "        mlflow.pytorch.log_model(model, f\"model_{datetime.datetime.now()}\")\n",
    "        torch.save(obj = model.state_dict(), f = f\"model_{datetime.datetime.now()}.pth\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() missing 1 required positional argument: 'revenue_cap'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_model_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mtrain_model_from_config\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_model_from_config\u001b[39m(cfg: TrainConfig) -> GRUModel:\n\u001b[32m     89\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Main training function\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompany_data_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfirm_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmacro_data_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmacro_data\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_fract\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_fract\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscheduler_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscheduler_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscheduler_patience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscheduler_patience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecay_ih\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecay_ih\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecay_hh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecay_hh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecay_other\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecay_other\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseed\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: train() missing 1 required positional argument: 'revenue_cap'"
     ]
    }
   ],
   "source": [
    "train_model_from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model_from_config(cfg: TrainConfig) -> GRUModel:\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    return train(\n",
    "        company_data_path = Path(cfg.firm_data),\n",
    "        macro_data_path = [Path(path) for path in cfg.macro_data],\n",
    "        bankruptcy_col = cfg.bankruptcy_col,\n",
    "        company_col=cfg.company_col,\n",
    "        revenue_cap=cfg.revenue_cap,\n",
    "        metrics=cfg.get_metrics().to(cfg.device),\n",
    "        device=cfg.device,\n",
    "        num_layers=cfg.num_classes,\n",
    "        hidden_size=cfg.hidden_size,\n",
    "        output_size=cfg.num_classes,\n",
    "        epochs=cfg.epochs,\n",
    "        lr=cfg.lr,\n",
    "        train_fract=cfg.train_fract,\n",
    "        dropout=cfg.dropout,\n",
    "        scheduler_factor=cfg.scheduler_factor,\n",
    "        scheduler_patience=cfg.scheduler_patience,\n",
    "        decay_ih=cfg.decay_ih,\n",
    "        decay_hh=cfg.decay_hh,\n",
    "        decay_other=cfg.decay_other,\n",
    "        seed=cfg.seed\n",
    "    )\n",
    "\n",
    "def train(\n",
    "    company_data_path: str,\n",
    "    macro_data_path: list[str],\n",
    "    bankruptcy_col: str,\n",
    "    company_col: str,\n",
    "    revenue_cap: int,\n",
    "    metrics: list[Metric],\n",
    "    seed: int,\n",
    "    num_layers: int = 2,\n",
    "    hidden_size: int = 64,\n",
    "    output_size: int = 1,\n",
    "    epochs: int = 50,\n",
    "    lr: float = 1e-3,\n",
    "    train_fract: float = 0.8,\n",
    "    dropout: float = 0.2,\n",
    "    scheduler_factor: float = 0.85,\n",
    "    scheduler_patience: int = 50,\n",
    "    min_lr: float = 0.0,\n",
    "    decay_ih:float = 1e-5,\n",
    "    decay_hh:float = 1e-5,\n",
    "    decay_other:float = 1e-5,\n",
    "    device: str=\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "):  \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    ingestion = DataIngestionPipeline(\n",
    "        company_data_path=company_data_path,\n",
    "        macro_data_path=macro_data_path,\n",
    "        company_col=company_col,\n",
    "        bankruptcy_col=bankruptcy_col,\n",
    "        sheet_name=\"Results\",\n",
    "        revenue_cap=revenue_cap\n",
    "    )\n",
    "    ingestion.load()\n",
    "    series = ingestion.process_data()\n",
    "    financials, macro, labels = series.export_tensors()\n",
    "    \n",
    "    dataset = DualInputSequenceDataset(\n",
    "        firm_tensor = financials,\n",
    "        macro_tensor = macro,\n",
    "        labels = labels\n",
    "    )\n",
    "    \n",
    "    train_ds, val_ds, seed = dataset.stratified_split(train_fract)\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_with_macro)\n",
    "    val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=collate_with_macro)\n",
    "\n",
    "    logger.info(f\"Device: {device}\")\n",
    "    \n",
    "    metrics_dict = dict()\n",
    "    for metric in metrics:\n",
    "        metrics_dict[metric._get_name()] = metric.to(device)\n",
    "    metrics = metrics_dict \n",
    "    \n",
    "    train_ds = train_ds.to_device(device)\n",
    "    val_ds = val_ds.to_device(device)\n",
    "    \n",
    "    firm_input_size, macro_input_size = dataset.input_dims()\n",
    "    \n",
    "    mlflow.set_tracking_uri('http://127.0.0.1:8080')\n",
    "    mlflow.set_experiment('bankruptcy-predictions')\n",
    "    \n",
    "    mlflow.log_param(\"seed\", seed)\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        model = GRUModel(\n",
    "            firm_input_size=firm_input_size,\n",
    "            macro_input_size=macro_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=output_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        model = model.to(device)\n",
    "        \n",
    "        pos_weight = dataset.pos_weight()\n",
    "        loss_fn = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        \n",
    "        # Logging hyperparameters\n",
    "        mlflow.log_param(\"hidden_size\", hidden_size)\n",
    "        mlflow.log_param(\"output_size\", output_size)\n",
    "        mlflow.log_param(\"num_layers\", num_layers)\n",
    "        mlflow.log_param(\"dropout\", dropout)\n",
    "        mlflow.log_param(\"lr\", lr)\n",
    "\n",
    "        ih_params = []\n",
    "        hh_params = []\n",
    "        other_params = []\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                ih_params.append(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                hh_params.append(param)\n",
    "            else:\n",
    "                other_params.append(param)\n",
    "        \n",
    "        optimizer = Adam([\n",
    "                {'params': ih_params, 'weight_decay': decay_ih},\n",
    "                {'params': hh_params, 'weight_decay': decay_hh},\n",
    "                {'params': other_params, 'weight_decay': decay_other},\n",
    "            ], lr=lr\n",
    "        )\n",
    "        scheduler=CustomReduceLROnPlateau(\n",
    "            optimizer=optimizer,\n",
    "            mode=\"min\",\n",
    "            factor=scheduler_factor,\n",
    "            patience=scheduler_patience,\n",
    "            min_lr=min_lr\n",
    "        )\n",
    "        \n",
    "        train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            epochs=epochs,\n",
    "            metrics=metrics\n",
    "        )\n",
    "        \n",
    "        mlflow.pytorch.log_model(model, f\"model_{datetime.datetime.now()}\")\n",
    "        torch.save(obj = model.state_dict(), f = f\"model_{datetime.datetime.now()}.pth\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataIngestionPipeline.__init__() missing 1 required positional argument: 'first_year'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_model_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mtrain_model_from_config\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_model_from_config\u001b[39m(cfg: TrainConfig) -> GRUModel:\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Main training function\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompany_data_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfirm_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmacro_data_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmacro_data\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevenue_cap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrevenue_cap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_fract\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_fract\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscheduler_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscheduler_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscheduler_patience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscheduler_patience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecay_ih\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecay_ih\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecay_hh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecay_hh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecay_other\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecay_other\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseed\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(company_data_path, macro_data_path, bankruptcy_col, company_col, revenue_cap, metrics, seed, num_layers, hidden_size, output_size, epochs, lr, train_fract, dropout, scheduler_factor, scheduler_patience, min_lr, decay_ih, decay_hh, decay_other, device)\u001b[39m\n\u001b[32m     49\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     50\u001b[39m logging.basicConfig(level=logging.INFO)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m ingestion = \u001b[43mDataIngestionPipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompany_data_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompany_data_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmacro_data_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmacro_data_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mResults\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevenue_cap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevenue_cap\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m ingestion.load()\n\u001b[32m     61\u001b[39m series = ingestion.process_data()\n",
      "\u001b[31mTypeError\u001b[39m: DataIngestionPipeline.__init__() missing 1 required positional argument: 'first_year'"
     ]
    }
   ],
   "source": [
    "train_model_from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainConfig(firm_data='data/demo_data.xlsx', macro_data=['insee/serie_000857176_04042025/valeurs_mensuelles.csv', 'insee/serie_000857180_04042025/valeurs_mensuelles.csv', 'insee/serie_001763782_04042025/valeurs_mensuelles.csv'], bankruptcy_col='Status date', company_col='Company name Latin alphabet', revenue_cap=3000, num_classes=2, batch_size=32, epochs=100, lr='1e-3', hidden_size=64, num_layers=2, dropout=0.4, threshold=0.4, scheduler_factor=0.85, scheduler_patience=50, min_lr=0.0, decay_ih='1e-5', decay_hh='1e-4', decay_other='1e-5', train_fract=0.8, seed=2025, device='mps', metrics=['f1', 'accuracy', 'auroc', 'matthews'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataIngestionPipeline.__init__() missing 1 required positional argument: 'first_year'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_model_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mtrain_model_from_config\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_model_from_config\u001b[39m(cfg: TrainConfig) -> GRUModel:\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Main training function\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompany_data_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfirm_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmacro_data_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmacro_data\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevenue_cap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrevenue_cap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_fract\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_fract\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscheduler_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscheduler_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscheduler_patience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscheduler_patience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecay_ih\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecay_ih\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecay_hh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecay_hh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecay_other\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecay_other\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseed\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(company_data_path, macro_data_path, bankruptcy_col, company_col, revenue_cap, metrics, seed, num_layers, hidden_size, output_size, epochs, lr, train_fract, dropout, scheduler_factor, scheduler_patience, min_lr, decay_ih, decay_hh, decay_other, device)\u001b[39m\n\u001b[32m     49\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     50\u001b[39m logging.basicConfig(level=logging.INFO)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m ingestion = \u001b[43mDataIngestionPipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompany_data_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompany_data_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmacro_data_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmacro_data_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mResults\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevenue_cap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevenue_cap\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m ingestion.load()\n\u001b[32m     61\u001b[39m series = ingestion.process_data()\n",
      "\u001b[31mTypeError\u001b[39m: DataIngestionPipeline.__init__() missing 1 required positional argument: 'first_year'"
     ]
    }
   ],
   "source": [
    "train_model_from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restarted base (Python 3.12.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data.time_series'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimpleNamespace\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgru\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GRUModel\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mseries_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SeriesDataset\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtrain_entry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainConfig\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_model_forward\u001b[39m():\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Minimal config for testing\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/src/datasets/series_dataset.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtime_series\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MacroTimeSeries\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtensor_factory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorFactory\n\u001b[32m      6\u001b[39m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSeriesDataset\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'data.time_series'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from src.models.gru import GRUModel\n",
    "from src.datasets.series_dataset import SeriesDataset\n",
    "from train_entry import TrainConfig\n",
    "\n",
    "def test_model_forward():\n",
    "    # Minimal config for testing\n",
    "    config = SimpleNamespace(\n",
    "        data_path=\"demo_data.xlsx\",\n",
    "        target_year=2025,\n",
    "        micro_window=3,\n",
    "        macro_window=5,\n",
    "        prediction_years=2,\n",
    "        batch_size=2,\n",
    "        hidden_dim=32,\n",
    "        dropout=0.2,\n",
    "        learning_rate=0.001,\n",
    "        n_epochs=1,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Initialize handler\n",
    "    handler = TrainConfig(config)\n",
    "    X_micro, X_macro, y, _, _ = handler.prepare_datasets()\n",
    "\n",
    "    # Use only the first batch for quick test\n",
    "    dataset = SeriesDataset(X_micro, X_macro, y)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=config.batch_size)\n",
    "\n",
    "    # Model instantiation\n",
    "    input_dim_micro = X_micro.shape[2]\n",
    "    input_dim_macro = X_macro.shape[2]\n",
    "    model = GRUModel(input_dim_micro, input_dim_macro, hidden_dim=config.hidden_dim)\n",
    "\n",
    "    # Sanity check on one batch\n",
    "    for x_micro_batch, x_macro_batch, y_batch in loader:\n",
    "        out = model(x_micro_batch, x_macro_batch)\n",
    "        assert out.shape == (config.batch_size, 1), f\"Unexpected output shape: {out.shape}\"\n",
    "        assert not torch.isnan(out).any(), \"Model output contains NaNs\"\n",
    "        break\n",
    "\n",
    "    print(\" Forward pass test successful.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_model_forward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restarted base (Python 3.12.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data.time_series'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimpleNamespace\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgru\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GRUModel\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mseries_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SeriesDataset\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtrain_entry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainConfig\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_model_forward\u001b[39m():\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Minimal config for testing\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/src/datasets/series_dataset.py:4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtime_series\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MacroTimeSeries\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtensor_factory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorFactory\n\u001b[32m      6\u001b[39m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSeriesDataset\u001b[39;00m:\n\u001b[32m      8\u001b[39m     financial_df: pd.DataFrame\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/src/data/tensor_factory.py:9\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RobustScaler\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtime_series\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MacroTimeSeries\n\u001b[32m     11\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mTensorFactory\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'data.time_series'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from src.models.gru import GRUModel\n",
    "from src.datasets.series_dataset import SeriesDataset\n",
    "from train_entry import TrainConfig\n",
    "\n",
    "def test_model_forward():\n",
    "    # Minimal config for testing\n",
    "    config = SimpleNamespace(\n",
    "        data_path=\"demo_data.xlsx\",\n",
    "        target_year=2025,\n",
    "        micro_window=3,\n",
    "        macro_window=5,\n",
    "        prediction_years=2,\n",
    "        batch_size=2,\n",
    "        hidden_dim=32,\n",
    "        dropout=0.2,\n",
    "        learning_rate=0.001,\n",
    "        n_epochs=1,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Initialize handler\n",
    "    handler = TrainConfig(config)\n",
    "    X_micro, X_macro, y, _, _ = handler.prepare_datasets()\n",
    "\n",
    "    # Use only the first batch for quick test\n",
    "    dataset = SeriesDataset(X_micro, X_macro, y)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=config.batch_size)\n",
    "\n",
    "    # Model instantiation\n",
    "    input_dim_micro = X_micro.shape[2]\n",
    "    input_dim_macro = X_macro.shape[2]\n",
    "    model = GRUModel(input_dim_micro, input_dim_macro, hidden_dim=config.hidden_dim)\n",
    "\n",
    "    # Sanity check on one batch\n",
    "    for x_micro_batch, x_macro_batch, y_batch in loader:\n",
    "        out = model(x_micro_batch, x_macro_batch)\n",
    "        assert out.shape == (config.batch_size, 1), f\"Unexpected output shape: {out.shape}\"\n",
    "        assert not torch.isnan(out).any(), \"Model output contains NaNs\"\n",
    "        break\n",
    "\n",
    "    print(\" Forward pass test successful.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_model_forward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.data.processing'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgru\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GRUModel\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mseries_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SeriesDataset\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtrain_entry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainConfig\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_model_forward\u001b[39m():\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Minimal config for testing\u001b[39;00m\n\u001b[32m     10\u001b[39m     config = SimpleNamespace(\n\u001b[32m     11\u001b[39m         data_path=\u001b[33m\"\u001b[39m\u001b[33mdemo_data.xlsx\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m         target_year=\u001b[32m2025\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m         seed=\u001b[32m42\u001b[39m\n\u001b[32m     22\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/train_entry.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdual_input\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DualInputSequenceDataset\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgru\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GRUModel\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataIngestionPipeline\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_model\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CustomReduceLROnPlateau, collate_with_macro\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src.data.processing'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from src.models.gru import GRUModel\n",
    "from src.datasets.series_dataset import SeriesDataset\n",
    "from train_entry import TrainConfig\n",
    "\n",
    "def test_model_forward():\n",
    "    # Minimal config for testing\n",
    "    config = SimpleNamespace(\n",
    "        data_path=\"demo_data.xlsx\",\n",
    "        target_year=2025,\n",
    "        micro_window=3,\n",
    "        macro_window=5,\n",
    "        prediction_years=2,\n",
    "        batch_size=2,\n",
    "        hidden_dim=32,\n",
    "        dropout=0.2,\n",
    "        learning_rate=0.001,\n",
    "        n_epochs=1,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Initialize handler\n",
    "    handler = TrainConfig(config)\n",
    "    X_micro, X_macro, y, _, _ = handler.prepare_datasets()\n",
    "\n",
    "    # Use only the first batch for quick test\n",
    "    dataset = SeriesDataset(X_micro, X_macro, y)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=config.batch_size)\n",
    "\n",
    "    # Model instantiation\n",
    "    input_dim_micro = X_micro.shape[2]\n",
    "    input_dim_macro = X_macro.shape[2]\n",
    "    model = GRUModel(input_dim_micro, input_dim_macro, hidden_dim=config.hidden_dim)\n",
    "\n",
    "    # Sanity check on one batch\n",
    "    for x_micro_batch, x_macro_batch, y_batch in loader:\n",
    "        out = model(x_micro_batch, x_macro_batch)\n",
    "        assert out.shape == (config.batch_size, 1), f\"Unexpected output shape: {out.shape}\"\n",
    "        assert not torch.isnan(out).any(), \"Model output contains NaNs\"\n",
    "        break\n",
    "\n",
    "    print(\" Forward pass test successful.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_model_forward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restarted base (Python 3.12.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.data.processing'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgru\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GRUModel\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mseries_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SeriesDataset\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtrain_entry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainConfig\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_model_forward\u001b[39m():\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Minimal config for testing\u001b[39;00m\n\u001b[32m     10\u001b[39m     config = SimpleNamespace(\n\u001b[32m     11\u001b[39m         data_path=\u001b[33m\"\u001b[39m\u001b[33mdemo_data.xlsx\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m         target_year=\u001b[32m2025\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m         seed=\u001b[32m42\u001b[39m\n\u001b[32m     22\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/train_entry.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdual_input\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DualInputSequenceDataset\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgru\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GRUModel\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataIngestionPipeline\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_model\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CustomReduceLROnPlateau, collate_with_macro\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src.data.processing'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from src.models.gru import GRUModel\n",
    "from src.datasets.series_dataset import SeriesDataset\n",
    "from train_entry import TrainConfig\n",
    "\n",
    "def test_model_forward():\n",
    "    # Minimal config for testing\n",
    "    config = SimpleNamespace(\n",
    "        data_path=\"demo_data.xlsx\",\n",
    "        target_year=2025,\n",
    "        micro_window=3,\n",
    "        macro_window=5,\n",
    "        prediction_years=2,\n",
    "        batch_size=2,\n",
    "        hidden_dim=32,\n",
    "        dropout=0.2,\n",
    "        learning_rate=0.001,\n",
    "        n_epochs=1,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Initialize handler\n",
    "    handler = TrainConfig(config)\n",
    "    X_micro, X_macro, y, _, _ = handler.prepare_datasets()\n",
    "\n",
    "    # Use only the first batch for quick test\n",
    "    dataset = SeriesDataset(X_micro, X_macro, y)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=config.batch_size)\n",
    "\n",
    "    # Model instantiation\n",
    "    input_dim_micro = X_micro.shape[2]\n",
    "    input_dim_macro = X_macro.shape[2]\n",
    "    model = GRUModel(input_dim_micro, input_dim_macro, hidden_dim=config.hidden_dim)\n",
    "\n",
    "    # Sanity check on one batch\n",
    "    for x_micro_batch, x_macro_batch, y_batch in loader:\n",
    "        out = model(x_micro_batch, x_macro_batch)\n",
    "        assert out.shape == (config.batch_size, 1), f\"Unexpected output shape: {out.shape}\"\n",
    "        assert not torch.isnan(out).any(), \"Model output contains NaNs\"\n",
    "        break\n",
    "\n",
    "    print(\" Forward pass test successful.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_model_forward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgru\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GRUModel\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mseries_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SeriesDataset\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtrain_entry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainConfig\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_model_forward\u001b[39m():\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Minimal config for testing\u001b[39;00m\n\u001b[32m     10\u001b[39m     config = SimpleNamespace(\n\u001b[32m     11\u001b[39m         data_path=\u001b[33m\"\u001b[39m\u001b[33mdemo_data.xlsx\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m         target_year=\u001b[32m2025\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m         seed=\u001b[32m42\u001b[39m\n\u001b[32m     22\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/train_entry.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdual_input\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DualInputSequenceDataset\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgru\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GRUModel\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataIngestionPipeline\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_model\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CustomReduceLROnPlateau, collate_with_macro\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/src/data/pipeline.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlflow\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mseries_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SeriesDataset\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CompanyDataLoader, MacroDataLoader\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtensor_factory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorFactory\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from src.models.gru import GRUModel\n",
    "from src.datasets.series_dataset import SeriesDataset\n",
    "from train_entry import TrainConfig\n",
    "\n",
    "def test_model_forward():\n",
    "    # Minimal config for testing\n",
    "    config = SimpleNamespace(\n",
    "        data_path=\"demo_data.xlsx\",\n",
    "        target_year=2025,\n",
    "        micro_window=3,\n",
    "        macro_window=5,\n",
    "        prediction_years=2,\n",
    "        batch_size=2,\n",
    "        hidden_dim=32,\n",
    "        dropout=0.2,\n",
    "        learning_rate=0.001,\n",
    "        n_epochs=1,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Initialize handler\n",
    "    handler = TrainConfig(config)\n",
    "    X_micro, X_macro, y, _, _ = handler.prepare_datasets()\n",
    "\n",
    "    # Use only the first batch for quick test\n",
    "    dataset = SeriesDataset(X_micro, X_macro, y)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=config.batch_size)\n",
    "\n",
    "    # Model instantiation\n",
    "    input_dim_micro = X_micro.shape[2]\n",
    "    input_dim_macro = X_macro.shape[2]\n",
    "    model = GRUModel(input_dim_micro, input_dim_macro, hidden_dim=config.hidden_dim)\n",
    "\n",
    "    # Sanity check on one batch\n",
    "    for x_micro_batch, x_macro_batch, y_batch in loader:\n",
    "        out = model(x_micro_batch, x_macro_batch)\n",
    "        assert out.shape == (config.batch_size, 1), f\"Unexpected output shape: {out.shape}\"\n",
    "        assert not torch.isnan(out).any(), \"Model output contains NaNs\"\n",
    "        break\n",
    "\n",
    "    print(\" Forward pass test successful.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_model_forward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgru\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GRUModel\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mseries_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SeriesDataset\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtrain_entry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainConfig\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_model_forward\u001b[39m():\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# Minimal config for testing\u001b[39;00m\n\u001b[32m     10\u001b[39m     config = SimpleNamespace(\n\u001b[32m     11\u001b[39m         data_path=\u001b[33m\"\u001b[39m\u001b[33mdemo_data.xlsx\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m         target_year=\u001b[32m2025\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m         seed=\u001b[32m42\u001b[39m\n\u001b[32m     22\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/train_entry.py:19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdual_input\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DualInputSequenceDataset\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgru\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GRUModel\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IngestionPipeline\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_model\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CustomReduceLROnPlateau, collate_with_macro\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/src/data/pipeline.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlflow\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mseries_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SeriesDataset\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CompanyDataLoader, MacroDataLoader\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtensor_factory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorFactory\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from src.models.gru import GRUModel\n",
    "from src.datasets.series_dataset import SeriesDataset\n",
    "from train_entry import TrainConfig\n",
    "\n",
    "def test_model_forward():\n",
    "    # Minimal config for testing\n",
    "    config = SimpleNamespace(\n",
    "        data_path=\"demo_data.xlsx\",\n",
    "        target_year=2025,\n",
    "        micro_window=3,\n",
    "        macro_window=5,\n",
    "        prediction_years=2,\n",
    "        batch_size=2,\n",
    "        hidden_dim=32,\n",
    "        dropout=0.2,\n",
    "        learning_rate=0.001,\n",
    "        n_epochs=1,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Initialize handler\n",
    "    handler = TrainConfig(config)\n",
    "    X_micro, X_macro, y, _, _ = handler.prepare_datasets()\n",
    "\n",
    "    # Use only the first batch for quick test\n",
    "    dataset = SeriesDataset(X_micro, X_macro, y)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=config.batch_size)\n",
    "\n",
    "    # Model instantiation\n",
    "    input_dim_micro = X_micro.shape[2]\n",
    "    input_dim_macro = X_macro.shape[2]\n",
    "    model = GRUModel(input_dim_micro, input_dim_macro, hidden_dim=config.hidden_dim)\n",
    "\n",
    "    # Sanity check on one batch\n",
    "    for x_micro_batch, x_macro_batch, y_batch in loader:\n",
    "        out = model(x_micro_batch, x_macro_batch)\n",
    "        assert out.shape == (config.batch_size, 1), f\"Unexpected output shape: {out.shape}\"\n",
    "        assert not torch.isnan(out).any(), \"Model output contains NaNs\"\n",
    "        break\n",
    "\n",
    "    print(\" Forward pass test successful.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_model_forward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainConfig.__init__() missing 3 required positional arguments: 'macro_data', 'bankruptcy_col', and 'company_col'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m Forward pass test successful.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[43mtest_model_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtest_model_forward\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     10\u001b[39m config = SimpleNamespace(\n\u001b[32m     11\u001b[39m     data_path=\u001b[33m\"\u001b[39m\u001b[33mdemo_data.xlsx\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     target_year=\u001b[32m2025\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     seed=\u001b[32m42\u001b[39m\n\u001b[32m     22\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Initialize handler\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m handler = \u001b[43mTrainConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m X_micro, X_macro, y, _, _ = handler.prepare_datasets()\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Use only the first batch for quick test\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: TrainConfig.__init__() missing 3 required positional arguments: 'macro_data', 'bankruptcy_col', and 'company_col'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from src.models.gru import GRUModel\n",
    "from src.datasets.series_dataset import SeriesDataset\n",
    "from train_entry import TrainConfig\n",
    "\n",
    "def test_model_forward():\n",
    "    # Minimal config for testing\n",
    "    config = SimpleNamespace(\n",
    "        data_path=\"demo_data.xlsx\",\n",
    "        target_year=2025,\n",
    "        micro_window=3,\n",
    "        macro_window=5,\n",
    "        prediction_years=2,\n",
    "        batch_size=2,\n",
    "        hidden_dim=32,\n",
    "        dropout=0.2,\n",
    "        learning_rate=0.001,\n",
    "        n_epochs=1,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Initialize handler\n",
    "    handler = TrainConfig(config)\n",
    "    X_micro, X_macro, y, _, _ = handler.prepare_datasets()\n",
    "\n",
    "    # Use only the first batch for quick test\n",
    "    dataset = SeriesDataset(X_micro, X_macro, y)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=config.batch_size)\n",
    "\n",
    "    # Model instantiation\n",
    "    input_dim_micro = X_micro.shape[2]\n",
    "    input_dim_macro = X_macro.shape[2]\n",
    "    model = GRUModel(input_dim_micro, input_dim_macro, hidden_dim=config.hidden_dim)\n",
    "\n",
    "    # Sanity check on one batch\n",
    "    for x_micro_batch, x_macro_batch, y_batch in loader:\n",
    "        out = model(x_micro_batch, x_macro_batch)\n",
    "        assert out.shape == (config.batch_size, 1), f\"Unexpected output shape: {out.shape}\"\n",
    "        assert not torch.isnan(out).any(), \"Model output contains NaNs\"\n",
    "        break\n",
    "\n",
    "    print(\" Forward pass test successful.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_model_forward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_entry.TrainConfig() argument after ** must be a mapping, not types.SimpleNamespace",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m Forward pass test successful.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[43mtest_model_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtest_model_forward\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     10\u001b[39m config = SimpleNamespace(\n\u001b[32m     11\u001b[39m     data_path=\u001b[33m\"\u001b[39m\u001b[33mdemo_data.xlsx\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     target_year=\u001b[32m2025\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     seed=\u001b[32m42\u001b[39m\n\u001b[32m     22\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Initialize handler\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m handler = TrainConfig(**config)\n\u001b[32m     26\u001b[39m X_micro, X_macro, y, _, _ = handler.prepare_datasets()\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Use only the first batch for quick test\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: train_entry.TrainConfig() argument after ** must be a mapping, not types.SimpleNamespace"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from src.models.gru import GRUModel\n",
    "from src.datasets.series_dataset import SeriesDataset\n",
    "from train_entry import TrainConfig\n",
    "\n",
    "def test_model_forward():\n",
    "    # Minimal config for testing\n",
    "    config = SimpleNamespace(\n",
    "        data_path=\"demo_data.xlsx\",\n",
    "        target_year=2025,\n",
    "        micro_window=3,\n",
    "        macro_window=5,\n",
    "        prediction_years=2,\n",
    "        batch_size=2,\n",
    "        hidden_dim=32,\n",
    "        dropout=0.2,\n",
    "        learning_rate=0.001,\n",
    "        n_epochs=1,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Initialize handler\n",
    "    handler = TrainConfig(**config)\n",
    "    X_micro, X_macro, y, _, _ = handler.prepare_datasets()\n",
    "\n",
    "    # Use only the first batch for quick test\n",
    "    dataset = SeriesDataset(X_micro, X_macro, y)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=config.batch_size)\n",
    "\n",
    "    # Model instantiation\n",
    "    input_dim_micro = X_micro.shape[2]\n",
    "    input_dim_macro = X_macro.shape[2]\n",
    "    model = GRUModel(input_dim_micro, input_dim_macro, hidden_dim=config.hidden_dim)\n",
    "\n",
    "    # Sanity check on one batch\n",
    "    for x_micro_batch, x_macro_batch, y_batch in loader:\n",
    "        out = model(x_micro_batch, x_macro_batch)\n",
    "        assert out.shape == (config.batch_size, 1), f\"Unexpected output shape: {out.shape}\"\n",
    "        assert not torch.isnan(out).any(), \"Model output contains NaNs\"\n",
    "        break\n",
    "\n",
    "    print(\" Forward pass test successful.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_model_forward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_model_from_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_model_from_config\u001b[49m(*config)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_model_from_config' is not defined"
     ]
    }
   ],
   "source": [
    "train_model_from_config(*config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_entry.TrainConfig() argument after * must be an iterable, not types.SimpleNamespace",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m Forward pass test successful.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[43mtest_model_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtest_model_forward\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     10\u001b[39m config = SimpleNamespace(\n\u001b[32m     11\u001b[39m     data_path=\u001b[33m\"\u001b[39m\u001b[33mdemo_data.xlsx\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     target_year=\u001b[32m2025\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     seed=\u001b[32m42\u001b[39m\n\u001b[32m     22\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Initialize handler\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m handler = \u001b[43mTrainConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m X_micro, X_macro, y, _, _ = handler.prepare_datasets()\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Use only the first batch for quick test\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: train_entry.TrainConfig() argument after * must be an iterable, not types.SimpleNamespace"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from src.models.gru import GRUModel\n",
    "from src.datasets.series_dataset import SeriesDataset\n",
    "from train_entry import TrainConfig\n",
    "\n",
    "def test_model_forward():\n",
    "    # Minimal config for testing\n",
    "    config = SimpleNamespace(\n",
    "        data_path=\"demo_data.xlsx\",\n",
    "        target_year=2025,\n",
    "        micro_window=3,\n",
    "        macro_window=5,\n",
    "        prediction_years=2,\n",
    "        batch_size=2,\n",
    "        hidden_dim=32,\n",
    "        dropout=0.2,\n",
    "        learning_rate=0.001,\n",
    "        n_epochs=1,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Initialize handler\n",
    "    handler = TrainConfig(*config)\n",
    "    X_micro, X_macro, y, _, _ = handler.prepare_datasets()\n",
    "\n",
    "    # Use only the first batch for quick test\n",
    "    dataset = SeriesDataset(X_micro, X_macro, y)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=config.batch_size)\n",
    "\n",
    "    # Model instantiation\n",
    "    input_dim_micro = X_micro.shape[2]\n",
    "    input_dim_macro = X_macro.shape[2]\n",
    "    model = GRUModel(input_dim_micro, input_dim_macro, hidden_dim=config.hidden_dim)\n",
    "\n",
    "    # Sanity check on one batch\n",
    "    for x_micro_batch, x_macro_batch, y_batch in loader:\n",
    "        out = model(x_micro_batch, x_macro_batch)\n",
    "        assert out.shape == (config.batch_size, 1), f\"Unexpected output shape: {out.shape}\"\n",
    "        assert not torch.isnan(out).any(), \"Model output contains NaNs\"\n",
    "        break\n",
    "\n",
    "    print(\" Forward pass test successful.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_model_forward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_entry.TrainConfig() argument after ** must be a mapping, not types.SimpleNamespace",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m Forward pass test successful.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[43mtest_model_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtest_model_forward\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     10\u001b[39m config = SimpleNamespace(\n\u001b[32m     11\u001b[39m     data_path=\u001b[33m\"\u001b[39m\u001b[33mdemo_data.xlsx\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     target_year=\u001b[32m2025\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     seed=\u001b[32m42\u001b[39m\n\u001b[32m     22\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Initialize handler\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m handler = TrainConfig(**config)\n\u001b[32m     26\u001b[39m X_micro, X_macro, y, _, _ = handler.prepare_datasets()\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Use only the first batch for quick test\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: train_entry.TrainConfig() argument after ** must be a mapping, not types.SimpleNamespace"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from src.models.gru import GRUModel\n",
    "from src.datasets.series_dataset import SeriesDataset\n",
    "from train_entry import TrainConfig\n",
    "\n",
    "def test_model_forward():\n",
    "    # Minimal config for testing\n",
    "    config = SimpleNamespace(\n",
    "        data_path=\"demo_data.xlsx\",\n",
    "        target_year=2025,\n",
    "        micro_window=3,\n",
    "        macro_window=5,\n",
    "        prediction_years=2,\n",
    "        batch_size=2,\n",
    "        hidden_dim=32,\n",
    "        dropout=0.2,\n",
    "        learning_rate=0.001,\n",
    "        n_epochs=1,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Initialize handler\n",
    "    handler = TrainConfig(**config)\n",
    "    X_micro, X_macro, y, _, _ = handler.prepare_datasets()\n",
    "\n",
    "    # Use only the first batch for quick test\n",
    "    dataset = SeriesDataset(X_micro, X_macro, y)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=config.batch_size)\n",
    "\n",
    "    # Model instantiation\n",
    "    input_dim_micro = X_micro.shape[2]\n",
    "    input_dim_macro = X_macro.shape[2]\n",
    "    model = GRUModel(input_dim_micro, input_dim_macro, hidden_dim=config.hidden_dim)\n",
    "\n",
    "    # Sanity check on one batch\n",
    "    for x_micro_batch, x_macro_batch, y_batch in loader:\n",
    "        out = model(x_micro_batch, x_macro_batch)\n",
    "        assert out.shape == (config.batch_size, 1), f\"Unexpected output shape: {out.shape}\"\n",
    "        assert not torch.isnan(out).any(), \"Model output contains NaNs\"\n",
    "        break\n",
    "\n",
    "    print(\" Forward pass test successful.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_model_forward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "import datetime\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Metric, MetricCollection\n",
    "from torchmetrics.classification import (\n",
    "    BinaryAccuracy, BinaryAUROC, BinaryF1Score, BinaryMatthewsCorrCoef,\n",
    "    MulticlassAccuracy, MulticlassAUROC, MulticlassF1Score)\n",
    "from pathlib import Path\n",
    "\n",
    "from src.datasets.dual_input import DualInputSequenceDataset\n",
    "from src.models.gru import GRUModel\n",
    "from src.data.pipeline import IngestionPipeline\n",
    "from src.train import train_model\n",
    "from src.utils.utils import CustomReduceLROnPlateau, collate_with_macro\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "def load_yaml_file(path):\n",
    "    with open(path) as stream:\n",
    "        try:\n",
    "            config_dict=yaml.safe_load(stream)\n",
    "            return config_dict\n",
    "        except yaml.YAMLError as e:\n",
    "            TypeError(f\"Config file could not be loaded: {e}\")\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    firm_data: str\n",
    "    macro_data: list[str]\n",
    "    bankruptcy_col: str\n",
    "    company_col: str\n",
    "    revenue_cap: int = 3_000\n",
    "    num_classes: int = 2\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 40\n",
    "    lr: float = 1e-3\n",
    "    hidden_size: int = 64\n",
    "    num_layers: int = 2\n",
    "    dropout: float = .2\n",
    "    threshold: float = 0.5\n",
    "    scheduler_factor: float=0.85\n",
    "    scheduler_patience: int = 50\n",
    "    min_lr:float = 0.0\n",
    "    decay_ih: float = 1e-5\n",
    "    decay_hh: float = 1e-5\n",
    "    decay_other: float = 1e-5\n",
    "    train_fract: float = .8\n",
    "    seed: int = 42\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "    metrics: list[str] = field(default_factory=lambda: [\"f1\", \"accuracy\"])\n",
    "    \n",
    "    def get_metrics(self) -> MetricCollection:\n",
    "        \"\"\"Constructs a MetricCollection from the specified config\"\"\"\n",
    "        if self.num_classes == 2:\n",
    "            available = {\n",
    "                \"f1\": BinaryF1Score(self.threshold),\n",
    "                \"accuracy\": BinaryAccuracy(self.threshold),\n",
    "                \"auroc\": BinaryAUROC(),\n",
    "                \"matthews\": BinaryMatthewsCorrCoef(self.threshold)\n",
    "            }\n",
    "        else:\n",
    "            available = {\n",
    "                \"f1\": MulticlassF1Score(num_classes=self.num_classes),\n",
    "                \"accuracy\": MulticlassAccuracy(num_classes=self.num_classes),\n",
    "                \"auroc\": MulticlassAUROC(num_classes=self.num_classes)\n",
    "            }\n",
    "        selected = {k: available[k] for k in self.metrics if k in available}\n",
    "        return MetricCollection(selected)\n",
    "    \n",
    "def _make_class_weights(labels: torch.Tensor, num_classes:int) -> torch.Tensor:\n",
    "    \"\"\"Helper function to compute class weights.\"\"\"\n",
    "    counts = torch.bincount(labels.long(), minlength=num_classes)\n",
    "    weights = counts.sum() / (num_classes * counts.float())\n",
    "    return weights\n",
    "    \n",
    "def train_model_from_config(cfg: TrainConfig) -> GRUModel:\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    return train(\n",
    "        company_data_path = Path(cfg.firm_data),\n",
    "        macro_data_path = [Path(path) for path in cfg.macro_data],\n",
    "        bankruptcy_col = cfg.bankruptcy_col,\n",
    "        company_col=cfg.company_col,\n",
    "        revenue_cap=cfg.revenue_cap,\n",
    "        metrics=cfg.get_metrics().to(cfg.device),\n",
    "        device=cfg.device,\n",
    "        num_layers=cfg.num_classes,\n",
    "        hidden_size=cfg.hidden_size,\n",
    "        output_size=cfg.num_classes,\n",
    "        epochs=cfg.epochs,\n",
    "        lr=cfg.lr,\n",
    "        train_fract=cfg.train_fract,\n",
    "        dropout=cfg.dropout,\n",
    "        scheduler_factor=cfg.scheduler_factor,\n",
    "        scheduler_patience=cfg.scheduler_patience,\n",
    "        decay_ih=cfg.decay_ih,\n",
    "        decay_hh=cfg.decay_hh,\n",
    "        decay_other=cfg.decay_other,\n",
    "        seed=cfg.seed\n",
    "    )\n",
    "\n",
    "def train(\n",
    "    company_data_path: str,\n",
    "    macro_data_path: list[str],\n",
    "    bankruptcy_col: str,\n",
    "    company_col: str,\n",
    "    revenue_cap: int,\n",
    "    metrics: list[Metric],\n",
    "    seed: int,\n",
    "    num_layers: int = 2,\n",
    "    hidden_size: int = 64,\n",
    "    output_size: int = 1,\n",
    "    epochs: int = 50,\n",
    "    lr: float = 1e-3,\n",
    "    train_fract: float = 0.8,\n",
    "    dropout: float = 0.2,\n",
    "    scheduler_factor: float = 0.85,\n",
    "    scheduler_patience: int = 50,\n",
    "    min_lr: float = 0.0,\n",
    "    decay_ih:float = 1e-5,\n",
    "    decay_hh:float = 1e-5,\n",
    "    decay_other:float = 1e-5,\n",
    "    device: str=\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "):  \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    ingestion = IngestionPipeline(\n",
    "        company_data_path=company_data_path,\n",
    "        macro_data_path=macro_data_path,\n",
    "        company_col=company_col,\n",
    "        bankruptcy_col=bankruptcy_col,\n",
    "        sheet_name=\"Results\",\n",
    "        revenue_cap=revenue_cap\n",
    "    )\n",
    "    ingestion.load()\n",
    "    series = ingestion.process_data()\n",
    "    financials, macro, labels = series.export_tensors()\n",
    "    \n",
    "    dataset = DualInputSequenceDataset(\n",
    "        firm_tensor = financials,\n",
    "        macro_tensor = macro,\n",
    "        labels = labels\n",
    "    )\n",
    "    \n",
    "    train_ds, val_ds, seed = dataset.stratified_split(train_fract)\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_with_macro)\n",
    "    val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=collate_with_macro)\n",
    "\n",
    "    logger.info(f\"Device: {device}\")\n",
    "    \n",
    "    metrics_dict = dict()\n",
    "    for metric in metrics:\n",
    "        metrics_dict[metric._get_name()] = metric.to(device)\n",
    "    metrics = metrics_dict \n",
    "    \n",
    "    train_ds = train_ds.to_device(device)\n",
    "    val_ds = val_ds.to_device(device)\n",
    "    \n",
    "    firm_input_size, macro_input_size = dataset.input_dims()\n",
    "    \n",
    "    mlflow.set_tracking_uri('http://127.0.0.1:8080')\n",
    "    mlflow.set_experiment('bankruptcy-predictions')\n",
    "    \n",
    "    mlflow.log_param(\"seed\", seed)\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        model = GRUModel(\n",
    "            firm_input_size=firm_input_size,\n",
    "            macro_input_size=macro_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=output_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        model = model.to(device)\n",
    "        \n",
    "        pos_weight = dataset.pos_weight()\n",
    "        loss_fn = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        \n",
    "        # Logging hyperparameters\n",
    "        mlflow.log_param(\"hidden_size\", hidden_size)\n",
    "        mlflow.log_param(\"output_size\", output_size)\n",
    "        mlflow.log_param(\"num_layers\", num_layers)\n",
    "        mlflow.log_param(\"dropout\", dropout)\n",
    "        mlflow.log_param(\"lr\", lr)\n",
    "\n",
    "        ih_params = []\n",
    "        hh_params = []\n",
    "        other_params = []\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                ih_params.append(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                hh_params.append(param)\n",
    "            else:\n",
    "                other_params.append(param)\n",
    "        \n",
    "        optimizer = Adam([\n",
    "                {'params': ih_params, 'weight_decay': decay_ih},\n",
    "                {'params': hh_params, 'weight_decay': decay_hh},\n",
    "                {'params': other_params, 'weight_decay': decay_other},\n",
    "            ], lr=lr\n",
    "        )\n",
    "        scheduler=CustomReduceLROnPlateau(\n",
    "            optimizer=optimizer,\n",
    "            mode=\"min\",\n",
    "            factor=scheduler_factor,\n",
    "            patience=scheduler_patience,\n",
    "            min_lr=min_lr\n",
    "        )\n",
    "        \n",
    "        train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            epochs=epochs,\n",
    "            metrics=metrics\n",
    "        )\n",
    "        \n",
    "        mlflow.pytorch.log_model(model, f\"model_{datetime.datetime.now()}\")\n",
    "        torch.save(obj = model.state_dict(), f = f\"model_{datetime.datetime.now()}.pth\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'config/model_config.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m config_dict = \u001b[43mload_yaml_file\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfig/model_config.yaml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mload_yaml_file\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_yaml_file\u001b[39m(path):\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[32m     31\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     32\u001b[39m             config_dict=yaml.safe_load(stream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'config/model_config.yaml'"
     ]
    }
   ],
   "source": [
    "config_dict = load_yaml_file(\"config/model_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = load_yaml_file(\"config/model_config.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'firm_data': 'data/demo_data.xlsx',\n",
       " 'macro_data': ['insee/serie_000857176_04042025/valeurs_mensuelles.csv',\n",
       "  'insee/serie_000857180_04042025/valeurs_mensuelles.csv',\n",
       "  'insee/serie_001763782_04042025/valeurs_mensuelles.csv'],\n",
       " 'bankruptcy_col': 'Status date',\n",
       " 'company_col': 'Company name Latin alphabet',\n",
       " 'revenue_cap': 3000,\n",
       " 'num_classes': 2,\n",
       " 'batch_size': 32,\n",
       " 'epochs': 100,\n",
       " 'lr': '1e-3',\n",
       " 'hidden_size': 64,\n",
       " 'num_layers': 2,\n",
       " 'dropout': 0.4,\n",
       " 'threshold': 0.4,\n",
       " 'scheduler_factor': 0.85,\n",
       " 'scheduler_patience': 50,\n",
       " 'min_lr': 0.0,\n",
       " 'decay_ih': '1e-5',\n",
       " 'decay_hh': '1e-4',\n",
       " 'decay_other': '1e-5',\n",
       " 'train_fract': 0.8,\n",
       " 'seed': 2025,\n",
       " 'device': 'mps',\n",
       " 'metrics': ['f1', 'accuracy', 'auroc', 'matthews']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainConfig(**config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainConfig(firm_data='data/demo_data.xlsx', macro_data=['insee/serie_000857176_04042025/valeurs_mensuelles.csv', 'insee/serie_000857180_04042025/valeurs_mensuelles.csv', 'insee/serie_001763782_04042025/valeurs_mensuelles.csv'], bankruptcy_col='Status date', company_col='Company name Latin alphabet', revenue_cap=3000, num_classes=2, batch_size=32, epochs=100, lr='1e-3', hidden_size=64, num_layers=2, dropout=0.4, threshold=0.4, scheduler_factor=0.85, scheduler_patience=50, min_lr=0.0, decay_ih='1e-5', decay_hh='1e-4', decay_other='1e-5', train_fract=0.8, seed=2025, device='mps', metrics=['f1', 'accuracy', 'auroc', 'matthews'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_data_path = Path(cfg.firm_data),\n",
    "macro_data_path = [Path(path) for path in cfg.macro_data],\n",
    "bankruptcy_col = cfg.bankruptcy_col,\n",
    "company_col=cfg.company_col,\n",
    "revenue_cap=cfg.revenue_cap,\n",
    "metrics=cfg.get_metrics().to(cfg.device),\n",
    "device=cfg.device,\n",
    "num_layers=cfg.num_classes,\n",
    "hidden_size=cfg.hidden_size,\n",
    "output_size=cfg.num_classes,\n",
    "epochs=cfg.epochs,\n",
    "lr=cfg.lr,\n",
    "train_fract=cfg.train_fract,\n",
    "dropout=cfg.dropout,\n",
    "scheduler_factor=cfg.scheduler_factor,\n",
    "scheduler_patience=cfg.scheduler_patience,\n",
    "decay_ih=cfg.decay_ih,\n",
    "decay_hh=cfg.decay_hh,\n",
    "decay_other=cfg.decay_other,\n",
    "seed=cfg.seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "IngestionPipeline.__init__() got an unexpected keyword argument 'company_data_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m ingestion = \u001b[43mIngestionPipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompany_data_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompany_data_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmacro_data_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmacro_data_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mResults\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevenue_cap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevenue_cap\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: IngestionPipeline.__init__() got an unexpected keyword argument 'company_data_path'"
     ]
    }
   ],
   "source": [
    "ingestion = IngestionPipeline(\n",
    "    company_data_path=company_data_path,\n",
    "    macro_data_path=macro_data_path,\n",
    "    company_col=company_col,\n",
    "    bankruptcy_col=bankruptcy_col,\n",
    "    sheet_name=\"Results\",\n",
    "    revenue_cap=revenue_cap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "IngestionPipeline.__init__() got an unexpected keyword argument 'macro_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m ingestion = \u001b[43mIngestionPipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompany_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompany_data_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmacro_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmacro_data_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mResults\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevenue_cap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevenue_cap\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: IngestionPipeline.__init__() got an unexpected keyword argument 'macro_path'"
     ]
    }
   ],
   "source": [
    "ingestion = IngestionPipeline(\n",
    "    company_path=company_data_path,\n",
    "    macro_path=macro_data_path,\n",
    "    company_col=company_col,\n",
    "    bankruptcy_col=bankruptcy_col,\n",
    "    sheet_name=\"Results\",\n",
    "    revenue_cap=revenue_cap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "IngestionPipeline.__init__() got an unexpected keyword argument 'sheet_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m ingestion = \u001b[43mIngestionPipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompany_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompany_data_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmacro_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmacro_data_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mResults\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevenue_cap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevenue_cap\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: IngestionPipeline.__init__() got an unexpected keyword argument 'sheet_name'"
     ]
    }
   ],
   "source": [
    "ingestion = IngestionPipeline(\n",
    "    company_path=company_data_path,\n",
    "    macro_paths=macro_data_path,\n",
    "    company_col=company_col,\n",
    "    bankruptcy_col=bankruptcy_col,\n",
    "    sheet_name=\"Results\",\n",
    "    revenue_cap=revenue_cap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestion = IngestionPipeline(\n",
    "    company_path=company_data_path,\n",
    "    macro_paths=macro_data_path,\n",
    "    company_col=company_col,\n",
    "    bankruptcy_col=bankruptcy_col,\n",
    "    revenue_cap=revenue_cap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'IngestionPipeline' object has no attribute 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mingestion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m()\n",
      "\u001b[31mAttributeError\u001b[39m: 'IngestionPipeline' object has no attribute 'load'"
     ]
    }
   ],
   "source": [
    "ingestion.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.loaders:Reading file: (PosixPath('data/demo_data.xlsx'),)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid file path or buffer object type: <class 'tuple'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mingestion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/src/data/pipeline.py:41\u001b[39m, in \u001b[36mIngestionPipeline.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     company_df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompany_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     macro_df= \u001b[38;5;28mself\u001b[39m.macro_loader.load()\n\u001b[32m     49\u001b[39m     years=\u001b[38;5;28mself\u001b[39m.company_loader.years\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py:22\u001b[39m, in \u001b[36mCompanyDataLoader.load\u001b[39m\u001b[34m(self, company_col, bankruptcy_col, mode)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mself\u001b[39m.bankruptcy_col=bankruptcy_col\n\u001b[32m     20\u001b[39m \u001b[38;5;28mself\u001b[39m.mode = mode\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mself\u001b[39m.years = \u001b[38;5;28msorted\u001b[39m({col[-\u001b[32m4\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df.columns \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mrevenue\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m col})\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._clean(df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py:28\u001b[39m, in \u001b[36mCompanyDataLoader._read_file\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_file\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> pd.DataFrame:\n\u001b[32m     27\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReading file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn.a.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompany_col\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/io/excel/_base.py:495\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[32m    494\u001b[39m     should_close = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     io = \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine != io.engine:\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/io/excel/_base.py:1550\u001b[39m, in \u001b[36mExcelFile.__init__\u001b[39m\u001b[34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   1548\u001b[39m     ext = \u001b[33m\"\u001b[39m\u001b[33mxls\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     ext = \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m   1552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1554\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1555\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mExcel file format cannot be determined, you must specify \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1556\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33man engine manually.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1557\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/io/excel/_base.py:1402\u001b[39m, in \u001b[36minspect_excel_format\u001b[39m\u001b[34m(content_or_path, storage_options)\u001b[39m\n\u001b[32m   1399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m   1400\u001b[39m     content_or_path = BytesIO(content_or_path)\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m   1404\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[32m   1405\u001b[39m     stream = handle.handle\n\u001b[32m   1406\u001b[39m     stream.seek(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/io/common.py:728\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    725\u001b[39m     codecs.lookup_error(errors)\n\u001b[32m    727\u001b[39m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m ioargs = \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    736\u001b[39m handle = ioargs.filepath_or_buffer\n\u001b[32m    737\u001b[39m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/io/common.py:472\u001b[39m, in \u001b[36m_get_filepath_or_buffer\u001b[39m\u001b[34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[39m\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[32m    469\u001b[39m     \u001b[38;5;28mhasattr\u001b[39m(filepath_or_buffer, \u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(filepath_or_buffer, \u001b[33m\"\u001b[39m\u001b[33mwrite\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    470\u001b[39m ):\n\u001b[32m    471\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid file path or buffer object type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(filepath_or_buffer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m IOArgs(\n\u001b[32m    475\u001b[39m     filepath_or_buffer=filepath_or_buffer,\n\u001b[32m    476\u001b[39m     encoding=encoding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    479\u001b[39m     mode=mode,\n\u001b[32m    480\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Invalid file path or buffer object type: <class 'tuple'>"
     ]
    }
   ],
   "source": [
    "ingestion.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainConfig(firm_data='data/demo_data.xlsx', macro_data=['insee/serie_000857176_04042025/valeurs_mensuelles.csv', 'insee/serie_000857180_04042025/valeurs_mensuelles.csv', 'insee/serie_001763782_04042025/valeurs_mensuelles.csv'], bankruptcy_col='Status date', company_col='Company name Latin alphabet', revenue_cap=3000, num_classes=2, batch_size=32, epochs=100, lr='1e-3', hidden_size=64, num_layers=2, dropout=0.4, threshold=0.4, scheduler_factor=0.85, scheduler_patience=50, min_lr=0.0, decay_ih='1e-5', decay_hh='1e-4', decay_other='1e-5', train_fract=0.8, seed=2025, device='mps', metrics=['f1', 'accuracy', 'auroc', 'matthews'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('data/demo_data.xlsx'),)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingestion.company_loader.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data/demo_data.xlsx')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingestion.company_loader.path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reload' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mreload\u001b[49m(src.data.loaders)\n",
      "\u001b[31mNameError\u001b[39m: name 'reload' is not defined"
     ]
    }
   ],
   "source": [
    "reload(src.data.loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'src' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m importlib.reload(\u001b[43msrc\u001b[49m.data.loaders)\n",
      "\u001b[31mNameError\u001b[39m: name 'src' is not defined"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(src.data.loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m importlib.reload(\u001b[43mdata\u001b[49m.loaders)\n",
      "\u001b[31mNameError\u001b[39m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(data.loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loaders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m importlib.reload(\u001b[43mloaders\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'loaders' is not defined"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'src' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m importlib.reload(\u001b[43msrc\u001b[49m.data.loaders)\n",
      "\u001b[31mNameError\u001b[39m: name 'src' is not defined"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(src.data.loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "import datetime\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Metric, MetricCollection\n",
    "from torchmetrics.classification import (\n",
    "    BinaryAccuracy, BinaryAUROC, BinaryF1Score, BinaryMatthewsCorrCoef,\n",
    "    MulticlassAccuracy, MulticlassAUROC, MulticlassF1Score)\n",
    "from pathlib import Path\n",
    "\n",
    "from src.datasets.dual_input import DualInputSequenceDataset\n",
    "from src.models.gru import GRUModel\n",
    "from src.data.pipeline import IngestionPipeline\n",
    "from src.train import train_model\n",
    "from src.utils.utils import CustomReduceLROnPlateau, collate_with_macro\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "def load_yaml_file(path):\n",
    "    with open(path) as stream:\n",
    "        try:\n",
    "            config_dict=yaml.safe_load(stream)\n",
    "            return config_dict\n",
    "        except yaml.YAMLError as e:\n",
    "            TypeError(f\"Config file could not be loaded: {e}\")\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    firm_data: str\n",
    "    macro_data: list[str]\n",
    "    bankruptcy_col: str\n",
    "    company_col: str\n",
    "    revenue_cap: int = 3_000\n",
    "    num_classes: int = 2\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 40\n",
    "    lr: float = 1e-3\n",
    "    hidden_size: int = 64\n",
    "    num_layers: int = 2\n",
    "    dropout: float = .2\n",
    "    threshold: float = 0.5\n",
    "    scheduler_factor: float=0.85\n",
    "    scheduler_patience: int = 50\n",
    "    min_lr:float = 0.0\n",
    "    decay_ih: float = 1e-5\n",
    "    decay_hh: float = 1e-5\n",
    "    decay_other: float = 1e-5\n",
    "    train_fract: float = .8\n",
    "    seed: int = 42\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "    metrics: list[str] = field(default_factory=lambda: [\"f1\", \"accuracy\"])\n",
    "    \n",
    "    def get_metrics(self) -> MetricCollection:\n",
    "        \"\"\"Constructs a MetricCollection from the specified config\"\"\"\n",
    "        if self.num_classes == 2:\n",
    "            available = {\n",
    "                \"f1\": BinaryF1Score(self.threshold),\n",
    "                \"accuracy\": BinaryAccuracy(self.threshold),\n",
    "                \"auroc\": BinaryAUROC(),\n",
    "                \"matthews\": BinaryMatthewsCorrCoef(self.threshold)\n",
    "            }\n",
    "        else:\n",
    "            available = {\n",
    "                \"f1\": MulticlassF1Score(num_classes=self.num_classes),\n",
    "                \"accuracy\": MulticlassAccuracy(num_classes=self.num_classes),\n",
    "                \"auroc\": MulticlassAUROC(num_classes=self.num_classes)\n",
    "            }\n",
    "        selected = {k: available[k] for k in self.metrics if k in available}\n",
    "        return MetricCollection(selected)\n",
    "    \n",
    "def _make_class_weights(labels: torch.Tensor, num_classes:int) -> torch.Tensor:\n",
    "    \"\"\"Helper function to compute class weights.\"\"\"\n",
    "    counts = torch.bincount(labels.long(), minlength=num_classes)\n",
    "    weights = counts.sum() / (num_classes * counts.float())\n",
    "    return weights\n",
    "    \n",
    "def train_model_from_config(cfg: TrainConfig) -> GRUModel:\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    return train(\n",
    "        company_data_path = Path(cfg.firm_data),\n",
    "        macro_data_path = [Path(path) for path in cfg.macro_data],\n",
    "        bankruptcy_col = cfg.bankruptcy_col,\n",
    "        company_col=cfg.company_col,\n",
    "        revenue_cap=cfg.revenue_cap,\n",
    "        metrics=cfg.get_metrics().to(cfg.device),\n",
    "        device=cfg.device,\n",
    "        num_layers=cfg.num_classes,\n",
    "        hidden_size=cfg.hidden_size,\n",
    "        output_size=cfg.num_classes,\n",
    "        epochs=cfg.epochs,\n",
    "        lr=cfg.lr,\n",
    "        train_fract=cfg.train_fract,\n",
    "        dropout=cfg.dropout,\n",
    "        scheduler_factor=cfg.scheduler_factor,\n",
    "        scheduler_patience=cfg.scheduler_patience,\n",
    "        decay_ih=cfg.decay_ih,\n",
    "        decay_hh=cfg.decay_hh,\n",
    "        decay_other=cfg.decay_other,\n",
    "        seed=cfg.seed\n",
    "    )\n",
    "\n",
    "def train(\n",
    "    company_data_path: str,\n",
    "    macro_data_path: list[str],\n",
    "    bankruptcy_col: str,\n",
    "    company_col: str,\n",
    "    revenue_cap: int,\n",
    "    metrics: list[Metric],\n",
    "    seed: int,\n",
    "    num_layers: int = 2,\n",
    "    hidden_size: int = 64,\n",
    "    output_size: int = 1,\n",
    "    epochs: int = 50,\n",
    "    lr: float = 1e-3,\n",
    "    train_fract: float = 0.8,\n",
    "    dropout: float = 0.2,\n",
    "    scheduler_factor: float = 0.85,\n",
    "    scheduler_patience: int = 50,\n",
    "    min_lr: float = 0.0,\n",
    "    decay_ih:float = 1e-5,\n",
    "    decay_hh:float = 1e-5,\n",
    "    decay_other:float = 1e-5,\n",
    "    device: str=\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "):  \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    ingestion = IngestionPipeline(\n",
    "        company_data_path=company_data_path,\n",
    "        macro_data_path=macro_data_path,\n",
    "        company_col=company_col,\n",
    "        bankruptcy_col=bankruptcy_col,\n",
    "        sheet_name=\"Results\",\n",
    "        revenue_cap=revenue_cap\n",
    "    )\n",
    "    ingestion.load()\n",
    "    series = ingestion.process_data()\n",
    "    financials, macro, labels = series.export_tensors()\n",
    "    \n",
    "    dataset = DualInputSequenceDataset(\n",
    "        firm_tensor = financials,\n",
    "        macro_tensor = macro,\n",
    "        labels = labels\n",
    "    )\n",
    "    \n",
    "    train_ds, val_ds, seed = dataset.stratified_split(train_fract)\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_with_macro)\n",
    "    val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=collate_with_macro)\n",
    "\n",
    "    logger.info(f\"Device: {device}\")\n",
    "    \n",
    "    metrics_dict = dict()\n",
    "    for metric in metrics:\n",
    "        metrics_dict[metric._get_name()] = metric.to(device)\n",
    "    metrics = metrics_dict \n",
    "    \n",
    "    train_ds = train_ds.to_device(device)\n",
    "    val_ds = val_ds.to_device(device)\n",
    "    \n",
    "    firm_input_size, macro_input_size = dataset.input_dims()\n",
    "    \n",
    "    mlflow.set_tracking_uri('http://127.0.0.1:8080')\n",
    "    mlflow.set_experiment('bankruptcy-predictions')\n",
    "    \n",
    "    mlflow.log_param(\"seed\", seed)\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        model = GRUModel(\n",
    "            firm_input_size=firm_input_size,\n",
    "            macro_input_size=macro_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=output_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        model = model.to(device)\n",
    "        \n",
    "        pos_weight = dataset.pos_weight()\n",
    "        loss_fn = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        \n",
    "        # Logging hyperparameters\n",
    "        mlflow.log_param(\"hidden_size\", hidden_size)\n",
    "        mlflow.log_param(\"output_size\", output_size)\n",
    "        mlflow.log_param(\"num_layers\", num_layers)\n",
    "        mlflow.log_param(\"dropout\", dropout)\n",
    "        mlflow.log_param(\"lr\", lr)\n",
    "\n",
    "        ih_params = []\n",
    "        hh_params = []\n",
    "        other_params = []\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                ih_params.append(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                hh_params.append(param)\n",
    "            else:\n",
    "                other_params.append(param)\n",
    "        \n",
    "        optimizer = Adam([\n",
    "                {'params': ih_params, 'weight_decay': decay_ih},\n",
    "                {'params': hh_params, 'weight_decay': decay_hh},\n",
    "                {'params': other_params, 'weight_decay': decay_other},\n",
    "            ], lr=lr\n",
    "        )\n",
    "        scheduler=CustomReduceLROnPlateau(\n",
    "            optimizer=optimizer,\n",
    "            mode=\"min\",\n",
    "            factor=scheduler_factor,\n",
    "            patience=scheduler_patience,\n",
    "            min_lr=min_lr\n",
    "        )\n",
    "        \n",
    "        train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            epochs=epochs,\n",
    "            metrics=metrics\n",
    "        )\n",
    "        \n",
    "        mlflow.pytorch.log_model(model, f\"model_{datetime.datetime.now()}\")\n",
    "        torch.save(obj = model.state_dict(), f = f\"model_{datetime.datetime.now()}.pth\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestion = IngestionPipeline(\n",
    "    company_path=company_data_path,\n",
    "    macro_paths=macro_data_path,\n",
    "    company_col=company_col,\n",
    "    bankruptcy_col=bankruptcy_col,\n",
    "    revenue_cap=revenue_cap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('data/demo_data.xlsx'),)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingestion.company_loader.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restarted base (Python 3.12.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "import datetime\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Metric, MetricCollection\n",
    "from torchmetrics.classification import (\n",
    "    BinaryAccuracy, BinaryAUROC, BinaryF1Score, BinaryMatthewsCorrCoef,\n",
    "    MulticlassAccuracy, MulticlassAUROC, MulticlassF1Score)\n",
    "from pathlib import Path\n",
    "\n",
    "from src.datasets.dual_input import DualInputSequenceDataset\n",
    "from src.models.gru import GRUModel\n",
    "from src.data.pipeline import IngestionPipeline\n",
    "from src.train import train_model\n",
    "from src.utils.utils import CustomReduceLROnPlateau, collate_with_macro\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "def load_yaml_file(path):\n",
    "    with open(path) as stream:\n",
    "        try:\n",
    "            config_dict=yaml.safe_load(stream)\n",
    "            return config_dict\n",
    "        except yaml.YAMLError as e:\n",
    "            TypeError(f\"Config file could not be loaded: {e}\")\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    firm_data: str\n",
    "    macro_data: list[str]\n",
    "    bankruptcy_col: str\n",
    "    company_col: str\n",
    "    revenue_cap: int = 3_000\n",
    "    num_classes: int = 2\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 40\n",
    "    lr: float = 1e-3\n",
    "    hidden_size: int = 64\n",
    "    num_layers: int = 2\n",
    "    dropout: float = .2\n",
    "    threshold: float = 0.5\n",
    "    scheduler_factor: float=0.85\n",
    "    scheduler_patience: int = 50\n",
    "    min_lr:float = 0.0\n",
    "    decay_ih: float = 1e-5\n",
    "    decay_hh: float = 1e-5\n",
    "    decay_other: float = 1e-5\n",
    "    train_fract: float = .8\n",
    "    seed: int = 42\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "    metrics: list[str] = field(default_factory=lambda: [\"f1\", \"accuracy\"])\n",
    "    \n",
    "    def get_metrics(self) -> MetricCollection:\n",
    "        \"\"\"Constructs a MetricCollection from the specified config\"\"\"\n",
    "        if self.num_classes == 2:\n",
    "            available = {\n",
    "                \"f1\": BinaryF1Score(self.threshold),\n",
    "                \"accuracy\": BinaryAccuracy(self.threshold),\n",
    "                \"auroc\": BinaryAUROC(),\n",
    "                \"matthews\": BinaryMatthewsCorrCoef(self.threshold)\n",
    "            }\n",
    "        else:\n",
    "            available = {\n",
    "                \"f1\": MulticlassF1Score(num_classes=self.num_classes),\n",
    "                \"accuracy\": MulticlassAccuracy(num_classes=self.num_classes),\n",
    "                \"auroc\": MulticlassAUROC(num_classes=self.num_classes)\n",
    "            }\n",
    "        selected = {k: available[k] for k in self.metrics if k in available}\n",
    "        return MetricCollection(selected)\n",
    "    \n",
    "def _make_class_weights(labels: torch.Tensor, num_classes:int) -> torch.Tensor:\n",
    "    \"\"\"Helper function to compute class weights.\"\"\"\n",
    "    counts = torch.bincount(labels.long(), minlength=num_classes)\n",
    "    weights = counts.sum() / (num_classes * counts.float())\n",
    "    return weights\n",
    "    \n",
    "def train_model_from_config(cfg: TrainConfig) -> GRUModel:\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    return train(\n",
    "        company_data_path = Path(cfg.firm_data),\n",
    "        macro_data_path = [Path(path) for path in cfg.macro_data],\n",
    "        bankruptcy_col = cfg.bankruptcy_col,\n",
    "        company_col=cfg.company_col,\n",
    "        revenue_cap=cfg.revenue_cap,\n",
    "        metrics=cfg.get_metrics().to(cfg.device),\n",
    "        device=cfg.device,\n",
    "        num_layers=cfg.num_classes,\n",
    "        hidden_size=cfg.hidden_size,\n",
    "        output_size=cfg.num_classes,\n",
    "        epochs=cfg.epochs,\n",
    "        lr=cfg.lr,\n",
    "        train_fract=cfg.train_fract,\n",
    "        dropout=cfg.dropout,\n",
    "        scheduler_factor=cfg.scheduler_factor,\n",
    "        scheduler_patience=cfg.scheduler_patience,\n",
    "        decay_ih=cfg.decay_ih,\n",
    "        decay_hh=cfg.decay_hh,\n",
    "        decay_other=cfg.decay_other,\n",
    "        seed=cfg.seed\n",
    "    )\n",
    "\n",
    "def train(\n",
    "    company_data_path: str,\n",
    "    macro_data_path: list[str],\n",
    "    bankruptcy_col: str,\n",
    "    company_col: str,\n",
    "    revenue_cap: int,\n",
    "    metrics: list[Metric],\n",
    "    seed: int,\n",
    "    num_layers: int = 2,\n",
    "    hidden_size: int = 64,\n",
    "    output_size: int = 1,\n",
    "    epochs: int = 50,\n",
    "    lr: float = 1e-3,\n",
    "    train_fract: float = 0.8,\n",
    "    dropout: float = 0.2,\n",
    "    scheduler_factor: float = 0.85,\n",
    "    scheduler_patience: int = 50,\n",
    "    min_lr: float = 0.0,\n",
    "    decay_ih:float = 1e-5,\n",
    "    decay_hh:float = 1e-5,\n",
    "    decay_other:float = 1e-5,\n",
    "    device: str=\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "):  \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    ingestion = IngestionPipeline(\n",
    "        company_data_path=company_data_path,\n",
    "        macro_data_path=macro_data_path,\n",
    "        company_col=company_col,\n",
    "        bankruptcy_col=bankruptcy_col,\n",
    "        sheet_name=\"Results\",\n",
    "        revenue_cap=revenue_cap\n",
    "    )\n",
    "    ingestion.load()\n",
    "    series = ingestion.process_data()\n",
    "    financials, macro, labels = series.export_tensors()\n",
    "    \n",
    "    dataset = DualInputSequenceDataset(\n",
    "        firm_tensor = financials,\n",
    "        macro_tensor = macro,\n",
    "        labels = labels\n",
    "    )\n",
    "    \n",
    "    train_ds, val_ds, seed = dataset.stratified_split(train_fract)\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_with_macro)\n",
    "    val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=collate_with_macro)\n",
    "\n",
    "    logger.info(f\"Device: {device}\")\n",
    "    \n",
    "    metrics_dict = dict()\n",
    "    for metric in metrics:\n",
    "        metrics_dict[metric._get_name()] = metric.to(device)\n",
    "    metrics = metrics_dict \n",
    "    \n",
    "    train_ds = train_ds.to_device(device)\n",
    "    val_ds = val_ds.to_device(device)\n",
    "    \n",
    "    firm_input_size, macro_input_size = dataset.input_dims()\n",
    "    \n",
    "    mlflow.set_tracking_uri('http://127.0.0.1:8080')\n",
    "    mlflow.set_experiment('bankruptcy-predictions')\n",
    "    \n",
    "    mlflow.log_param(\"seed\", seed)\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        model = GRUModel(\n",
    "            firm_input_size=firm_input_size,\n",
    "            macro_input_size=macro_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=output_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        model = model.to(device)\n",
    "        \n",
    "        pos_weight = dataset.pos_weight()\n",
    "        loss_fn = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        \n",
    "        # Logging hyperparameters\n",
    "        mlflow.log_param(\"hidden_size\", hidden_size)\n",
    "        mlflow.log_param(\"output_size\", output_size)\n",
    "        mlflow.log_param(\"num_layers\", num_layers)\n",
    "        mlflow.log_param(\"dropout\", dropout)\n",
    "        mlflow.log_param(\"lr\", lr)\n",
    "\n",
    "        ih_params = []\n",
    "        hh_params = []\n",
    "        other_params = []\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                ih_params.append(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                hh_params.append(param)\n",
    "            else:\n",
    "                other_params.append(param)\n",
    "        \n",
    "        optimizer = Adam([\n",
    "                {'params': ih_params, 'weight_decay': decay_ih},\n",
    "                {'params': hh_params, 'weight_decay': decay_hh},\n",
    "                {'params': other_params, 'weight_decay': decay_other},\n",
    "            ], lr=lr\n",
    "        )\n",
    "        scheduler=CustomReduceLROnPlateau(\n",
    "            optimizer=optimizer,\n",
    "            mode=\"min\",\n",
    "            factor=scheduler_factor,\n",
    "            patience=scheduler_patience,\n",
    "            min_lr=min_lr\n",
    "        )\n",
    "        \n",
    "        train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            epochs=epochs,\n",
    "            metrics=metrics\n",
    "        )\n",
    "        \n",
    "        mlflow.pytorch.log_model(model, f\"model_{datetime.datetime.now()}\")\n",
    "        torch.save(obj = model.state_dict(), f = f\"model_{datetime.datetime.now()}.pth\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = load_yaml_file(\"config/model_config.yml\")\n",
    "cfg = TrainConfig(**config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_data_path = Path(cfg.firm_data),\n",
    "macro_data_path = [Path(path) for path in cfg.macro_data],\n",
    "bankruptcy_col = cfg.bankruptcy_col,\n",
    "company_col=cfg.company_col,\n",
    "revenue_cap=cfg.revenue_cap,\n",
    "metrics=cfg.get_metrics().to(cfg.device),\n",
    "device=cfg.device,\n",
    "num_layers=cfg.num_classes,\n",
    "hidden_size=cfg.hidden_size,\n",
    "output_size=cfg.num_classes,\n",
    "epochs=cfg.epochs,\n",
    "lr=cfg.lr,\n",
    "train_fract=cfg.train_fract,\n",
    "dropout=cfg.dropout,\n",
    "scheduler_factor=cfg.scheduler_factor,\n",
    "scheduler_patience=cfg.scheduler_patience,\n",
    "decay_ih=cfg.decay_ih,\n",
    "decay_hh=cfg.decay_hh,\n",
    "decay_other=cfg.decay_other,\n",
    "seed=cfg.seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestion = IngestionPipeline(\n",
    "    company_path=company_data_path,\n",
    "    macro_paths=macro_data_path,\n",
    "    company_col=company_col,\n",
    "    bankruptcy_col=bankruptcy_col,\n",
    "    revenue_cap=revenue_cap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.loaders:Reading file: data/demo_data.xlsx\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mingestion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/src/data/pipeline.py:41\u001b[39m, in \u001b[36mIngestionPipeline.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     company_df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompany_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompany_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbankruptcy_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     macro_df= \u001b[38;5;28mself\u001b[39m.macro_loader.load()\n\u001b[32m     49\u001b[39m     years=\u001b[38;5;28mself\u001b[39m.company_loader.years\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py:27\u001b[39m, in \u001b[36mCompanyDataLoader.load\u001b[39m\u001b[34m(self, company_col, bankruptcy_col, mode)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mself\u001b[39m.bankruptcy_col=bankruptcy_col\n\u001b[32m     25\u001b[39m \u001b[38;5;28mself\u001b[39m.mode = mode\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mself\u001b[39m.years = \u001b[38;5;28msorted\u001b[39m({col[-\u001b[32m4\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df.columns \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mrevenue\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m col})\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._clean(df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/202504 Bankruptcy prediction on restaurants/src/data/loaders.py:33\u001b[39m, in \u001b[36mCompanyDataLoader._read_file\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_file\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> pd.DataFrame:\n\u001b[32m     32\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReading file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn.a.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompany_col\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/io/excel/_base.py:508\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m     data = \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_filter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    534\u001b[39m     \u001b[38;5;66;03m# make sure to close opened file handles\u001b[39;00m\n\u001b[32m    535\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m should_close:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/io/excel/_base.py:1616\u001b[39m, in \u001b[36mExcelFile.parse\u001b[39m\u001b[34m(self, sheet_name, header, names, index_col, usecols, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, date_format, thousands, comment, skipfooter, dtype_backend, **kwds)\u001b[39m\n\u001b[32m   1576\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse\u001b[39m(\n\u001b[32m   1577\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1578\u001b[39m     sheet_name: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1596\u001b[39m     **kwds,\n\u001b[32m   1597\u001b[39m ) -> DataFrame | \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, DataFrame] | \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, DataFrame]:\n\u001b[32m   1598\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1599\u001b[39m \u001b[33;03m    Parse specified sheet(s) into a DataFrame.\u001b[39;00m\n\u001b[32m   1600\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1614\u001b[39m \u001b[33;03m    >>> file.parse()  # doctest: +SKIP\u001b[39;00m\n\u001b[32m   1615\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1617\u001b[39m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1618\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1619\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1620\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1635\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1636\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/io/excel/_base.py:868\u001b[39m, in \u001b[36mBaseExcelReader.parse\u001b[39m\u001b[34m(self, sheet_name, header, names, index_col, usecols, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, dtype_backend, **kwds)\u001b[39m\n\u001b[32m    865\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index_col, Sequence)\n\u001b[32m    867\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m index_col:\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m     last = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    870\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(offset + \u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(data)):\n\u001b[32m    871\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m data[row][col] == \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m data[row][col] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mTypeError\u001b[39m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "ingestion.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

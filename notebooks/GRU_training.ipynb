{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf07fc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parent\n",
    "sys.path.append(str(ROOT))\n",
    "\n",
    "import torch\n",
    "import mlflow\n",
    "import datetime\n",
    "import logging\n",
    "import yaml\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import Metric, MetricCollection\n",
    "from torchmetrics.classification import (\n",
    "    BinaryAccuracy, BinaryAUROC, BinaryF1Score, BinaryMatthewsCorrCoef,\n",
    "    MulticlassAccuracy, MulticlassAUROC, MulticlassF1Score)\n",
    "from pathlib import Path\n",
    "\n",
    "from src.datasets.dual_input import DualInputSequenceDataset\n",
    "from src.models.gru import GRUModel\n",
    "from src.data.pipeline import IngestionPipeline\n",
    "from src.train.gru import train_gru\n",
    "from src.utils.utils import CustomReduceLROnPlateau, collate_with_macro, TrainConfig, FocalLoss\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "def load_yaml_file(path):\n",
    "    with open(path) as stream:\n",
    "        try:\n",
    "            config_dict=yaml.safe_load(stream)\n",
    "            return config_dict\n",
    "        except yaml.YAMLError as e:\n",
    "            TypeError(f\"Config file could not be loaded: {e}\")\n",
    "    \n",
    "def train_model_from_config(cfg: TrainConfig) -> GRUModel:\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    return train(\n",
    "        company_path = Path(\"../\" + cfg.firm_data),\n",
    "        macro_paths = [str(id) for id in cfg.macro_data],\n",
    "        bankruptcy_col = str(cfg.bankruptcy_col),\n",
    "        company_col=str(cfg.company_col),\n",
    "        revenue_cap=int(cfg.revenue_cap),\n",
    "        metrics=cfg.get_metrics().to(cfg.device),\n",
    "        device=str(cfg.device),\n",
    "        num_layers=int(cfg.num_classes),\n",
    "        hidden_size=int(cfg.hidden_size),\n",
    "        output_size=1,\n",
    "        epochs=int(cfg.epochs),\n",
    "        lr=float(cfg.lr),\n",
    "        train_fract=float(cfg.train_fract),\n",
    "        dropout=int(cfg.dropout),\n",
    "        alpha=float(cfg.alpha),\n",
    "        gamma=float(cfg.gamma),\n",
    "        scheduler_factor=float(cfg.scheduler_factor),\n",
    "        scheduler_patience=int(cfg.scheduler_patience),\n",
    "        stopping_patience=int(cfg.stopping_patience),\n",
    "        decay_ih=float(cfg.decay_ih),\n",
    "        decay_hh=float(cfg.decay_hh),\n",
    "        decay_other=float(cfg.decay_other),\n",
    "        seed=int(cfg.seed)\n",
    "    )\n",
    "\n",
    "def train(\n",
    "    company_path: str,\n",
    "    macro_paths: list[str],\n",
    "    bankruptcy_col: str,\n",
    "    company_col: str,\n",
    "    revenue_cap: int,\n",
    "    metrics: list[Metric],\n",
    "    seed: int,\n",
    "    num_layers: int = 2,\n",
    "    hidden_size: int = 64,\n",
    "    output_size: int = 1,\n",
    "    epochs: int = 50,\n",
    "    lr: float = 1e-3,\n",
    "    train_fract: float = 0.8,\n",
    "    dropout: float = 0.2,\n",
    "    alpha: float = 0.9,\n",
    "    gamma: float = 2.0,\n",
    "    scheduler_factor: float = 0.85,\n",
    "    scheduler_patience: int = 50,\n",
    "    stopping_patience: int = 10,\n",
    "    stopping_window: int = 5,\n",
    "    min_lr: float = 0.0,\n",
    "    decay_ih:float = 1e-5,\n",
    "    decay_hh:float = 1e-5,\n",
    "    decay_other:float = 1e-5,\n",
    "    device: str=\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "):  \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    ingestion = IngestionPipeline(\n",
    "        company_path=company_path,\n",
    "        macro_paths=macro_paths,\n",
    "        company_col=company_col,\n",
    "        bankruptcy_col=bankruptcy_col,\n",
    "        revenue_cap=revenue_cap\n",
    "    )\n",
    "    \n",
    "    ingestion.run()\n",
    "    X, M_past, M_future, y = ingestion.get_tensors()\n",
    "    \n",
    "    dataset = DualInputSequenceDataset(\n",
    "        firm_tensor = X,\n",
    "        macro_past_tensor = M_past,\n",
    "        macro_future_tensor = M_future,\n",
    "        labels = y\n",
    "    )\n",
    "    \n",
    "    train_ds, val_ds, seed = dataset.stratified_split(train_fract)\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_with_macro)\n",
    "    val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=collate_with_macro)\n",
    "\n",
    "    logger.info(f\"Device: {device}\")\n",
    "    \n",
    "    metrics.to(device)\n",
    "    train_ds = train_ds.to_device(device)\n",
    "    val_ds = val_ds.to_device(device)\n",
    "    \n",
    "    pos_weight = dataset.pos_weight()\n",
    "    \n",
    "    firm_input_size, macro_input_size, _ = dataset.input_dims()\n",
    "    firm_input_size = firm_input_size[-1]\n",
    "    macro_input_size = macro_input_size[-2]\n",
    "    \n",
    "    mlflow.set_tracking_uri('http://127.0.0.1:8080')\n",
    "    mlflow.set_experiment('bankruptcy-predictions')\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag(\"model_type\", \"gru\")\n",
    "        mlflow.log_param(\"seed\", seed)\n",
    "        model = GRUModel(\n",
    "            firm_input_size=firm_input_size,\n",
    "            macro_input_size=macro_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=output_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        model = model.to(device)\n",
    "        \n",
    "        # loss_fn = FocalLoss(alpha=alpha, gamma=gamma, reduction=\"mean\")\n",
    "        loss_fn = BCEWithLogitsLoss(pos_weight = pos_weight)\n",
    "        \n",
    "        # Logging hyperparameters\n",
    "        mlflow.log_param(\"hidden_size\", hidden_size)\n",
    "        mlflow.log_param(\"output_size\", output_size)\n",
    "        mlflow.log_param(\"num_layers\", num_layers)\n",
    "        mlflow.log_param(\"dropout\", dropout)\n",
    "        mlflow.log_param(\"lr\", lr)\n",
    "\n",
    "        ih_params = []\n",
    "        hh_params = []\n",
    "        other_params = []\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                ih_params.append(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                hh_params.append(param)\n",
    "            else:\n",
    "                other_params.append(param)\n",
    "        \n",
    "        optimizer = Adam([\n",
    "                {'params': ih_params, 'weight_decay': decay_ih},\n",
    "                {'params': hh_params, 'weight_decay': decay_hh},\n",
    "                {'params': other_params, 'weight_decay': decay_other},\n",
    "            ], lr=lr\n",
    "        )\n",
    "        scheduler=ReduceLROnPlateau(\n",
    "            optimizer=optimizer,\n",
    "            mode=\"min\",\n",
    "            factor=scheduler_factor,\n",
    "            patience=scheduler_patience,\n",
    "            min_lr=min_lr\n",
    "        )\n",
    "        \n",
    "        train_gru(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            stopping_patience=stopping_patience,\n",
    "            stopping_window=stopping_window,\n",
    "            device=device,\n",
    "            epochs=epochs,\n",
    "            metrics=metrics\n",
    "        )\n",
    "        \n",
    "        model_name = f\"GRUModel_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        mlflow.pytorch.log_model(model, model_name)\n",
    "        torch.save(obj = model.state_dict(), f = f\"../models/{model_name}.pth\")\n",
    "        print(f\"Model saved: {model_name}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "383adb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = load_yaml_file(\"../config/gru_config.yml\")\n",
    "cfg = TrainConfig(**config_dict)\n",
    "\n",
    "company_data_path = Path(\"../\" + cfg.firm_data)\n",
    "macro_data_path = [str(id) for id in cfg.macro_data]\n",
    "bankruptcy_col = str(cfg.bankruptcy_col)\n",
    "company_col=str(cfg.company_col)\n",
    "revenue_cap=int(cfg.revenue_cap)\n",
    "metrics=cfg.get_metrics().to(cfg.device)\n",
    "device=str(cfg.device)\n",
    "num_layers=int(cfg.num_classes)\n",
    "hidden_size=16\n",
    "output_size=1\n",
    "epochs=int(cfg.epochs)\n",
    "lr=float(cfg.lr)\n",
    "train_fract=float(cfg.train_fract)\n",
    "dropout=int(cfg.dropout)\n",
    "scheduler_factor=float(cfg.scheduler_factor)\n",
    "scheduler_patience=int(cfg.scheduler_patience)\n",
    "decay_ih=float(cfg.decay_ih)\n",
    "decay_hh=float(cfg.decay_hh)\n",
    "decay_other=float(cfg.decay_other)\n",
    "seed=int(cfg.seed)\n",
    "\n",
    "ingestion = IngestionPipeline(\n",
    "    company_path=company_data_path,\n",
    "    macro_paths=macro_data_path,\n",
    "    company_col=company_col,\n",
    "    bankruptcy_col=bankruptcy_col,\n",
    "    revenue_cap=revenue_cap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5486b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.loaders:Reading file: ../data/demo_data2.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.loaders:Dropping high-revenue outliers...\n",
      "INFO:src.data.loaders:Applying feature engineering...\n",
      "INFO:src.data.feature_engineer:Fitting feature engineer...\n",
      "INFO:src.data.feature_engineer:Engineering features...\n",
      "INFO:src.data.loaders:Loading 3 macroeconomic series...\n",
      "/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/sdmx/rest/common.py:367: UserWarning: 'agency_id' argument is redundant for data queries\n",
      "  getattr(self, f\"handle_{query_type}\")()\n",
      "INFO:sdmx.client:Request https://www.bdm.insee.fr/series/sdmx/data/SERIES_BDM/010774417\n",
      "INFO:sdmx.client:with headers {'User-Agent': 'python-requests/2.32.3', 'Accept-Encoding': 'gzip, deflate, br, zstd', 'Accept': 'application/vnd.sdmx.genericdata+xml;version=2.1', 'Connection': 'keep-alive'}\n",
      "/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/sdmx/rest/common.py:367: UserWarning: 'agency_id' argument is redundant for data queries\n",
      "  getattr(self, f\"handle_{query_type}\")()\n",
      "INFO:sdmx.client:Request https://www.bdm.insee.fr/series/sdmx/data/SERIES_BDM/001763782\n",
      "INFO:sdmx.client:with headers {'User-Agent': 'python-requests/2.32.3', 'Accept-Encoding': 'gzip, deflate, br, zstd', 'Accept': 'application/vnd.sdmx.genericdata+xml;version=2.1', 'Connection': 'keep-alive'}\n",
      "/Users/guillaumedecina-halmi/miniforge3/lib/python3.12/site-packages/sdmx/rest/common.py:367: UserWarning: 'agency_id' argument is redundant for data queries\n",
      "  getattr(self, f\"handle_{query_type}\")()\n",
      "INFO:sdmx.client:Request https://www.bdm.insee.fr/series/sdmx/data/SERIES_BDM/001587668\n",
      "INFO:sdmx.client:with headers {'User-Agent': 'python-requests/2.32.3', 'Accept-Encoding': 'gzip, deflate, br, zstd', 'Accept': 'application/vnd.sdmx.genericdata+xml;version=2.1', 'Connection': 'keep-alive'}\n",
      "INFO:src.data.tensor_factory:Converting financial series to tensors...\n",
      "INFO:src.data.tensor_factory:Scaling financial data with RobustScaler...\n",
      "INFO:src.data.tensor_factory:Financial data tensor shape: (6318, 3, 13)\n",
      "INFO:src.data.tensor_factory:Converting macroeconomic series to tensors...\n",
      "INFO:src.data.tensor_factory:Macro data tensor shape: torch.Size([3, 43]) (past), torch.Size([3, 12]) (future)\n",
      "INFO:__main__:Device: mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Past: torch.Size([3, 43]), Future: torch.Size([3, 12])\n",
      "Data sent to device: mps\n",
      "Data sent to device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        model = train_model_from_config(cfg)\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.mps.empty_cache()\n",
    "    except:\n",
    "        logging.error(\"Training failed.\", exc_info=True)\n",
    "        time.sleep(3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ee888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "import logging\n",
    "import mlflow\n",
    "import yaml\n",
    "\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import MetricCollection\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from src.data.pipeline import IngestionPipeline\n",
    "from src.datasets.dual_input import DualInputSequenceDataset\n",
    "from src.models.tft import TFTModel\n",
    "from src.utils.utils import CustomReduceLROnPlateau, TrainConfig, collate_with_macro\n",
    "\n",
    "def load_yaml_file(path):\n",
    "    with open(path) as stream:\n",
    "        try:\n",
    "            config_dict=yaml.safe_load(stream)\n",
    "            return config_dict\n",
    "        except yaml.YAMLError as e:\n",
    "            TypeError(f\"Config file could not be loaded: {e}\")\n",
    "\n",
    "\n",
    "def train_tft(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    num_epochs: int,\n",
    "    metrics: MetricCollection,\n",
    "    criterion: nn.Module = nn.MSELoss(),\n",
    "    optimizer_cls=torch.optim.Adam,\n",
    "    lr: float = 1e-3,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"mps\" \n",
    "        if torch.mps.is_available() else \"cpu\",\n",
    "    early_stopping_patience: int = 10,\n",
    "    scheduler_cls=None,\n",
    "    scheduler_kwargs=None,\n",
    "    log_fn=None,\n",
    "):\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer=optimizer_cls(model.parameters(), lr=lr)\n",
    "    scheduler=scheduler_cls(optimizer, **scheduler_kwargs) if scheduler_cls else None\n",
    "    \n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} / {num_epochs} — Training\"):\n",
    "            firm_x = batch[\"firm_seq\"].to(device)\n",
    "            macro_x = batch[\"macro_seq\"].to(device)\n",
    "            decoder_x = macro_x[:, -12:, :] # Condition on the last year of macro data\n",
    "            y = batch[\"label\"].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(firm_x, macro_x, decoder_x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "        train_loss = sum(train_losses) / len(train_losses)\n",
    "        computed_metrics = {\n",
    "            name: metric.compute().item() for name, metric in metrics.items()\n",
    "        }\n",
    "        computed_metrics[\"loss\"] = train_loss\n",
    "        for name, value in metrics.items():\n",
    "            mlflow.log_metric(f\"train_{name}\", value, step = epoch)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} / {num_epochs} — Validation\"):\n",
    "                firm_x = batch[\"firm_seq\"].to(device)\n",
    "                macro_x = batch[\"macro_seq\"].to(device)\n",
    "                decoder_x = macro_x[:, -12:, :] # Condition on the last year of macro data\n",
    "                y_hat = model(firm_x, macro_x, decoder_x)\n",
    "                \n",
    "                val_loss = criterion(y_hat, y)\n",
    "                val_losses.append(val_loss.item())\n",
    "        \n",
    "        val_loss = sum(val_losses) / len(val_losses)\n",
    "        computed_metrics = {\n",
    "            name: metric.compute().item() for name, metric in metrics.items()\n",
    "        }\n",
    "        for name, value in metrics.items():\n",
    "            mlflow.log_metric(f\"val_{name}\", value, step = epoch)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        \n",
    "        if log_fn:\n",
    "            log_fn(epoch=epoch, train_loss=train_loss, val_loss=val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_tft_model.pt\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "    model_name = f\"model_{datetime.datetime.now()}\"\n",
    "    mlflow.pytorch.log_model(model, model_name)\n",
    "    torch.save(obj = model.state_dict(), f = f\"models/{model_name}.pth\")\n",
    "    print(f\"Model saved: {model_name}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "def main():\n",
    "    config_dict = load_yaml_file(\"config/model_config.yml\")\n",
    "    cfg = TrainConfig(**config_dict)\n",
    "\n",
    "    company_path = Path(cfg.firm_data)\n",
    "    macro_paths = [Path(path) for path in cfg.macro_data]\n",
    "    bankruptcy_col = str(cfg.bankruptcy_col)\n",
    "    company_col=str(cfg.company_col)\n",
    "    revenue_cap=int(cfg.revenue_cap)\n",
    "    metrics=cfg.get_metrics().to(cfg.device)\n",
    "    device=str(cfg.device)\n",
    "    num_layers=int(cfg.num_classes)\n",
    "    hidden_size=int(cfg.hidden_size)\n",
    "    output_size=1\n",
    "    epochs=int(cfg.epochs)\n",
    "    lr=float(cfg.lr)\n",
    "    train_fract=float(cfg.train_fract)\n",
    "    dropout=int(cfg.dropout)\n",
    "    scheduler_factor=float(cfg.scheduler_factor)\n",
    "    scheduler_patience=int(cfg.scheduler_patience)\n",
    "    decay_ih=float(cfg.decay_ih)\n",
    "    decay_hh=float(cfg.decay_hh)\n",
    "    decay_other=float(cfg.decay_other)\n",
    "    seed=int(cfg.seed)\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    ingestion = IngestionPipeline(\n",
    "        company_path=company_path,\n",
    "        macro_paths=macro_paths,\n",
    "        company_col=company_col,\n",
    "        bankruptcy_col=bankruptcy_col,\n",
    "        revenue_cap=revenue_cap\n",
    "    )\n",
    "    \n",
    "    ingestion.run()\n",
    "    \n",
    "    X, M, y = ingestion.get_tensors()\n",
    "    \n",
    "    dataset = DualInputSequenceDataset(\n",
    "        firm_tensor = X,\n",
    "        macro_tensor = M,\n",
    "        labels = y\n",
    "    )\n",
    "    \n",
    "    train_ds, val_ds, seed = dataset.stratified_split(train_fract)\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_with_macro)\n",
    "    val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=collate_with_macro)\n",
    "\n",
    "    logger.info(f\"Device: {device}\")\n",
    "    \n",
    "    firm_input_size, macro_input_size = dataset.input_dims()\n",
    "\n",
    "    optimizer = Adam\n",
    "    scheduler=CustomReduceLROnPlateau\n",
    "    scheduler_kwargs = {\n",
    "        \"mode\": \"min\",\n",
    "        \"factor\": scheduler_factor,\n",
    "        \"patience\": scheduler_patience,\n",
    "        \"min_lr\": 0.0\n",
    "    }\n",
    "    \n",
    "    pos_weight = dataset.pos_weight()\n",
    "    loss_fn = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    mlflow.set_tracking_uri('http://127.0.0.1:8080')\n",
    "    mlflow.set_experiment('bankruptcy-predictions')\n",
    "    \n",
    "    # Logging hyperparameters\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"hidden_size\", hidden_size)\n",
    "        mlflow.log_param(\"output_size\", output_size)\n",
    "        mlflow.log_param(\"num_layers\", num_layers)\n",
    "        mlflow.log_param(\"dropout\", dropout)\n",
    "        mlflow.log_param(\"lr\", lr)\n",
    "        mlflow.log_param(\"seed\", seed)\n",
    "        \n",
    "        model = TFTModel(\n",
    "            static_input_dim=firm_input_size,\n",
    "            encoder_input_dims=[1] * macro_input_size,\n",
    "            decoder_input_dims=[1] * macro_input_size,\n",
    "            hidden_dim=hidden_size,\n",
    "            attention_heads=4,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        train_tft(\n",
    "            model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            num_epochs=epochs,\n",
    "            criterion=loss_fn,\n",
    "            optimizer_cls=optimizer,\n",
    "            lr=lr,\n",
    "            scheduler_cls=scheduler,\n",
    "            scheduler_kwargs=scheduler_kwargs,\n",
    "            metrics=metrics\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88fe24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db3aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160dc5d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
